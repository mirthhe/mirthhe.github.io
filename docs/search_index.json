[["index.html", "Curriculum Vitae Contact Profile Job experience Education Skills Volunteering Languages", " Curriculum Vitae Contact Utrecht, The Netherlands mirtheklaassen@outlook.com Github Linkedin Medium Profile I am goal-oriented, curious, and analytical with a versatile background. I am eager to combine my skills with my passion for data science, structural biology, and biocomplexity in the dynamic field of bioinformatics. By doing so, I aim to make valuable contributions to the rapid advancement of biomedical research and promote the reduction of animal testing. Job experience Assistant manager at Fitness Factory, Utrecht June 2017 - October 2018 My responsibilities included remodeling projects, contact with externals, hiring and teaching new employees, basic accountancy, inventory management and technical maintenance. Also active as spinning-instructor. Examtrainer Math at Lyceo, Utrecht March 2018 - September 2018 Sales employee at Lens, Utrecht May 2018 - August 2019 Assistant manager at TrainMore Black Label, Utrecht August 2019 - September 2020 My responsibilities included directing the sales-team, technical maintenance, inventory management, member administration, contacting debtors/debt collection agencies and contact with externals. Also active as kickboxing- and HIIT-instructor. Webdeveloper, WisMon, Utrecht March 2021 - September 2021 Sales employee, Pipoos, Utrecht September 2022 - Present Education Communication &amp; Multimedia design, Avans, Den Bosch September 2015 - December 2017 With a personal focus on front-end webdevelopment and datavisualisation Optometry, University of Applied Sciences, Utrecht September 2018 - Augustus 2019 Biology &amp; Medical Labresearch, University of Applied Sciences, Utrecht September 2020 - Present GPA of 4.0 Honours star in genetic recombination and transfection Specialization in Data Science for Biology Skills Informatics: Basic R, basic BASH, HTML and CSS Design: Adobe Photoshop, Adobe Illustrator and Adobe InDesign Volunteering Design-expert, Innovation in a Week, Veghel July 2016 Won the ‘Teamwork-award’ Student Association Uerius Brabus, Den Bosch October 2015 - December 2016 Active as co-founder, captain of the PR committee and active in the committee for external contacts Languages Dutch: native speaker English: level C1 "],["1_CurrentProjects.html", "Chapter 1 Current work Progress", " Chapter 1 Current work I am currently working on developing a quantitative approach to assess the weight of evidence for Adverse Outcome Pathways (AOPs) using artificial intelligence. AOPs serve as frameworks for organizing knowledge about how a molecular initiating event (MIE) can lead to an adverse outcome (AO) at various levels of biological organization. The aim of AOPs is to establish a systematic and transparent linkage between molecular-level data and observed effects at higher biological organization levels. Furthermore, they support the development of alternative testing methods that can decrease the reliance on animal testing for chemical safety assessments. Currently, the assessment for weight of evidence for AOPs relies heavily on expert judgment. However, considering the growing utilization of deep learning and natural language processing (NLP) in toxicology, there is a need for a standardized quantitative approach. The objective is to automate this process using artificial intelligence. The progress and findings of the project will be shared through a series of blog posts and a final report. Progress Introduction to the project (week 1 - 10) OECD guidelines and avoiding possible downfalls (week 11 - 13) Categories and scoring system per criterion (week 14) Using AI for objective scoring (week 15 - 16) Generating and maintaining optimal training data (week 17) "],["2_SBtab.html", "Chapter 2 SBtabr package", " Chapter 2 SBtabr package Our team, including C. Niemel, J. Nijdam, and myself, developed an R package for the ONTOX project in collaboration with the Life Sciences and Chemistry mentorship at the University of Applied Sciences in Utrecht. The ONTOX project’s goal is to reduce animal testing in toxicology through ways like optimizing the utilization of existing data with artificial intelligence and SysRev, a software platform that extracts relevant data from scientific documents and presents it in SBtab format. We created SBtabr, an R package which can merge multiple SBtab files and visualize the data as a network using six functions, including four for merging files and two for visualizing them. Figure 2.1 depicts the workflow. These functions streamline the identification of correlations and the development of predictive models, which could improve human risk assessment. Figure 2.1: A diagram showing how the functions can be combined into one workflow. My role in this project involved handling the merging functions. Firstly, I have developed the “sep_sbtab()” function, which separates tables in the SBtab format and creates an edges table specifically designed for visualization purposes. Secondly, the “prep_merge()” function has been created to prepare the tables for merging by ensuring that the identifiers are made unique across multiple files. Next, the “merge_prepped()” function performs the actual merging of the prepared tables. Lastly, the comprehensive “merge_sbtab()” function combines all three of the aforementioned functions into a single step, streamlining the merging process. "],["3_3D_visualizations.html", "Chapter 3 Protein structures in R 3.1 Protein structure data 3.2 Visualization methods 3.3 Applying in R 3.4 Looking forward", " Chapter 3 Protein structures in R As part of assignment 3.2 from the DSFB2 Workflows course and as a continuation from chapter 3. There are two paths within three-dimensional structural bioinformatics, one focusing on the evolutionary history and the other on the physical interactions (Jumper et al. 2021). Since I am most interested in the latter, I have done the following: Research databases and formats with protein structure data Research visualization methods for protein structure Visualise kinesin using R Reflecting and looking forward 3.1 Protein structure data High-throughput technologies have improved in the biomedical field. There’s a large variety of types for bioinformatics databases like sequences, enzyme/pathway, chemistry or 3D structures. (Chen, Huang, and Wu 2017). The worldwide Protein Data Bank archive (wwPDB) provides databases for studies on genomics, transcriptomics, and proteomics. Established in 2003, wwPDB offers three file formats, containing over 113,000 biological macromolecular structures, and PDBx/mmCIF is its primary format used since 2007. Compared to other formats, PDBx/mmCIF is machine-readable and able to archive structural models of any size. It was also adopted by major software developers in 2011 for molecule visualizations (Burley et al. 2017). 3.2 Visualization methods There are multiple programs for the visualization of protein structures, like PyMol, UCSF’s Chimera and OpenRasMol. To take a closer look at what these types of programs are all about and the different ways of visualizing protein structures, PyMol was downloaded. For this analysis we will be looking at my favorite protein, kinesin. The PDBx file for kinesin was taken from 3COB in the RCSB database. After uploading the PDBx file to PyMol, the protein was first shown as in figure 3.1. Figure 3.1: Kinesin as shown after uploading the 3COB file to PyMol. In this first visualization the protein is shown in what is known as a ‘cartoon’ form. It is a simplified version where sidechains are ignored. Only the backbone is shown with the alpha helices as coils and the beta sheets as arrows. This leaves a lot of valuable information out so other styles are needed. To show different forms the external GUI (top window) can be used, as it’s text area logs commands. We’ll use this to look at two different visualization styles, spheres and sticks. PyMol makes switching between visualizations really easy. To show a different type, like spheres, the command is ‘show sphere’. To hide another, like cartoon, the command is ‘hide cartoon’. After these commands only the spheres are shown, as seen in figure 3.2. Figure 3.2: Kinesin as shown after uploading the 3COB file to PyMol and only showing the spheres visual. Spheres, unlike cartoon, shows the atoms (see figure 3.2). Each atom is shown as a sphere. The size of each sphere is decided by the atoms electron orbitals. This can be used to show packing and spheric hindrance but doesn’t show the interior. The colors indicate different atoms, in the default settings carbon atoms are green, nitrogen is blue, oxygen is red, etc. The last visual style will be sticks, this shows the chemical structure (see figure 3.3). The colors still indicate the same atoms as they did for the sphere style. Figure 3.3: Kinesin as shown after uploading the 3COB file to PyMol and only showing the sticks visual. 3.3 Applying in R It is also possible to visualize the structure in R using packages like r3dmol or protein8k. Both of these packages are still in very early development with the last update being at least 5 months ago. Still, they are good resources to practice visualizing proteins in R. Using the r3dmol package I will try to replicate the visuals as seen using PyMol. The r3dmol functions have been used to create four functions, protein_cartoon, protein_stick, protein_sphere and protein_visual in the portrobbo package. The input for the functions is a PDBx file, turned into an R object using the readLines() function. The portrobbo package comes with an ready-made object called kinesin, this is from the earlier used PDBx file taken from 3COB. The PyMol visuals will be recreated using these functions and the kinesin object. At first I tried using the colorScheme = “ssPyMOL” option but this did not seem to work as promised so I went over the elements separately to match the colors. 3.3.1 Cartoon visual With the first visual, cartoon style, PyMOL uses the HEX code #3aff3a for both the beta sheets and alpha helices. The beta sheets are also accompanied by an arrow to indicate the direction. Both of these characteristics have been incorporated in the function. One thing the result does not recreate is the small spheres, showing specific atoms. The result can be seen in interactive figure 3.4. Function protein_cartoon() protein_cartoon &lt;- function(pdbobject){ r3dmol( viewer_spec = m_viewer_spec( # How far user can zoom in lowerZoomLimit = 30, # How far user can zoom out upperZoomLimit = 400, # Decrease quality to improve performance antialias = FALSE, disableFog = TRUE, backgroundColor = &quot;black&quot;)) %&gt;% # Adding the protein m_add_model(data = pdbobject, format = &quot;pdb&quot;) %&gt;% # Center the protein at first m_zoom_to() %&gt;% # Set style of structures m_set_style( sel = m_sel(ss = &quot;s&quot;), # Style beta sheets style = m_style_cartoon(color = &quot;#3aff3a&quot;, arrows = TRUE)) %&gt;% m_set_style( sel = m_sel(ss = &quot;h&quot;), # Style alpha helix style = m_style_cartoon(color = &quot;#3aff3a&quot;)) %&gt;% # Rotate the scene by given angle on given axis m_rotate(angle = 90, axis = &quot;y&quot;) } # The kinesin file used earlier has been incorporated in the portrobbo package # The r3dmol package has been incorporated in the protein_* functions of the portrobbo package protein_cartoon(kinesin) Figure 3.4: Kinesin as described by the 3COB PDBx file from wwPDB. Visualised in cartoon style using the r3dmol package and PyMOL color scheme. Using a click and drag method the image can be turned and by scrolling the image can be zoomed in and out. 3.3.2 Sphere and sticks visual For the spheres and sticks visuals the recreation is more accurate than the cartoon visual. All atoms got their own color, as stated on the PyMOLWiki. The functions can be seen below and the results in interactive figures 3.5 and 3.6. Function protein_sphere() protein_sphere &lt;- function(pdbobject){ r3dmol( viewer_spec = m_viewer_spec( # How far user can zoom in lowerZoomLimit = 30, # How far user can zoom out upperZoomLimit = 400, # Decrease quality to improve performance antialias = FALSE, disableFog = TRUE, backgroundColor = &quot;black&quot;)) %&gt;% # Adding the protein m_add_model(data = pdbobject, format = &quot;pdb&quot;) %&gt;% # Center the protein at first m_zoom_to() %&gt;% # Set style of atoms m_set_style( sel = m_sel(elem = &quot;C&quot;), # Style carbon style = m_style_sphere(color = &quot;#33ff33&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;N&quot;), # Style nitrogen style = m_style_sphere(color = &quot;#3333ff&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;O&quot;), # Style oxygen style = m_style_sphere(color = &quot;#ff4c4c&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;S&quot;), # Style sulfur style = m_style_sphere(color = &quot;#e5c53f&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;P&quot;), # Style phosphorus style = m_style_sphere(color = &quot;#ff7f00&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;MG&quot;), # Style magnesium style = m_style_sphere(color = &quot;#8aff00&quot;)) %&gt;% # Rotate by given angle on given axis m_rotate(angle = 90, axis = &quot;y&quot;) } Function protein_stick() protein_sphere &lt;- function(pdbobject){ r3dmol( viewer_spec = m_viewer_spec( # How far user can zoom in lowerZoomLimit = 30, # How far user can zoom out upperZoomLimit = 400, # Decrease quality to improve performance antialias = FALSE, disableFog = TRUE, backgroundColor = &quot;black&quot;)) %&gt;% # Adding the protein m_add_model(data = pdbobject, format = &quot;pdb&quot;) %&gt;% # Center the protein at first m_zoom_to() %&gt;% # Set style of atoms m_set_style( sel = m_sel(elem = &quot;C&quot;), # Style carbon style = m_style_stick(color = &quot;#33ff33&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;N&quot;), # Style nitrogen style = m_style_stick(color = &quot;#3333ff&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;O&quot;), # Style oxygen style = m_style_stick(color = &quot;#ff4c4c&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;S&quot;), # Style sulfur style = m_style_stick(color = &quot;#e5c53f&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;P&quot;), # Style phosphorus style = m_style_stick(color = &quot;#ff7f00&quot;)) %&gt;% m_set_style( sel = m_sel(elem = &quot;MG&quot;), # Style magnesium style = m_style_stick(color = &quot;#8aff00&quot;)) %&gt;% # Rotate by given angle on given axis m_rotate(angle = 90, axis = &quot;y&quot;) } # The kinesin file used earlier has been incorporated in the portrobbo package # The r3dmol package has been incorporated in the protein_* functions of the portrobbo package protein_sphere(kinesin) Figure 3.5: Kinesin as described by the 3COB PDBx file from wwPDB. Visualised in sphere style using the r3dmol package and PyMOL color scheme. Using a click and drag method the image can be turned and by scrolling the image can be zoomed in and out. # The kinesin file used earlier has been incorporated in the portrobbo package # The r3dmol package has been incorporated in the protein_* functions of the portrobbo package protein_stick(kinesin) Figure 3.6: Kinesin as described by the 3COB PDBx file from wwPDB. Visualised in stick style using the r3dmol package and PyMOL color scheme. Using a click and drag method the image can be turned and by scrolling the image can be zoomed in and out. 3.3.3 Combining styles A big downside of these visualizations is how none of them show a complete picture. Cartoon style doesn’t show the atoms, sphere styles doesn’t show the chemical structure and stick style doesn’t show the sferic hindrance. Because of this, one more function was made, to combine the stick and sphere visuals in a way that clearly shows both of them. To do this they were combined in one function with the stick visual on full opacity and the sphere visual on an opacity of 0.7. The result can be seen in figure 3.7. The clarity of the spherical style could be better and the overlap definitely has a negative effect on the readability, but the combination still gives a good overview of the protein’s 3D structure. Function protein_visual() protein_visual &lt;- function(pdbobject){ r3dmol( viewer_spec = m_viewer_spec( # How far user can zoom in lowerZoomLimit = 30, # How far user can zoom out upperZoomLimit = 400, # Decrease quality to improve performance antialias = FALSE, disableFog = TRUE, backgroundColor = &quot;black&quot;)) %&gt;% # Adding the protein m_add_model(data = pdbobject, format = &quot;pdb&quot;) %&gt;% # Center the protein at first m_zoom_to() %&gt;% # Set style of atoms m_set_style( sel = m_sel(elem = &quot;C&quot;), # Style carbon style = c(m_style_stick(color = &quot;#33ff33&quot;), m_style_sphere(opacity = 0.7, color = &quot;#33ff33&quot;))) %&gt;% m_set_style( sel = m_sel(elem = &quot;N&quot;), # Style nitrogen style = c(m_style_stick(color = &quot;#3333ff&quot;), m_style_sphere(opacity = 0.7, color = &quot;#3333ff&quot;))) %&gt;% m_set_style( sel = m_sel(elem = &quot;O&quot;), # Style oxygen style = c(m_style_stick(color = &quot;#ff4c4c&quot;), m_style_sphere(opacity = 0.7, color = &quot;#ff4c4c&quot;))) %&gt;% m_set_style( sel = m_sel(elem = &quot;S&quot;), # Style sulfur style = c(m_style_stick(color = &quot;#e5c53f&quot;), m_style_sphere(opacity = 0.7, color = &quot;#e5c53f&quot;))) %&gt;% m_set_style( sel = m_sel(elem = &quot;P&quot;), # Style phosphorus style = c(m_style_stick(color = &quot;#ff7f00&quot;), m_style_sphere(opacity = 0.7, color = &quot;#ff7f00&quot;))) %&gt;% m_set_style( sel = m_sel(elem = &quot;MG&quot;), # Style magnesium style = c(m_style_stick(color = &quot;#8aff00&quot;), m_style_sphere(opacity = 0.7, color = &quot;#8aff00&quot;))) %&gt;% # Rotate by given angle on given axis m_rotate(angle = 90, axis = &quot;y&quot;) } # The kinesin file used earlier has been incorporated in the portrobbo package # The r3dmol package has been incorporated in the protein_* functions of the portrobbo package protein_visual(kinesin) Figure 3.7: Kinesin as described by the 3COB PDBx file from wwPDB. Visualised in stick and spherical style using the r3dmol package and PyMOL color scheme. Using a click and drag method the image can be turned and by scrolling the image can be zoomed in and out. 3.4 Looking forward After this free assignment I’ve gotten a peak into the field of visualizing molecular structures. It gave me a sense of the possibilities and current state of events. Here I focused on the visualization of an existing structure after the folding has already taken place. In the future I would like to dive deeper into the prediction of protein folding, maybe one day offering a software which visualises the folding process for different proteins. My next steps in this will be literature research about protein folding and expanding my knowledge on visualization and animation techniques. "],["4_SQL.html", "Chapter 4 SQL and relational data 4.1 Connecting to DBeaver 4.2 Data inspection 4.3 Joining datasets 4.4 Descriptive statistics 4.5 Visualizations", " Chapter 4 SQL and relational data As part of assignment 7 from the DSFB2 Workflows course. With this assignment the focus was on learning the basics of SQL and applying it to relational data. To start, the files provided in the assignment were imported and tidied up. # Import flu data ---- flu_df &lt;- read_csv(&quot;data-raw/relationaldata/flu_data.csv&quot;, skip=11) ## First 11 rows are metadata # Make flu dataframe tidy ---- flu_df_tidy &lt;- flu_df %&gt;% pivot_longer(cols = c(2:ncol(flu_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Import dengue data ---- dengue_df &lt;- read_csv(&quot;data-raw/relationaldata/dengue_data.csv&quot;, skip=11) ## First 11 rows are metadata # Make dengue data tidy ---- dengue_df_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = c(2:ncol(dengue_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Gapminder data ---- ## The third dataframe is the gapminder dataframe from the dslabs package ## This dataframe is already tidy # Change country column ---- ## All &#39;country&#39; columns should be the same in type, class and values ## For flu and dengue they&#39;re characters, for gapminder it&#39;s a factor ## Turning the &#39;country&#39; column of both dengue and flu into a factor flu_df_tidy$country &lt;- as.factor(flu_df_tidy$country) dengue_df_tidy$country &lt;- as.factor(dengue_df_tidy$country) # Change date column ---- ## All &#39;date&#39; columns should be the same in type, class and values ## For flu and dengue the dates are specified to the day instead of the year ## We&#39;ll create a new &#39;year&#39; column for both which matches the column of gapminder flu_df_tidy &lt;- flu_df_tidy %&gt;% mutate(year = substr(flu_df_tidy$Date, start=1, stop=4)) dengue_df_tidy &lt;- dengue_df_tidy %&gt;% mutate(year = substr(dengue_df_tidy$Date, start=1, stop=4)) # Make them integers ---- ## Now they&#39;re characters, they should be integers flu_df_tidy$year &lt;- as.integer(flu_df_tidy$year) dengue_df_tidy$year &lt;- as.integer(dengue_df_tidy$year) save_csv_rds(flu_df_tidy, &quot;data/relationaldata/flu&quot;) save_csv_rds(dengue_df_tidy, &quot;data/relationaldata/dengue&quot;) save_csv_rds(gapminder, &quot;data/relationaldata/gapminder&quot;) 4.1 Connecting to DBeaver The three R dataframes were imported into a DBeaver database using RPostgreSQL. The tables were inserted into the hierarchy of this database as shown in figure 4.1. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;insertpassword&quot;) # Password changed after connecting for privacy reasons # Using the DBI library dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbWriteTable(con, &quot;flu&quot;, flu_df_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_df_tidy) Figure 4.1: The hierarchy of the workflowsdb database including the three tables. 4.2 Data inspection The data was inspected using the SQL editor. A script was created for the inspection of the gapminder, flu and dengue data. The same inspection can also be done using R as shown below. SQL script Gapminder -- Checking the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;gapminder&#39;; -- Checking the timeframe in which the data has been collected select distinct year from gapminder order by year desc; -- from 1960 to 2016 -- Checking the countries with the highest and lowest average population select country, AVG (population):: NUMERIC(11,0) from gapminder group by country order by avg desc; -- China has the highest with around 1,000,000,000 -- Greenland the lowest with around 51,000 -- Checking the countries with the highest and lowest average life expectancy select country, AVG (life_expectancy):: NUMERIC(6,2) from gapminder where life_expectancy is not null group by country order by avg desc; -- Iceland has the highest with 78.17 -- Central Africa Republic the lowest with 46.31 -- Checking how many countries are noted in the data select count(distinct country) from gapminder; -- 185 different countries SQL script flu data inspection -- First we check the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;flu&#39;; -- We check in which timeframe the data has been collected select distinct year from flu order by year desc; -- from 2002 to 2015 AVG (cases):: NUMERIC(6,1) from flu group by country order by avg desc; -- South Africa had the most with 2698.8 on average -- Sweden the least 5.5 -- We check how many countries are noted in the data and if they are spelled correctly select distinct country from flu; -- 29 different countries, all spelled correctly SQL script dengue data inspection -- First we check the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;dengue&#39;; -- We check in which timeframe the data has been collected select distinct year from dengue order by year desc; -- from 2002 to 2015 -- We check which countries had the most and least dengue cases on average over this timeframe select country, AVG (cases):: NUMERIC(6,4) from dengue group by country order by avg desc; -- Venezuela had the most with 0.2774 on average -- Bolivia the least with 0.0447 -- We check how many countries are noted in the data and if they are spelled correctly select distinct country from dengue; -- 10 different countries, all spelled correctly R script data inspection # Check the columnnames and types ---- str(gapminder_df) str(flu_df_tidy) str(dengue_df_tidy) # Check the timeframes ---- paste(&quot;From&quot;, min(unique(gapminder_df$year)), &quot;to&quot;, max(unique(gapminder_df$year))) paste(&quot;From&quot;, min(unique(flu_df_tidy$year)), &quot;to&quot;, max(unique(flu_df_tidy$year))) paste(&quot;From&quot;, min(unique(dengue_df_tidy$year)), &quot;to&quot;, max(unique(dengue_df_tidy$year))) # Inspect gapminder ---- ## Checking the highest and lowest average population in the gapminder data options(scipen=999) gapminder_pop &lt;- gapminder_df %&gt;% group_by(country) %&gt;% summarize(average = mean(population, na.rm = TRUE)) gapminder_pop %&gt;% filter(average == max(gapminder_pop$average) | average == min(gapminder_pop$average)) ## Checking the highest and lowest average life expectancy in the gapminder data gapminder_life &lt;- gapminder_df %&gt;% group_by(country) %&gt;% summarize(average = mean(life_expectancy, na.rm = TRUE)) gapminder_life %&gt;% filter(average == max(gapminder_life$average) | average == min(gapminder_life$average)) # Inspect flu ---- ## Checking the most and least average flu cases in the flu data flu_cases &lt;- flu_df_tidy %&gt;% group_by(country) %&gt;% summarize(average = mean(cases, na.rm = TRUE)) flu_cases %&gt;% filter(average == max(flu_cases$average) | average == min(flu_cases$average)) # Inspect dengue ---- ## Checking the most and least average dengue cases in the dengue data dengue_cases &lt;- dengue_df_tidy %&gt;% group_by(country) %&gt;% summarize(average = mean(cases, na.rm = TRUE)) dengue_cases %&gt;% filter(average == max(dengue_cases$average) | average == min(dengue_cases$average)) # Check countries ---- length(unique(gapminder_df$country)) # 185 countries length(unique(flu_df_tidy$country)) # 29 countries length(unique(dengue_df_tidy$country)) # 10 countries 4.3 Joining datasets The three datasets share two common variables, country and year. However, the data provided in the flu and dengue databases is very outdated and unclear, as the links in the metadata are no longer functional. It is difficult to estimate the details of the data, such as the criteria for considering a case. To merge the datasets, the average weekly cases per year per country were used instead of the total average per year, as not every country has data for every week of each year. We did not combine the flu and dengue datasets, since the dengue dataset has a significantly smaller amount of countries to look at. Two tables were created instead: one merging the flu and gapminder data based on continents and another merging the dengue and gapminder data based on country. SQL scripts for both joins are provided (joining_gapminder_dengue.sql and joining_gapminder_flu), and the resulting tables were imported into R using the provided code. gd_country &lt;- read_csv(&quot;data/relationaldata/gapminder_dengue_country.csv&quot;) gf_continent &lt;- read_csv(&quot;data/relationaldata/gapminder_flu_continent.csv&quot;) 4.4 Descriptive statistics The new joined tables were used to perform descriptive statistics. Since the cases were not normalized for population, they were normalized to reflect the number of cases per 100,000 inhabitants. The average number of cases and standard deviation were then calculated for each country/continent. However, the Shapiro-Wilk test revealed that both databases had at least a few non-normally distributed variables (5 out of 10 for dengue and 3 out of 5 for flu). Additionally, the Levene’s test showed that neither database had an equality of variance between countries/continents. Therefore, the Kruskal-Wallis test was used to check for significant differences, which returned a p-value of .0000 for both databases. To determine where the statistically significant differences came from, the Dunn test was used as a post-hoc test. The script used to find descriptive statistics # Normalize for population ---- gd_norm_country &lt;- gd_country %&gt;% mutate(cases_per_100000 = cases/population*100000) gf_norm_continent &lt;- gf_continent %&gt;% mutate(cases_per_100000 = cases/population*100000) # Mean and stdev ---- gd_summ &lt;- gd_norm_country %&gt;% group_by(country)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) gf_summ &lt;- gf_norm_continent %&gt;% group_by(continent)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) # Check normality ---- ## To check the normality, dataframes are made with the countries/continents as columns dengue_country &lt;- gd_norm_country %&gt;% select(country, cases_per_100000, year) %&gt;% pivot_wider(names_from = country, values_from = cases_per_100000) %&gt;% filter(year != 2002) flu_continent &lt;- gf_norm_continent %&gt;% select(continent, cases_per_100000, year) %&gt;% pivot_wider(names_from = continent, values_from = cases_per_100000) %&gt;% filter(year != 2002) ## Shapiro wilk for dengue stats_dengue_country &lt;- data.frame(country = colnames(dengue_country[,2:length(colnames(dengue_country))]), pvalue_shap = round(c(shapiro.test(dengue_country$Venezuela)$p.value, shapiro.test(dengue_country$Thailand)$p.value, shapiro.test(dengue_country$Singapore)$p.value, shapiro.test(dengue_country$Philippines)$p.value, shapiro.test(dengue_country$Mexico)$p.value, shapiro.test(dengue_country$Indonesia)$p.value, shapiro.test(dengue_country$India)$p.value, shapiro.test(dengue_country$Brazil)$p.value, shapiro.test(dengue_country$Bolivia)$p.value, shapiro.test(dengue_country$Argentina)$p.value), 4)) ## Shapiro wilk for flu stats_flu_continent &lt;- data.frame(continent = colnames(flu_continent[,2:length(colnames(flu_continent))]), pvalue_shap = round(c(shapiro.test(flu_continent$Oceania)$p.value, shapiro.test(flu_continent$Europe)$p.value, shapiro.test(flu_continent$Asia)$p.value, shapiro.test(flu_continent$Americas)$p.value, shapiro.test(flu_continent$Africa)$p.value), 4)) ## Normality results stats_dengue_country &lt;- stats_dengue_country %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) ## 5 out of 10 are normally distributed stats_flu_continent &lt;- stats_flu_continent %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) ## 3 out of 5 are normally distributed # Levene test ---- leveneTest(cases_per_100000 ~ country, data = gd_norm_country)[1,3] # P-value of .0000 ## [1] 3.28782e-07 leveneTest(cases_per_100000 ~ continent, data = gf_norm_continent)[1,3] # P-value of .0015 ## [1] 0.001455027 ## The variances are not equal # Kruskal Wallis test ---- kruskal.test(cases_per_100000 ~ country, data = gd_norm_country) ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by country ## Kruskal-Wallis chi-squared = 110.54, df = 9, p-value &lt; 2.2e-16 ## P-value of .0000, there is statistically significant difference kruskal.test(cases_per_100000 ~ continent, data = gf_norm_continent) ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by continent ## Kruskal-Wallis chi-squared = 51.754, df = 4, p-value = 1.553e-10 ## P-value of .0000, there is statistically significant difference # Post-Hoc Dunn test ---- dunn_dengue_country &lt;- dunnTest(cases_per_100000 ~ country, data = gd_norm_country, method = &quot;holm&quot;)$res dunn_flu_continent &lt;- dunnTest(cases_per_100000 ~ continent, data = gf_norm_continent, method = &quot;holm&quot;)$res # Significant differences ---- dunn_dengue_country &lt;- dunn_dengue_country %&gt;% mutate(different = P.adj&lt;0.05) dunn_flu_continent &lt;- dunn_flu_continent %&gt;% mutate(different = P.adj&lt;0.05) # Note the significant differences between countries/continents diff_dengue_country &lt;- dunn_dengue_country %&gt;% filter(different == TRUE) diff_flu_continent &lt;- dunn_flu_continent %&gt;% filter(different == TRUE) # Note the comparable countries/continents nondiff_dengue_country &lt;- dunn_dengue_country %&gt;% filter(different == FALSE) nondiff_flu_continent &lt;- dunn_flu_continent %&gt;% filter(different == FALSE) Seperate tables were made to show the combinations with significant difference and without. For the dengue data, 26 out of 45 combinations between the 10 countries were not significantly different in amount of dengue cases per week per 100.000 inhabitants. For the flu data, 3 out of 10 combinations between the 5 continents were not significantly different in amount of flu cases per week per 100.000 inhabitants. 4.5 Visualizations We can also create varying visualizations to get an overall look of how the countries/continents compare to each other and how the numbers changed through the years. 4.5.1 Dengue cases In figure 4.2 a scatterplot shows the average amount of weekly dengue cases per year with a different color for each country. Singapore has a clear higher number of cases for each year, except 2009. Singapore’s climate is a natural breeding ground for the dengue-carrying Aedes mosquitoes, which prefer a tropical climate according to Khetarpal and Khanna (2016). By the end of 2016 27% of outbreak-associated dengue cases reported in literature had been in Singapore, second to China with 27.9% (Guo et al. 2017). # Dengue scatterplot ---- ## Visualising the weekly dengue cases per country per year in a scatterplot plot_ly( data = gd_norm_country, x = ~year, y = ~cases_per_100000, type = &quot;scatter&quot;, mode = &quot;lines&quot;, color = ~country) %&gt;% layout( title = list( text = &quot;Dengue cases&quot;), legend = list( title = list(text = &quot;Countries&quot;)), xaxis = list( title = &quot;Year&quot;), yaxis = list( title = &quot;Average dengue cases per week per 100000 inhabitants&quot;)) Figure 4.2: Scatterplot to show the average amount of weekly dengue cases per 100.000 inhabitants per year per country. 4.5.2 Flu cases For the flu cases we will be comparing continents instead of countries, first comparing the average amount of weekly flu cases per 100.000 inhabitants per continent per year. In figure 4.3 a bar graph shows the average per continent with the standard deviation as error bars. Africa has a clear higher average of 5.2 per 100.000 inhabitants per week. In less tropical continents, like Europe and North America, most flu cases occur in the colder months. In more tropical continents, like Africa, they occur all throughout the year (Yazdanbakhsh and Kremsner 2009). Multiple factors like, access to health care, poor nutrition, chronic infections and indoor air pollution could contribute to the increase in flu cases (Ortiz et al. 2012). # Flu bargraph ---- ## Make a bar graph with the relative average population and weekly flu cases plot_ly( data = gf_summ, x = ~reorder(continent,-avg_cases_per_100000), y = ~avg_cases_per_100000, type = &quot;bar&quot;, text = ~round(avg_cases_per_100000,3), textposition = &quot;outside&quot;, color = ~continent, showlegend=FALSE, error_y = ~list(array = sd_cases_per_100000, color = &#39;#000000&#39;, thickness = 1) ) %&gt;% layout( title = list( text = &quot;Flu cases per continent&quot;), xaxis = list( title = &quot; &quot;), yaxis = list( title = &quot;Flu cases per week per 100.000 inhabitants&quot;)) Figure 4.3: The average amount of weekly flu cases per 100.000 inhabitants per continent from approx. 2003 to 2015. For further analysis a scatterplot has been made, shown in figure 4.4. One points which stands out for all continents except Oceania is the increase in 2009, only to go back down again in 2010. In 2009 there was a pandemic called the Swine Flu which might be the cause of this sudden jump in flu cases for all continents (Akin and Gözel 2020). This also explains why Oceania was affected less, as it is separated from other continents by oceans. # Flu scatterplot ---- ## Visualising the weekly dengue cases per continent per year in a scatterplot plot_ly( data = gf_norm_continent, x = ~year, y = ~cases_per_100000, type = &quot;scatter&quot;, mode = &quot;lines&quot;, color = ~continent) %&gt;% layout( title = list( text = &quot;Weekly flu cases across the world&quot;), legend = list( title = list(text = &quot;Continents&quot;)), xaxis = list( title = &quot;Year&quot;), yaxis = list( title = &quot;Average weekly flu cases per 100000 inhabitants&quot;)) Figure 4.4: Scatterplot to show the average amount of weekly flu cases per year per continent. "],["5_package_portrobbo.html", "Chapter 5 R package 5.1 The Demo 5.2 RMarkdown driven development 5.3 Package “portrobbo”", " Chapter 5 R package As part of assignment 8 from the DSFB2 Workflows course. For this assignment the previous assignments will be analysed to check for any repetitive code. This code will be turned into functions, combined into my first R package. 5.1 The Demo The Whole Game demo made by Hadley Wickham was followed to start creating the R package. This demo was a complete guide through setting up the package, connecting it to Git and writing the first function. It went over useful packages like roxygen2 and testthat. A short summary of packages and functions was made based on what I learned in the demo for later use. R package building cheatsheet Important libraries: library(devtools) library(usethis) library(roxygen2) library(testthat) How roxygen2 works: Put cursor in function Code -&gt; insert roxygen skeleton/li&gt;&gt; Fill in necessary details Important functions: create_package(“~/path/to/package/packaganame”) – Creates package use_git() – Connects to git use_r(“filename”) – Opens/creates script in R/ load_all() – Loads all functions without using globalenv exists(“function”, where = globalenv(), inherits = FALSE) – Checks if function exists in globalenv check() – Checks entire package for errors/warnings/notes use_mit_license() – Sets license to MIT document() – Creates/updates the manuals in man/ based on roxygen – Updates the NAMESPACE file install() – Installs the package use_testthat() – Declares our intent to write tests – Creates test directories use_test(“functionname”) – Opens/create a testfile test() – Performs all tests use_package(“packagename”) – Add a package to import in DESCRIPTION rename_files(“oldname”, “newname”) – Rename files use_readme_rmd() – Writes README template – Adds readme to the ignore file build_readme() – Updates the readme.md file based on the readme.rmd 5.2 RMarkdown driven development In the Workflows course the use of “Start with RMarkdown” was emphasized. This style of creating an R package does not start from zero like the demo suggested. It is based on a pre-written RMarkdown and the scripts that came with it. There are a lot of advantages to creating a complete all-in-one package when sharing a RMarkdown based analysis for others to reproduce. Usually, this sharing happens through an RMarkdown, some functions, the tidy version of the data, the steps used to get it tidy, the analysis itself and the used packages. These are multiple separate elements for someone to go over before they can reproduce the analysis. It can be made a lot more efficient by taking a few extra steps to create an R package from the RMarkdown. This way the elements are combined in one unit which is easily shared and can be used for multiple purposes and on different datasets. 5.3 Package “portrobbo” For this assignment we were asked to create a package which supports our portfolio. This will make the RMarkdowns more fluid and reduce duplicated code. It can also be used for later analysis. 5.3.1 Picking a name To find an available but still meaningful name I wanted to combine the word ‘portfolio’ with my partner’s name. My partner happens to work on ships so the name portrobbo (combining the words ‘portfolio’, ‘port’ and his name) came to mind. After checking the availability I decided to use that as the name for my first R package. After setting up the project for the package and the github repository it was ready for it’s first function. 5.3.2 An overview The package currently contains the following functions: uniq_val save_csv_rds graph_jitter protein visualizations: protein_cartoon protein_sphere protein_stick protein_visual The package also contains two data objects: COVID19, a dataframe containing COVID19 cases and deaths in multiple European countries. The data came from the ECDC. It was imported using this code and is used to show the functionality of uniq_val(), save_csv_rds() and graph_jitter(). Kinesin, an R object containing the PDBx file 3COB. The data came from the RCSB PDB. It was imported using this code and is used to show the functionality of the protein visualizing functions. 5.3.3 uniq_val The first function focuses on the data inspection. When working with a big dataset it can be used to check the amount of unique values within a column. For example, in chapter 7 we look at the dataset COVID19. The dataset has 28729 observations. Since there are multiple observations per country it is important to know how many countries are looked at. This is where uniq_val() comes in. The only input for the function is the column of which the amount of unique values should be checked. It’s use can be seen in chapter 7 or in the code chunk below. # The column with the names of the countries is countriesAndTerritories uniq_val(covid19$countriesAndTerritories) The 28729 observations are divided over 30 different countries. Good to know! There are other columns, also indicating a country or region. uniq_val() can also be used to check if those other columns show the same amount of unique values, by doing the following. # Another column indicating the country is geoId uniq_val(covid19$countriesAndTerritories) == uniq_val(covid19$geoId) 5.3.4 save_csv_rds After inspecting the data you might have made a few changes in the R object, like making it tidy or extracting data that is the most useful as shown in the following code. # Let&#39;s say we only want the data of summer 2020 for the Germany and France covid19_2020 &lt;- covid19 %&gt;% filter(countriesAndTerritories == c(&quot;Germany&quot;, &quot;France&quot;), year == 2020, month == c(6:8)) covid19_2020$dateRep &lt;- as.Date(covid19_2020$dateRep, &quot;%d/%m/%y&quot;) It is important to save your changes for later reference. A good way to do that is by using the save_csv_rds() function. This saves your dataframe as both a .csv and .rds file and uses the dataframe and an outputname as inputs. It’s use can be seen in chapter 5 or in the code chunk below. # In this case we want the outputname to be the same as the dataframe name save_csv_rds(covid19_2020, &quot;covid19_2020&quot;) 5.3.5 graph_jitter With the data inspected, extracted and saved, graphs can be made. The ggplot2 package contains all the necessary functions to make a graph. It’s functions have a lot of different input options for a lot of different graphs. This means the code for a simple graph can become quite long. This is where graph_jitter() comes in. The ggplot2 function position_jitter() uses multiple inputs to create a slight jitter to the datapoints. This is useful when data overlap makes the graph hard to read. The graph_jitter() function simplifies this function by only needing one input which will work for both the height and width of the jitter. It also sets the seed to keep jitter consistent throughout the analysis. In the following code chunk the use of graph_jitter() is shown. covid19_2020 %&gt;% ggplot(aes(x = dateRep, y = deaths)) + geom_line(aes(group=countriesAndTerritories, color = countriesAndTerritories)) + geom_point(aes(color = countriesAndTerritories), position = graph_jitter(0.3)) + labs(title = &quot;COVID19 in summer of 2020&quot;, x = &quot;Date&quot;, y = &quot;COVID19 related deaths&quot;) + theme_minimal() Figure 5.1: Scatterplot showing the COVID19 related deaths in summer of 2020 in Germany and France. 5.3.6 Protein visualization functions The last four functions use the r3dmol package to create 3D interactive visualizations of proteins. Each function shows a different style of visualization. They have been made to support the free assignment and their use can be seen in chapter 8. "],["6_Reproducibility.html", "Chapter 6 Reproducible Research 6.1 Excel’s downsides 6.2 Repita criteria 6.3 Open source code", " Chapter 6 Reproducible Research As part of assignment 1 from the DSFB2 Workflows course. 6.1 Excel’s downsides 6.1.1 Inspecting an Excel file As part of assignment 1.1 I was asked to review a file, by opening the file in Excel. The file contains the data of research done by J. Louter. This research appears to be about the effects of different substances on the reproduction of the C. elegans. The experimental condition ‘ControlVehicleA’ stood out to me. It appears to be the same as the ControlPositive but with less Ethanol. I’m guessing this shows the experimental conditions without the experimental compounds which would mean the compounds have been diluted in ethanol. There are also extra sheets with lists of inputs, logs of changes and input examples. I’m not sure what the difference in purpose is between the ExpLookup sheet and the Input example sheet. Three different compounds have been tested, 2,6-diisopropylnaphthalene, decane and naphthalene. The positive control for this experiment uses 1.5% ethanol in S-medium. The negative control for this experiment uses just S-medium without an added compound. Next up, the file was opened in R and the data types per column were checked. These differed from the expectations. # Read the excel file ---- CE.LIQ.FLOW.062_Tidydata &lt;- read_excel( &quot;data-raw/celegans/CE.LIQ.FLOW.062_Tidydata.xlsx&quot; ) The column RawData was recognized as a double while I’d expect an integer, compName was a character while I expected a factor and compConcentration was also character while it should be a double. It had not been assigned correctly. For further analysis the types were changed to the correct ones using the following code. # compConcentration to numeric ---- CE.LIQ.FLOW.062_Tidydata$compConcentration &lt;- as.double(CE.LIQ.FLOW.062_Tidydata$compConcentration) # RawData to integer ---- CE.LIQ.FLOW.062_Tidydata$RawData &lt;- as.integer(CE.LIQ.FLOW.062_Tidydata$RawData) # Change to tibble ---- CE.LIQ.FLOW.062_tbl &lt;- as.tibble(CE.LIQ.FLOW.062_Tidydata) # compName to factor ---- ## Creating a vector with the levels compName_factor &lt;- c( &quot;2,6-diisopropylnaphthalene&quot;, &quot;decane&quot;, &quot;naphthalene&quot;, &quot;Ethanol&quot;, &quot;S-medium&quot; ) ## Using those levels to create the factor CE.LIQ.FLOW.062_tbl$compName &lt;- factor(CE.LIQ.FLOW.062_tbl$compName, levels = compName_factor ) ## Check if it worked ## str(CE.LIQ.FLOW.062_tbl) # it did! 6.1.2 Creating the scatterplot Now that the columntypes are corrected the data can be used to create a scatterplot. The assignment asked to put the compConcentration on the x-axis, the DataRaw counts on the y-axis and to assign a colour to each level in compName. It also asked to assign a different symbol to each level in the expType variable. The controls have been added as dashed lines because they were measured in percentages, unlike the experimental conditions which were measure in nM. This also means the different shapes for expType variables are not neccessary since the only expType shown as points is “experiment”. In the assignment the first scatterplot made was followed by a question about the x-axis, “When creating the plot under C), what happened with the ordering of the x-axis labels. Explain why this happens.”. The compConcentration had already been changed to the correct type in the previous code chunk, so nothing went wrong when creating the first scatterplot. Had the type not been changed beforehand, the x-axis would have been in alphabetical order because they were seen as characters instead of doubles. The x-axis is shown with a log10 transformation and slight jitter has been added for better readability. A few details were added on top of the given assignment. This included each compound getting it’s own colour, the standard deviations being added as errorbars and the control values being added as dashed lines for comparison. The resulting plot is shown in figure 6.1. # Summarize data ---- ## Get the average counts per compound per concentration, keep the expType variable too CE.LIQ_summ &lt;- CE.LIQ.FLOW.062_tbl %&gt;% group_by(compName, compConcentration, expType) %&gt;% summarize( mean_counts = mean(RawData, na.rm = TRUE), stdev_counts = sd(RawData, na.rm = TRUE) ) # Extract data ---- ## Create a tibble containing only the data used for the graph CE.LIQ_summ_exp &lt;- CE.LIQ_summ %&gt;% filter(expType == &quot;experiment&quot;) # Create the scatterplot ---- ggplotly(CE.LIQ_summ_exp %&gt;% ggplot(aes( x = log10(compConcentration) ## log10 as suggested in question F , y = mean_counts )) + geom_smooth(aes(group = compName, color = compName), span = .5, size = 0.2) + geom_point(aes(color = compName), position = graph_jitter(0.2) ) + geom_errorbar( aes( ymin = mean_counts - stdev_counts, ymax = mean_counts + stdev_counts, color = compName ), width = .1, position = graph_jitter(0.2) ) + labs( title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count&quot;, x = &quot;Compound concentration in log10(nM)&quot;, fill = &quot;Compounds&quot; ) + geom_hline(yintercept = 85.9, linetype = &quot;dashed&quot;, size = 0.2) + ## Shows control negative geom_text(aes(0.3, 88, label = &quot;Control negative&quot;, vjust = -0.3), size = 3.5) + geom_hline(yintercept = 49.4, linetype = &quot;dashed&quot;, size = 0.2) + ## Shows control positive geom_text(aes(-3.5, 51.5, label = &quot;Control positive&quot;, vjust = -0.3), size = 3.5) + theme_minimal()) %&gt;% layout( title = list( text = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70) ), legend = list(title = list(text = &quot;Compounds&quot;)), xaxis = list( title = &quot;Compound concentration in log10(nM)&quot; ), yaxis = list( title = &quot;Offspring count&quot; ) ) Figure 6.1: Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations. 6.1.3 Normalize for the negative control In order to see how much the offspring count improved or decreased relatively to the baseline (in this case, the negative control) the data must be normalized. To do this we adjust the negative control to a value of 1 and adjust the other values in the same way. In this case that means dividing by 85.9. The scatterplot was created again with the new normalized data, further settings stayed the same, the result is shown in figure 6.2. # Normalize for negative control ---- ## Check the mean value of controlNegative ## view(CE.LIQ_summ) # It&#39;s 85.9 CE.LIQ_summ_norm &lt;- CE.LIQ_summ_exp %&gt;% mutate( norm_counts = mean_counts / 85.9, norm_stdev = stdev_counts / 85.9 ) # Create normalized plot ---- ggplotly(CE.LIQ_summ_norm %&gt;% ggplot(aes( x = log10(compConcentration), ## log10 as requested in question F y = norm_counts )) + geom_smooth(aes(group = compName, color = compName), span = .5, size = 0.2) + geom_point(aes(color = compName), position = graph_jitter(0.04)) + geom_errorbar(aes( ymin = norm_counts - norm_stdev, ymax = norm_counts + norm_stdev, color = compName ), width = .1, position = graph_jitter(0.04)) + labs( title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count relative to negative control&quot;, x = &quot;Compound concentration in log10(nM)&quot; ) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, size = 0.2) + ## Show control negative geom_text(aes(0.3, 1.02, label = &quot;Control negative&quot;, vjust = -0.3), size = 3.5) + geom_hline(yintercept = 0.57508731, linetype = &quot;dashed&quot;, size = 0.2) + ## Show control positive geom_text(aes(-3.5, 0.6, label = &quot;Control positive&quot;, vjust = -0.3), size = 3.5) + theme_minimal()) %&gt;% layout( title = list( text = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70) ), legend = list(title = list(text = &quot;Compounds&quot;)), xaxis = list( title = &quot;Compound concentration in log10(nM)&quot; ), yaxis = list( title = &quot;Offspring count&quot; ) ) Figure 6.2: Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations. Normalized to negative control = 1. 6.1.4 Statistical analysis In order to learn wether there is indeed an effect of the different compounds a few tests need to be performed. Starting with the Shapiro-Wilk test to check the normality of the data and a Levene’s test to check for equality of variance. Followed by an ANOVA with the different concentrations as added variable. To see the actual difference, the analysis should be finished with a Post hoc test like Tukey. 6.2 Repita criteria 6.2.1 Criteria for reproducibility This first part of assignment 1.2 will look at ‘Repita’ criteria. These criteria are used to check for the reproducibility of scientific research articles. In table 6.1 the different criteria are shown with a definition and the type of response it calls for. In order to further understand and apply these criteria, a research article will be analysed to check if this article follows the ‘Repita’ guidelines as shown in table 6.1. # Create variables ---- Transparency_Criteria &lt;- c( &quot;Study Purpose&quot;, &quot;Data Availability Statement&quot;, &quot;Data Location&quot;, &quot;Study Location&quot;, &quot;Author Review&quot;, &quot;Ethics Statement&quot;, &quot;Funding Statement&quot;, &quot;Code Availability&quot; ) Definition &lt;- c( &quot;A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.&quot;, &quot;A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.&quot;, &quot;Where the article’s data can be accessed, either raw or processed.&quot;, &quot;Author has stated in the methods section where the study took place or the data’s country/region of origin.&quot;, &quot;The professionalism of the contact information that the author has provided in the manuscript.&quot;, &quot;A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.&quot;, &quot;A statement within the manuscript indicating whether or not the authors received funding for their research.&quot;, &quot;Authors have shared access to the most updated code that they used in their study, including code used for analysis.&quot; ) Response_Type &lt;- c( &quot;Binary&quot;, &quot;Binary&quot;, &quot;Found Value&quot;, &quot;Binary;Found Value&quot;, &quot;Found Value&quot;, &quot;Binary&quot;, &quot;Binary&quot;, &quot;Binary&quot; ) # Create dataframe ---- repita_criteria &lt;- data.frame( Transparency_Criteria, Definition, Response_Type ) # Create table ---- repita_criteria %&gt;% kbl( col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;The repita criteria as given in portfolio assignment 1.2 in [lesson 1 of the reader](https://lesmaterialen.rstudio.hu.nl/workflows-reader/represintro.html).&quot; ) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, fixed_thead = T) %&gt;% column_spec(1, width = &quot;13em&quot;) %&gt;% column_spec(3, width = &quot;10em&quot;) %&gt;% row_spec(0, background = &quot;#faf4ec&quot;) Table 6.1: The repita criteria as given in portfolio assignment 1.2 in lesson 1 of the reader. Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary;Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary 6.2.2 The article Plosone was used to find a suitable article. The found article is a primary article describing emperical scientific findings by Bruckner et al. (2022). The focus is on how microbiotas play a role in developmental programs. Altered microbiota composition appears to be linked to neurodevelopmental conditions such as autism spectrum disorder. One of the findings described how the microbiota can influence forebrain neurons. Forebrain neurons are required for normal social behavior and localization of forebrain microglia. The study uses zebrafish which were kept at 28°C with a 14/10 light/dark cycle. For the controlgroup conventionalized (CVZ) fish were used. The experimental condition consisted of germ-free (GF) fish. These fish were made to be germ-free and therefore did not have their microbiota. When the fish were older (7 dpf) they were inoculated with normal microbiota (making them ex germ-free or XGF) and their optomotor respons was tested. Once the fish were adults (14 dpf) their social behavior was tested. The social behavior was assessed through a dyad assay for postflexion larval and adult zebrafish. For each condition a pair of siblings were placed in isolated tanks and allowed to interact for 10 minutes via transparent tank walls. Social interaction was defined as the average relative distance from the divider and the percentage of time spent orienting at 45° to 90°. These parameters were measured and analyzed using computer vision software written in Python (available at https://github.com/stednitzs/daniopen). Compared to the CVZ siblings, the GF larvae spent significantly less time in close proximity to and oriented at 45 to 90°. These results show that an intact microbiota is required during the early life-stages for later development of normal social behavior. Aside from the social behavior, the study also looked at optomotor responses. It is possible that the microbiota influences circuitry underlying the early vision and locomotion required for social behavior. To address this possibility, the vision and locomotion were assayed by comparing kinetics of the optomotor response to virtual motion in GF larvae and CVZ controls. Optomotor response was assessed using a “virtual reality” system for assessing zebrafish behavior, measuring swim response in larvae. A visual stimulus was projected on a screen underneath the dishes for 20 seconds and consisted of concentric rings moving toward the dish center, followed by a 20-second refractory period. Responses are the average of 46 to 59 stimulus trials per fish, presented over 1 hour. In this part of the study, no significant difference were found between the GF and CVZ fish. This suggests that the microbiota influences circuits specific to social behavior directly, rather than by modulating vision or locomotion. The results demonstrate that the microbiota influences zebrafish social behavior by stimulating microglial remodeling of forebrain circuits during early neurodevelopment. This conclusion suggests pathways for new interventions in multiple neurodevelopmental disorders. 6.2.3 The reproducibility To better understand these criteria (table 6.1) they will be applied to the found article. The results can be found in table 6.2. While answering this assignment I noticed how this is not as black and white as I thought. For example with the study location. At first I thought it was stated, because University of Oregon came back quite a few times, looking at it more closely it was not clear if that was the actual study location. # Create variables ---- Score &lt;- c( &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;[Link](https://figshare.com/projects/Bruckner_et_al_Data/136756)&quot;, &quot;FALSE&quot;, &quot;Non professional&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot; ) Explanation &lt;- c( &quot;In the second to last paragraph of the introduction the study objective is described.&quot;, &quot;On the left-hand side of the webpage the \\&quot;Accesibble Data\\&quot; can be found&quot;, &quot;The data can be accessed through the given link, found under the \\&quot;Accesibble Data\\&quot; header.&quot;, &quot;The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. &quot;, &quot;Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given.&quot;, &quot;There is an ethics statement in the methods paragraph.&quot;, &quot;The funding is stated under the abstract.&quot;, &quot;The data analysis is explained but the code has not been shared.&quot; ) # Create dataframe ---- article_repita_check &lt;- data.frame(Transparency_Criteria, Score, Explanation) # Create table ---- article_repita_check %&gt;% kbl( col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;The score for each repita criterium when looking at the research done by Bruckner et al. (2022)&quot; ) %&gt;% kable_styling( bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;, &quot;responsive&quot;), font_size = 12, fixed_thead = T ) %&gt;% column_spec(1, width = &quot;12em&quot;) %&gt;% column_spec(2, width = &quot;10em&quot;) %&gt;% row_spec(0, background = &quot;#faf4ec&quot;) Table 6.2: The score for each repita criterium when looking at the research done by Bruckner et al. (2022) Transparency Criteria Definition Response Type Study Purpose TRUE In the second to last paragraph of the introduction the study objective is described. Data Availability Statement TRUE On the left-hand side of the webpage the “Accesibble Data” can be found Data Location Link The data can be accessed through the given link, found under the “Accesibble Data” header. Study Location FALSE The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. Author Review Non professional Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given. Ethics Statement TRUE There is an ethics statement in the methods paragraph. Funding Statement TRUE The funding is stated under the abstract. Code Availability FALSE The data analysis is explained but the code has not been shared. 6.3 Open source code In this second part of assignment 1.2 we’ll be looking at the code from an existing study. The focus will be on understanding the code, noting what it tries to achieve, the readability, reproducibility, fixing errors, etc. 6.3.1 The article The article used for this assignment had to live up to a few criteria. First up, the data had to be analysed using R code. This code and the dataset had to be available. The chosen article is “Mental Health Impacts in Argentinean College Students During COVID-19 Quarantine” by López Steinmetz et al. (2021). The study aimed to analyze differences in mental health in college students who were exposed to different spread-rates of COVID-19. They also wanted to analyze the between group differences in mental health indicators at four different quarantine sub-periods. To do this a cross-sectional design was used. The sample included 2687 Argentinean college students and the data was collected online during the Argentinean quarantine. They used a one-way between-groups ANOVA with Tukey’s post hoc test for the analysis. The results showed that the center and most populated area only differed in psychological well-being and negative alcohol related consequences, but not in the other indicators. For the sub-periods there were differences in psychological well-being, social functioning and coping, psychological distress and negative alcohol-related consequences. Negative alcohol-related consequences were the only MHS indicator improving over time. This worsened mental health suggests that quarantine and its extensions contribute to negative mental health impacts. 6.3.2 The code The code, as well as the dataset, for the study were made available through OSF. The original can be found here and the version with my adjustments can be found at the bottom of this page. The dataset and the code were added to the repository as well. After reading the code it appears to consist of a few parts. Reference to the manuscript Loading the data and packages Methods in general Sample size Distribution by sex, also noted in percentages Mean, median and stdev of the age of the students Distribution by province in percentages Methods for data analysis Test of skewness per mental health indicator Test or Kurtosis per mental health indicator with criteria and criteria reference Division based on region for first aim Division based on sub-periods for second aim Results for aim 1 (differences in MHS based on region) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per region Results for aim 2 (differences in MHS based on sub-period) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per sub-period Plot with means per sub-period and 95% confidence interval The code is used to make the data clear and tidy by first noting details like sample size, distributions in percentages and tests of skewness. Then it moves on to perform statistical tests for the two different aims of the study. It shows the significant differences and creates plots for the second aim. It was very easy to read the code, I would give it a 5/5 on readability. It was made especially easy by the clear headers and comments. 6.3.3 Excuting the code To further inspect the code it will be executed on the dataset given by the study. To do this one word in the code was changed in order to link the dataset. The word “clipboard” under comment “Load the dataset” was changed into raw-data/reproducibility/dataset.xlsx, the location of the downloaded dataset. Errors will be noted and the code will be changed accordingly. 6.3.3.1 Changes and errors when executing The original code uses read.table and the clipboard to load the data. This can very quickly be done wrong. To fix this the readxl package was added and the excel file was added using table&lt;-read_excel(“data-raw/reproducibility/dataset.xlsx”). “Error in model.frame.default(formula = table$PSYCH.WELLBEING ~ table$REGIONS,:invalid type (NULL) for variable ’table$PSYCH.WELLBEING_. In the table the column”PSYCH WELLBEING” uses a space instead of a period. This also happens for other column names. To change this colnames(table) &lt;- str_replace_all(colnames(table), ” “,”.”) was added before testing skewness. The rest of the code stayed the same. To make the knitting of this rmarkdown easier the code has been adjusted to only look at “PSYCH.WELLBEING” as an example. All other indicators would have been analysed in the exact same manner. 6.3.4 Effort scoring Thanks to the great readability and consistency of the code it was very easy to reproduce. The only issue was adjusting the columnnames to use periods instead of spaces which was easily fixed. Therefore the score is a 5/5 on reproducibility. The code including my adjustments # R Code for the manuscript entitled: # &quot;Mental health impacts in Argentinean college students during COVID-19 quarantine&quot;. # López Steinmetz L.C., Leyes C.A., Dutto Florio M.A., Fong S.B., López Steinmetz R.L. &amp; Godoy J.C. ########################################################################## ################## LOAD THE DATASET &amp; PACKAGES ########################### ########################################################################## # Load the dataset table &lt;- read_excel(&quot;data-raw/reproducibility/dataset.xlsx&quot;) # Changed by Mirthe Klaassen for this portfolio assignment summary(table) ## SUB PERIODS IN PRE AND POST REGIONS PROVINCE ## Length:2687 Length:2687 Length:2687 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## SEX AGE PSYCH WELLBEING SOC FUNC AND COPING ## Length:2687 Min. :18.00 Min. :0.000 Min. :0.000 ## Class :character 1st Qu.:20.00 1st Qu.:2.000 1st Qu.:0.000 ## Mode :character Median :22.00 Median :3.000 Median :2.000 ## Mean :22.74 Mean :3.086 Mean :2.149 ## 3rd Qu.:24.00 3rd Qu.:5.000 3rd Qu.:4.000 ## Max. :39.00 Max. :6.000 Max. :6.000 ## K10 BDI STAIR YAACQ ISO ## Min. :10.0 Min. : 0 Min. : 1.00 Min. : 0.000 Min. : 1.00 ## 1st Qu.:19.0 1st Qu.:10 1st Qu.:21.00 1st Qu.: 0.000 1st Qu.:22.00 ## Median :25.0 Median :16 Median :29.00 Median : 3.000 Median :32.00 ## Mean :25.5 Mean :18 Mean :29.24 Mean : 3.699 Mean :34.56 ## 3rd Qu.:32.0 3rd Qu.:25 3rd Qu.:38.00 3rd Qu.: 6.000 3rd Qu.:46.00 ## Max. :49.0 Max. :57 Max. :58.00 Max. :21.000 Max. :82.00 # Load the packages: library(moments) library(gplots) library(readxl) ## Added by Mirthe Klaassen for this portfolio assignment ########################################################################## ###################### METHODS ########################################### ########################################################################## ###### SUB-TITLE: METHOD &gt; Sample and procedure # SAMPLE N = 2687 # Distribution by sex: table(table$SEX) ## ## man other woman ## 473 22 2192 # Absolute frequencies: Women = 2192, Men = 473, Other = 22 prop.table(table(table$SEX)) * 100 ## ## man other woman ## 17.603275 0.818757 81.577968 # Percentages: Women = 81.577968%, Men = 17.603275%, Other = 0.818757% # Central tendency measures by age (total sample) # mean mean(table$AGE) ## [1] 22.74023 # Mean age = 22.74023 # standard deviation sd(table$AGE) ## [1] 3.635612 # sd age = 3.635612 # median median(table$AGE) ## [1] 22 # median age = 22 # Distribution by provinces prop.table(table(table$PROVINCE)) * 100 ## ## CABA CBA JUJ PCIAB SAL STACR TDELF ## 11.9464086 39.0026051 6.6989207 31.9315221 7.1082992 0.9676219 2.3446223 # JUJ (JUJUY) = 6.6989207% # SAL (SALTA) = 7.1082992% # CBA (CÓRDOBA) = 39.0026051% # STACR (SANTA CRUZ) = 0.9676219% # TDELF (TIERRA DEL FUEGO) = 2.3446223% # CABA (CIUDAD AUTÓNOMA DE BUENOS AIRES) = 11.9464086% # PCIAB (PROVINCIA DE BUENOS AIRES) = 31.9315221% ###### SUB-TITLE: METHOD &gt; Data analysis # Adjustment of colnames to be the same as the names in the code == added by Mirthe Klaassen for this assignment colnames(table) &lt;- str_replace_all(colnames(table), &quot; &quot;, &quot;.&quot;) ### To test Skewness and Kurtosis # Criteria: range of acceptable values or near to (-3 and +3; Brown, 2006). # Reference: Brown T.A. (2006). Confirmatory factor analysis for applied research. New York: Guilford Press. # PSYCH.WELLBEING skewness(table$PSYCH.WELLBEING) ## [1] -0.05214941 # skewness PSYCH.WELLBEING = -0.05214941 kurtosis(table$PSYCH.WELLBEING) ## [1] 1.951112 # kurtosis PSYCH.WELLBEING = 1.951112 ### For analyses corresponding to the first aim, we divided the entire sample into four groups: table(table$REGIONS) ## ## CENTER MOST POPULATED NORTH SOUTH ## 1048 1179 371 89 # NORTH = 371 # CENTER = 1048 # SOUTH = 89 # MOST POPULATED = 1179 ### For analyses corresponding to the second aim, we divided the entire sample into four groups: table(table$SUB.PERIODS.IN.PRE.AND.POST) ## ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1508 525 364 ## 4. REMAINING WEEKS POST ## 290 # first week pre-quarantine extension (ONE WEEK PRE) = 1508 # second week pre-quarantine extension (TWO WEEK PRE) = 525 # first week post-quarantine extension (ONE WEEK POST) = 364 # remaining weeks post-quarantine extension (REMAINING WEEKS POST) = 290 ########################################################################## ###################### RESULTS ########################################### ########################################################################## ########################################################################## ####################### AIM 1 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four regions table$PSYCH.WELLBEING ## [1] 6 2 3 4 0 4 0 4 2 2 2 3 0 4 4 5 3 6 3 5 2 6 5 5 5 2 3 6 6 3 4 4 2 1 4 0 2 ## [38] 5 0 1 4 5 4 1 6 3 5 5 0 5 5 6 3 3 5 4 3 4 0 6 4 1 5 2 2 1 5 2 3 4 4 4 1 2 ## [75] 4 1 5 4 5 6 1 3 2 4 2 2 2 6 5 2 2 2 4 5 4 4 4 5 6 3 3 2 2 2 4 5 1 1 5 0 0 ## [112] 5 2 2 5 3 3 1 0 4 4 2 6 1 6 4 1 2 3 4 4 4 6 1 2 0 1 3 6 2 6 6 5 2 6 4 4 2 ## [149] 3 3 2 4 3 5 1 2 0 4 3 2 1 3 6 1 5 0 2 4 2 1 1 3 1 1 5 4 4 1 1 0 6 3 1 5 5 ## [186] 0 1 3 3 0 3 6 1 0 1 3 1 3 4 2 4 2 3 0 3 4 5 1 4 2 3 5 2 2 6 5 1 1 1 1 5 1 ## [223] 0 0 1 0 0 3 6 6 1 4 3 3 2 0 3 2 1 6 6 2 5 0 3 5 5 2 2 4 4 1 6 0 1 6 2 1 5 ## [260] 1 0 0 4 5 2 4 5 6 6 5 3 5 1 1 5 5 5 4 1 1 1 2 3 6 5 1 0 6 3 1 1 1 3 2 4 6 ## [297] 3 3 1 2 4 5 3 2 4 4 4 6 3 2 5 3 3 1 5 6 6 5 5 2 2 2 2 0 4 4 4 2 2 5 4 0 4 ## [334] 4 0 1 0 5 1 3 0 0 4 6 2 4 0 1 4 3 4 2 1 3 5 4 2 6 0 0 4 1 5 6 3 0 2 5 2 6 ## [371] 5 2 3 6 3 4 3 5 2 5 2 1 3 4 5 5 1 4 0 0 0 0 3 2 3 6 2 3 5 0 3 6 1 2 6 2 5 ## [408] 4 2 4 1 4 6 2 3 4 2 1 4 2 2 2 6 3 3 0 4 1 0 0 0 4 5 2 2 1 5 0 4 3 4 1 4 3 ## [445] 5 5 3 1 0 2 2 1 3 2 1 5 1 3 0 6 3 4 4 3 6 2 2 3 6 0 4 0 2 1 1 5 4 5 3 0 1 ## [482] 3 6 4 1 4 4 2 6 6 5 3 4 2 0 6 0 0 5 0 0 5 0 3 1 3 2 0 3 2 1 4 1 2 4 3 2 6 ## [519] 4 4 0 2 4 3 4 2 4 1 3 3 2 4 4 3 0 5 2 0 1 4 0 3 0 2 2 4 3 4 5 4 6 2 2 4 5 ## [556] 2 2 2 6 2 2 4 3 6 6 4 6 5 1 1 4 3 0 1 4 4 1 2 4 0 4 1 2 6 0 2 1 3 1 4 0 1 ## [593] 3 3 3 4 6 4 0 3 5 3 2 3 1 3 0 3 0 2 0 3 4 2 2 2 4 2 6 6 3 5 6 1 3 1 2 6 4 ## [630] 0 3 0 2 3 5 2 4 0 5 6 4 3 4 5 4 2 2 1 0 1 6 1 4 2 4 2 5 4 3 6 3 2 3 1 5 2 ## [667] 3 1 2 0 6 0 4 0 3 4 4 0 3 2 0 2 4 3 2 3 1 2 0 1 3 0 0 5 0 2 4 2 6 3 3 0 5 ## [704] 0 5 3 2 3 4 1 2 1 5 4 5 4 5 2 4 5 5 6 4 5 3 0 5 4 4 4 6 6 6 3 6 3 4 3 3 6 ## [741] 5 6 5 4 5 5 2 5 5 4 2 5 6 4 3 2 1 3 5 0 5 5 6 2 1 4 3 0 3 1 6 5 4 2 6 1 3 ## [778] 3 1 4 1 4 3 4 6 2 2 5 5 4 3 0 4 3 2 3 2 0 5 2 0 4 1 0 2 1 6 2 1 5 0 3 5 4 ## [815] 0 0 4 4 1 1 1 5 5 3 2 0 6 2 4 3 1 4 2 2 6 5 0 1 1 5 3 2 2 3 4 1 4 4 4 3 2 ## [852] 4 2 2 5 6 6 6 3 0 5 4 4 1 5 0 5 6 1 2 3 2 3 6 2 5 1 2 3 4 2 5 4 2 4 3 1 0 ## [889] 0 1 5 3 3 6 3 0 2 3 0 5 5 2 4 6 4 3 6 0 2 0 6 5 2 6 1 4 4 3 1 5 2 0 4 0 4 ## [926] 4 1 4 2 4 5 0 3 3 4 3 6 4 4 3 1 3 6 5 1 0 3 5 1 2 2 3 1 1 1 1 3 4 5 0 1 0 ## [963] 1 5 4 4 1 3 4 2 5 2 3 3 2 1 6 6 4 4 2 5 3 3 4 6 2 3 3 0 2 6 4 5 6 3 6 6 1 ## [1000] 5 2 4 1 1 5 1 3 2 5 3 1 0 5 1 5 3 5 4 2 1 2 2 6 0 3 0 4 6 4 3 0 3 3 3 1 1 ## [1037] 4 3 3 4 0 4 4 2 5 5 0 4 4 0 2 0 6 1 3 3 6 0 0 3 2 3 6 3 3 6 1 4 3 2 5 4 4 ## [1074] 1 5 2 3 2 2 5 5 5 4 5 6 3 3 4 3 6 2 6 5 5 1 0 3 3 1 6 2 4 5 5 3 4 4 0 3 0 ## [1111] 0 3 3 4 3 1 1 2 1 4 1 4 0 2 1 3 2 2 6 1 3 2 1 3 2 1 2 1 2 0 3 4 1 6 1 3 2 ## [1148] 5 2 4 1 3 1 0 0 2 1 3 4 0 0 4 1 5 4 3 2 4 5 4 5 5 3 3 6 1 0 4 2 3 4 2 2 0 ## [1185] 5 4 6 1 2 6 1 0 5 3 5 3 4 3 2 1 5 5 5 2 5 3 1 5 6 6 4 3 3 1 3 3 6 6 3 4 4 ## [1222] 2 3 0 4 2 0 4 4 1 2 2 2 3 1 0 1 6 5 1 5 3 6 6 1 4 1 6 4 1 1 4 2 2 3 5 0 1 ## [1259] 5 4 6 2 0 0 3 1 3 4 6 0 4 2 3 0 3 2 6 6 0 2 0 5 0 2 4 4 5 2 4 1 3 4 5 4 3 ## [1296] 6 3 4 0 3 4 4 2 0 4 2 5 3 5 1 0 6 0 0 2 6 2 0 1 1 3 1 0 4 5 3 1 2 4 3 2 1 ## [1333] 0 4 5 1 2 6 3 3 3 4 4 3 0 4 6 2 2 3 6 6 1 4 1 5 3 6 4 2 3 6 3 6 4 2 1 6 1 ## [1370] 3 2 1 1 4 3 1 4 4 5 3 6 0 5 1 4 5 0 0 1 3 2 0 1 2 2 3 5 3 4 3 6 2 4 0 0 0 ## [1407] 3 3 1 2 3 3 2 2 5 3 2 1 0 4 5 3 6 2 3 2 0 0 0 0 6 5 2 2 5 4 2 4 5 0 5 5 1 ## [1444] 2 1 2 6 2 4 0 6 0 3 4 3 6 0 2 3 2 4 1 4 0 2 5 1 4 4 2 2 2 4 3 0 6 3 0 3 5 ## [1481] 1 5 3 2 2 6 0 2 4 6 2 3 6 4 3 3 2 3 1 3 4 6 6 5 3 1 6 5 3 6 3 6 2 6 2 0 0 ## [1518] 4 2 1 2 3 1 0 5 2 2 3 3 6 4 1 5 3 2 3 2 5 1 6 3 1 0 3 6 2 2 4 6 6 4 0 1 5 ## [1555] 2 3 5 3 5 6 5 5 3 1 4 5 3 6 5 6 5 1 0 1 4 5 5 4 6 4 2 2 2 5 1 6 5 0 4 2 4 ## [1592] 5 4 3 3 4 0 3 2 0 3 1 4 0 6 4 2 5 2 1 3 2 2 3 1 4 1 1 5 6 6 5 5 3 6 1 0 3 ## [1629] 3 4 4 2 0 6 3 0 4 6 2 6 3 5 2 3 6 6 0 2 3 2 1 6 2 1 4 2 2 4 2 5 5 5 2 4 4 ## [1666] 5 1 0 2 3 5 4 2 6 2 4 1 3 2 2 4 1 3 5 3 3 4 5 6 3 6 4 1 1 4 6 3 2 5 3 3 0 ## [1703] 3 3 3 6 1 4 4 5 6 0 3 5 5 5 4 6 3 1 5 5 2 2 6 6 3 2 2 5 2 6 2 3 5 3 1 6 2 ## [1740] 6 5 5 6 1 6 1 2 3 1 3 4 5 4 1 4 3 2 5 5 3 3 1 1 2 3 5 6 2 4 4 4 1 1 6 2 4 ## [1777] 3 3 4 4 2 3 3 1 2 5 4 5 5 2 4 2 5 1 6 1 5 1 6 2 3 0 2 3 3 0 5 6 1 1 3 3 3 ## [1814] 3 4 4 2 5 3 4 4 2 5 2 0 2 6 0 5 5 6 1 4 5 0 5 5 5 5 1 3 1 2 4 5 4 2 6 3 4 ## [1851] 1 3 3 0 0 5 4 6 2 6 4 5 3 4 5 1 6 5 3 1 3 3 4 5 1 2 4 3 3 6 2 2 0 0 2 2 6 ## [1888] 3 6 3 6 4 0 4 5 3 3 1 0 5 4 2 4 4 2 4 4 0 4 5 4 0 2 1 2 1 1 2 0 3 0 6 2 0 ## [1925] 3 2 5 5 3 5 1 1 6 3 5 4 5 6 3 5 5 5 1 0 3 3 4 6 3 5 4 3 4 0 6 0 2 1 3 3 3 ## [1962] 3 4 2 0 4 4 6 5 1 3 0 2 3 5 3 2 3 3 1 4 5 3 3 2 3 6 4 2 1 6 0 4 2 0 4 5 2 ## [1999] 4 2 2 3 2 4 0 4 1 1 3 4 3 4 3 3 0 4 0 5 5 4 1 1 3 0 4 2 1 5 0 1 1 4 4 5 5 ## [2036] 4 3 1 5 3 1 6 4 5 4 0 5 3 2 6 2 6 6 5 1 1 0 3 2 2 4 2 0 3 3 3 5 3 1 5 6 3 ## [2073] 1 2 3 5 4 0 0 6 5 3 2 6 6 3 2 4 6 1 1 5 2 6 6 5 4 4 5 0 0 4 5 1 3 2 4 3 3 ## [2110] 4 6 6 3 0 2 4 6 0 1 0 5 1 5 0 3 3 6 5 4 6 5 5 3 5 2 1 6 0 3 0 4 6 4 2 2 2 ## [2147] 2 1 2 5 3 3 6 5 3 4 5 1 5 5 0 0 5 2 6 6 1 3 2 6 5 5 1 3 6 3 4 1 1 2 6 5 3 ## [2184] 2 5 4 5 4 4 1 5 5 6 6 6 5 2 2 3 3 6 2 1 1 5 3 5 0 3 6 4 3 0 5 5 1 5 6 1 6 ## [2221] 2 2 2 2 2 5 3 3 1 2 1 0 1 3 2 3 4 5 4 0 3 5 4 5 6 3 6 5 2 3 6 2 5 6 2 5 2 ## [2258] 4 5 6 5 3 2 0 0 6 2 1 1 5 6 4 4 6 3 1 5 6 2 2 5 4 3 2 6 0 2 4 1 0 4 6 3 5 ## [2295] 4 2 1 3 3 0 3 5 6 6 5 2 4 6 4 5 6 6 5 2 4 4 4 4 4 3 1 5 6 3 4 3 6 1 6 4 5 ## [2332] 1 3 5 3 2 3 2 3 3 3 5 0 4 4 4 6 6 5 3 0 4 2 1 5 4 5 3 5 4 5 6 0 4 2 5 2 5 ## [2369] 3 5 0 3 3 2 3 2 2 2 3 0 3 5 6 5 5 6 2 4 4 6 2 6 6 1 3 1 3 4 3 2 3 2 5 4 4 ## [2406] 6 4 5 2 3 6 1 2 2 5 1 1 6 3 4 2 5 5 4 4 2 3 0 5 2 5 1 1 1 2 6 1 1 4 5 3 3 ## [2443] 1 0 4 6 3 5 1 2 6 2 1 1 4 4 5 3 0 5 5 1 4 2 5 2 0 1 4 2 4 5 2 4 1 5 5 2 1 ## [2480] 6 1 6 1 3 1 2 4 5 0 3 4 5 3 5 1 4 0 0 6 4 5 6 0 4 4 2 5 4 2 4 6 3 2 2 2 0 ## [2517] 4 2 6 3 3 6 6 3 4 2 3 5 3 4 5 1 0 5 0 2 5 3 4 5 0 5 5 1 4 6 6 6 5 4 5 2 1 ## [2554] 5 1 0 4 5 4 4 6 3 3 6 2 4 5 2 6 2 2 6 2 4 3 6 1 3 3 4 6 2 5 5 6 0 2 2 5 3 ## [2591] 3 6 1 2 6 3 2 3 0 2 3 1 4 3 3 2 4 4 2 1 5 5 3 3 3 2 5 2 0 4 1 3 3 3 6 0 1 ## [2628] 1 4 1 2 1 2 6 6 2 2 3 5 2 2 1 4 2 5 4 4 6 4 4 0 4 5 4 5 5 5 6 5 5 5 3 5 3 ## [2665] 6 5 2 3 3 4 5 4 5 5 5 4 5 2 5 3 3 5 5 4 6 4 0 # PSYCHOLOGICAL WELL-BEING/DISCOMFORT (OF GENERAL HEALTH) anovaregpsychwellbeing &lt;- aov(table$PSYCH.WELLBEING ~ table$REGIONS) summary(anovaregpsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$REGIONS 3 46 15.340 4.569 0.00338 ** ## Residuals 2683 9008 3.358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovaregpsychwellbeing) TukeyHSD(anovaregpsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$REGIONS) ## ## $`table$REGIONS` ## diff lwr upr p adj ## MOST POPULATED-CENTER 0.28148855 0.08151381 0.4814633 0.0017158 ## NORTH-CENTER 0.22902049 -0.05554103 0.5135820 0.1636105 ## SOUTH-CENTER 0.17650527 -0.34355917 0.6965697 0.8191275 ## NORTH-MOST POPULATED -0.05246806 -0.33286583 0.2279297 0.9633105 ## SOUTH-MOST POPULATED -0.10498327 -0.62278119 0.4128146 0.9540137 ## SOUTH-NORTH -0.05251522 -0.60848288 0.5034524 0.9949688 plot(TukeyHSD(anovaregpsychwellbeing)) # significant differences # MOST POPULATED-CENTER p adj 0.0017158 #### MOST POPULATED mean = 3.206107, CENTER mean = 2.924618 tapply(table$PSYCH.WELLBEING, factor(table$REGIONS), mean) ## CENTER MOST POPULATED NORTH SOUTH ## 2.924618 3.206107 3.153639 3.101124 tapply(table$PSYCH.WELLBEING, factor(table$REGIONS), sd) ## CENTER MOST POPULATED NORTH SOUTH ## 1.836446 1.834613 1.838215 1.725774 ########################################################################## ####################### AIM 2 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four sub-periods of quarantine # PSYCHOLOGICAL WELL-BEING (OF GENERAL HEALTH) anovatemppsychwellbeing &lt;- aov(table$PSYCH.WELLBEING ~ table$SUB.PERIODS.IN.PRE.AND.POST) summary(anovatemppsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$SUB.PERIODS.IN.PRE.AND.POST 3 83 27.791 8.312 1.68e-05 *** ## Residuals 2683 8971 3.344 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovatemppsychwellbeing) TukeyHSD(anovatemppsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$SUB.PERIODS.IN.PRE.AND.POST) ## ## $`table$SUB.PERIODS.IN.PRE.AND.POST` ## diff lwr upr ## 2. TWO WEEK PRE-1. ONE WEEK PRE -0.1360136 -0.37421192 0.1021846 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2031072 -0.07139874 0.4776132 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.4771883 0.17578682 0.7785898 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.3391209 0.01851552 0.6597262 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.6132020 0.26928754 0.9571164 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2740811 -0.09590843 0.6440706 ## p adj ## 2. TWO WEEK PRE-1. ONE WEEK PRE 0.4571940 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2273681 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.0002823 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.0333276 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.0000283 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2264068 plot(TukeyHSD(anovatemppsychwellbeing)) # significant differences # 4. REMAINING WEEKS POST-1. ONE WEEK PRE p adj 0.0002823 # 3. ONE WEEK POST-2. TWO WEEK PRE p adj 0.0333276 # 4. REMAINING WEEKS POST-2. TWO WEEK PRE p adj 0.0000283 tapply(table$PSYCH.WELLBEING, factor(table$SUB.PERIODS.IN.PRE.AND.POST), mean) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 3.033156 2.897143 3.236264 ## 4. REMAINING WEEKS POST ## 3.510345 tapply(table$PSYCH.WELLBEING, factor(table$SUB.PERIODS.IN.PRE.AND.POST), sd) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1.823743 1.819344 1.886359 ## 4. REMAINING WEEKS POST ## 1.796256 # Figure S1: plotmeans(table$PSYCH.WELLBEING ~ table$SUB.PERIODS.IN.PRE.AND.POST, main = &quot;Fig. S1: Psychological well-being/discomfort by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Psychological well-being/discomfort&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) ########################################################################## ############################# THE END #################################### ########################################################################## "],["7_parameters.html", "Chapter 7 Parameters 7.1 Inspecting the data 7.2 Creating the graphs 7.3 Checking parameters", " Chapter 7 Parameters As part of assignment 9 from the DSFB2 Workflows course. After learning about reproducibility and creating packages, parameters are the next step in learning to create a reproducible analysis. Working with parameters will make it easier to re-use or apply the analysis to different datasets or different categories within the same dataset. In this assignment data about COVID19 cases and COVID19-related deaths will be used. The data was taken from “Data on the Daily Number of New Reported COVID-19 Cases and Deaths by EU/EEA Country” (2022) and the parameters have been set in the YAML of the index page as shown in figure 7.1. First the analysis will be executed with the parameters as they are in figure 7.1. The analysis will later be repeated with different inputs to show the functionality of the parameters. Figure 7.1: The parameters as set in the YAML of the index page of this website. 7.1 Inspecting the data Before using the parameters it is important to know what data we are working with. covid19 &lt;- read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/nationalcasedeath_eueea_daily_ei/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # The data is also available as dataframe in the portrobbo package # Check columnnames and types ---- str(covid19) ## There&#39;s 11 columns, all either integer or character # Check countries ---- uniq_val(covid19$countriesAndTerritories) ## 30 different countries took part uniq_val(covid19$geoId) == 30 ## Each country has it&#39;s own geoID uniq_val(covid19$countryterritoryCode) == 30 ## Each country has it&#39;s own territory code unique(covid19$continentExp) ## All countries are European # Check timeframe ---- min(covid19$dateRep) ## Data was taken from 01/01/2020 max(covid19$dateRep) ## Until 31/12/2021 7.2 Creating the graphs Two graphs will be made, one using the column ‘cases’ and one using the column ‘deaths’. The other inputs, like timeframe and country, will be parameterized. To make this easy the parameters will only be used in the first step, to extract the useful data from the main dataset. After this, only that extracted data will be used without referencing the parameters again. The four parameters used are “country”, “year”, “fromMonth” and “untilMonth”. # Extract data ---- ## The original dataset will be filtered using the parameters covid19_cases_graph &lt;- covid19 %&gt;% filter(countriesAndTerritories == params$country, year == params$year, month == c(params$fromMonth:params$untilMonth)) # dateRep to date type ---- ## To use the dateRep column as the x-axis in the graphs, it&#39;s class will be changed to &#39;date&#39; covid19_cases_graph$dateRep &lt;- as.Date(covid19_cases_graph$dateRep, &quot;%d/%m/%y&quot;) Now that only the data specified by the parameters has been extracted, it can be used for the graphs. It is important to refer to the newly created dataset, not just for the values themselves but also when writing captions or labels. This was achieved using the paste function to adjust the strings to the extracted data. The first graph looks at the COVID19 cases and the second graph is the same except it looks at COVID19 related deaths (figure 7.2). # Covid19 cases graph ---- cases &lt;- plot_ly( data = covid19_cases_graph, x = ~dateRep, y = ~cases, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, marker = list(line = list(width = 3)), name = &#39;Cases&#39;) # Covid19 deaths graph ---- deaths &lt;- plot_ly( data = covid19_cases_graph, x = ~dateRep, y = ~deaths, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, marker = list(line = list(width = 3)), name = &#39;Deaths&#39;) # Creating graph grid ---- # Grid plots &lt;- subplot(cases, deaths, nrows = 2) %&gt;% layout(title = paste0(&quot;COVID19 in &quot;, covid19_cases_graph$countriesAndTerritories[1])) plots Figure 7.2: Scatterplot to show the COVID19 cases and related deaths in Netherlands from 2020-05-02 until 2020-08-31. 7.3 Checking parameters Since the graphs are made using the parameters, it can easily be changed to show different data. Below are a few examples of other inputs for the parameters, with a screenshot of the resulting graphs. Germany from August 2020 until December 2020 Austria for all of 2020 Italy for February 2020 until May 2020 "],["8_References.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
