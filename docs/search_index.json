[["index.html", "Curriculum Vitae Contact Profile Job experience Education Skills Volunteering Languages", " Curriculum Vitae Contact Utrecht, The Netherlands mirtheklaassen@outlook.com Linkedin Instagram Profile I’m goal-oriented, curious, analytical and versatile with a passion for molecular biology and data science. I want to apply my skills in unraveling patterns and pathways in the field of bioinformatics. Along with combining my wide range of interests this would give me the opportunity of contributing to the development of big data analysis and support biomedical research. Job experience Management Assistent, Fitness Factory, Utrecht June 2017 - Oktober 2018 My responsibilities included the remodelling projects, contact with externals, hiring and teaching new employees, basic accountancy, inventory management and technical maintenance. Also active as spinning-instructor. Examtrainer Math, Lyceo, Utrecht March 2018 - September 2018 Sales employee, Lens Utrecht, Utrecht May 2018 - August 2019 Management Assistant, TrainMore Black Label, Utrecht August 2019 - September 2020 My responsibilities included directing the sales-team, technical maintenance, inventory management, member administration, contacting debtors/debt collection agencies, contact with externals, contactperson for employees and helping the manager when necessary. Also active as kickboxing- and HIIT-instructor. Webdeveloper, WisMon, Utrecht March 2021 - September 2021 Sales employee, Pipoos, Utrecht September 2022 - Present Education Communication &amp; Multimedia design, Avans, Den Bosch September 2015 - December 2017 With a personal focus on front-end webdevelopment and datavisualisation Optometry, University of Applied Sciences, Utrecht September 2018 - Augustus 2019 Biology &amp; Medical Labresearch, University of Applied Sciences, Utrecht September 2020 - Present GPA of 4.0 Honours star in genetic recombination and transfection Specialization in Data Science for Biology Skills Design: Adobe Photoshop, Adobe Illustrator and Adobe InDesign Informatics: Basic R, basic BASH, HTML and CSS Volunteering Design-expert, Innovation in a Week, Veghel July 2016 Won the ‘Teamwork-award’ Student Association Uerius Brabus, Den Bosch October 2015 - December 2016 Active as co-founder, captain of the PR committee and active in the committee for external contacts Languages Dutch: native speaker English: level C1 "],["Workflows_Portfolio_1.html", "Chapter 1 Reproducible Research 1.1 Excel’s downsides 1.2 Repita criteria 1.3 Open source code", " Chapter 1 Reproducible Research As part of assignment 1 from the DSFB2 Workflows course. 1.1 Excel’s downsides 1.1.1 Inspecting an Excel file As part of assignment 1.1 I was asked to review this file, by opening the file in Excel. The experimental condition ‘ControlVehicleA’ stood out to me. It appears to be the same as the ControlPositive but with less Ethanol. I’m guessing this shows the experimental conditions without the experimental compounds which would mean the compounds have been diluted in ethanol. There are also extra sheets with lists of inputs, logs of changes and input examples. I’m not sure what the difference in purpose is between the ExpLookup sheet and the Input example sheet. Three different compounds have been tested, 2,6-diisopropylnaphthalene, decane and naphthalene. The positive control for this experiment uses 1.5% ethanol in S-medium. The negative control for this experiment uses just S-medium without an added compound. Next up, the file was opened in R and the data types per column were checked, these seemed to differ from the expectations. # Read the excel file CE.LIQ.FLOW.062_Tidydata &lt;- read_excel( &quot;data-raw/data011/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) The column RawData was recognized as a double while I’d expect an integer, compName was a character while I expected a factor and compConcentration was also character while it should be a double. It had not been assigned correctly. For further analysis the types were changed to the correct ones using the following code. ## tibble [360 × 34] (S3: tbl_df/tbl/data.frame) ## $ plateRow : logi [1:360] NA NA NA NA NA NA ... ## $ plateColumn : logi [1:360] NA NA NA NA NA NA ... ## $ vialNr : num [1:360] 1 1 1 1 1 2 2 2 2 2 ... ## $ dropCode : chr [1:360] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... ## $ expType : chr [1:360] &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; ... ## $ expReplicate : num [1:360] 3 3 3 3 3 3 3 3 3 3 ... ## $ expName : chr [1:360] &quot;CE.LIQ.FLOW.062&quot; &quot;CE.LIQ.FLOW.062&quot; &quot;CE.LIQ.FLOW.062&quot; &quot;CE.LIQ.FLOW.062&quot; ... ## $ expDate : POSIXct[1:360], format: &quot;2020-11-30&quot; &quot;2020-11-30&quot; ... ## $ expResearcher : chr [1:360] &quot;Sergio Reijnders - Ellis Herder&quot; &quot;Sergio Reijnders - Ellis Herder&quot; &quot;Sergio Reijnders - Ellis Herder&quot; &quot;Sergio Reijnders - Ellis Herder&quot; ... ## $ expTime : num [1:360] 68 68 68 68 68 68 68 68 68 68 ... ## $ expUnit : chr [1:360] &quot;hour&quot; &quot;hour&quot; &quot;hour&quot; &quot;hour&quot; ... ## $ expVolumeCounted : num [1:360] 50 50 50 50 50 50 50 50 50 50 ... ## $ RawData : int [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compCASRN : chr [1:360] &quot;24157-81-1&quot; &quot;24157-81-1&quot; &quot;24157-81-1&quot; &quot;24157-81-1&quot; ... ## $ compName : Factor w/ 5 levels &quot;2,6-diisopropylnaphthalene&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ compConcentration : num [1:360] 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 ... ## $ compUnit : chr [1:360] &quot;nM&quot; &quot;nM&quot; &quot;nM&quot; &quot;nM&quot; ... ## $ compDelivery : chr [1:360] &quot;Liquid&quot; &quot;Liquid&quot; &quot;Liquid&quot; &quot;Liquid&quot; ... ## $ compVehicle : chr [1:360] &quot;controlVehicleA&quot; &quot;controlVehicleA&quot; &quot;controlVehicleA&quot; &quot;controlVehicleA&quot; ... ## $ elegansStrain : chr [1:360] &quot;N2&quot; &quot;N2&quot; &quot;N2&quot; &quot;N2&quot; ... ## $ elegansInput : num [1:360] 25 25 25 25 25 25 25 25 25 25 ... ## $ bacterialStrain : chr [1:360] &quot;OP50&quot; &quot;OP50&quot; &quot;OP50&quot; &quot;OP50&quot; ... ## $ bacterialTreatment : chr [1:360] &quot;heated&quot; &quot;heated&quot; &quot;heated&quot; &quot;heated&quot; ... ## $ bacterialOD600 : num [1:360] 0.743 0.743 0.743 0.743 0.743 0.743 0.743 0.743 0.743 0.743 ... ## $ bacterialConcX : num [1:360] 8 8 8 8 8 8 8 8 8 8 ... ## $ bacterialVolume : num [1:360] 300 300 300 300 300 300 300 300 300 300 ... ## $ bacterialVolUnit : chr [1:360] &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; ... ## $ incubationVial : chr [1:360] &quot;1,5 glass vial&quot; &quot;1,5 glass vial&quot; &quot;1,5 glass vial&quot; &quot;1,5 glass vial&quot; ... ## $ incubationVolume : num [1:360] 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ... ## $ incubationUnit : chr [1:360] &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; ... ## $ incubationMethod : chr [1:360] &quot;rockroll&quot; &quot;rockroll&quot; &quot;rockroll&quot; &quot;rockroll&quot; ... ## $ incubationRPM : num [1:360] 35 35 35 35 35 35 35 35 35 35 ... ## $ bubble : logi [1:360] NA NA NA NA NA NA ... ## $ incubateTemperature: num [1:360] 20 20 20 20 20 20 20 20 20 20 ... 1.1.2 Creating the scatterplot Now that the columntypes are corrected the data can be used to create a scatterplot. The assignment said to put the compConcentration on the x-axis, the DataRaw counts on the y-axis and to assign a colour to each level in compName. It also asked to assign a different symbol to each level in the expType variable. Since the compUnit of the controls are not nM but percentages, they have been added as dashed lines instead of measured values. This means the different shapes for expType variables are not neccessary since only “experiment” is shown out of the possible ExpType values. In the assignment the initial scatterplot was followed by a question about the x-axis, “When creating the plot under C), what happened with the ordering of the x-axis labels. Explain why this happens.”. The compConcentration had already been changed to the correct type in the previous code chunk, so nothing went wrong when creating the scatterplot. Had the type not been changed beforehand, the x-axis would have been in random orders because they were seen as characters instead of doubles. The x-axis is shown with a log10 transformation and slight jitter has been added for better readability. A few details were added on top of the given assignment. Each compound got it’s own colour, the standard deviations have been added as errorbars and the control values have been added as dashed lines for comparison. The resulting plot is shown in figure 1.1. # Get the average counts per compound per concentration, keep the expType variable too CE.LIQ_summ &lt;- CE.LIQ.FLOW.062_tbl %&gt;% group_by(compName, compConcentration, expType) %&gt;% summarize(mean_counts = mean(RawData, na.rm = TRUE), stdev_counts = sd(RawData, na.rm = TRUE),) # Create a tibble containing only the data used for the graph CE.LIQ_summ_exp &lt;- CE.LIQ_summ %&gt;% filter(expType == &quot;experiment&quot;) # Create the scatterplot CE.LIQ_summ_exp %&gt;% ggplot(aes(x = log10(compConcentration) # log10 as suggested in question F , y = mean_counts)) + geom_smooth(aes(group=compName, color = compName), span= .5) + geom_point(aes(color = compName), position = position_jitter(width = 0.2, height = 0.2, seed = 123)) + geom_errorbar(aes(ymin=mean_counts-stdev_counts, ymax=mean_counts+stdev_counts, color = compName), width=.1, position = position_jitter( width = 0.2, height = 0.2, seed = 123)) + labs(title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count&quot;, x = &quot;Compound concentration in log10(nM)&quot;, caption = str_wrap(&quot;Figure 1.1 Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations.&quot;, 80)) + geom_hline(yintercept = 85.9, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control negative geom_text(aes(0,85.9, label = &quot;Control negative&quot;, vjust=-0.3), size = 3.5) + geom_hline(yintercept = 49.4, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control positive geom_text(aes(-3.5,49.4, label = &quot;Control positive&quot;, vjust=-0.3), size = 3.5) + theme_minimal() 1.1.3 Normalize for the negative control In order to see how much the offspring count improved or decreased relatively to the negative control basline the data must be normalized. To do this we adjust the negative control to a value of 1 and adjust the other values in the same way. In this case that means dividing by 85.9. The scatterplot was created again with the new normalized data, further settings stayed the same, the result is shown in figure 1.2. # Check the mean value of controlNegative # view(CE.LIQ_summ) # It&#39;s 85.9 # Add the column with normalized average counts CE.LIQ_summ_norm &lt;- CE.LIQ_summ_exp %&gt;% mutate(norm_counts = mean_counts/85.9, norm_stdev = stdev_counts/85.9) CE.LIQ_summ_norm %&gt;% ggplot(aes(x = log10(compConcentration), # log10 as requested in question F y = norm_counts)) + geom_smooth(aes(group=compName, color = compName), span= .5) + geom_point(aes(color = compName), position = position_jitter(width = 0.15, height = 0.15, seed = 123)) + geom_errorbar(aes(ymin=norm_counts-norm_stdev, ymax=norm_counts+norm_stdev, color = compName), width=.1, position = position_jitter(width = 0.15, height = 0.15, seed = 123)) + labs(title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count relative to negative control&quot;, x = &quot;Compound concentration in log10(nM)&quot;, caption = str_wrap(&quot;Figure 1.2 Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations. Normalized to negative control == 1.&quot;, 70)) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, size = 0.2) + # Show control negative geom_text(aes(0,1, label = &quot;Control negative&quot;, vjust=-0.3), size = 3.5) + geom_hline(yintercept = 0.57508731, linetype = &quot;dashed&quot;, size = 0.2) + # Show control positive geom_text(aes(-3.5,0.57508731, label = &quot;Control positive&quot;, vjust=-0.3), size = 3.5) + theme_minimal() 1.1.4 Statistical analysis In order to learn wether there is indeed an effect of the different compounds a few tests need to be performed. Starting with the Shapiro-Wilk test to check the normality of the data. Followed by an ANOVA with the different concentrations as added variable. To see the actual difference, the analysis should be finished with a Post hoc test like Tukey. 1.2 Repita criteria 1.2.1 Criteria for reproducibility In this first part of assignment 1.2 we will look at ‘Repita’ criteria. The criteria are used to check for the reproducibility of scientific research articles. In table 1.1 the different criteria are shown with a definition and the type of response it calls for. In order to further understand and apply these criteria I will look for a research article and check if this article follows the ‘Repita’ guidelines as shown in table 1. # Create the different variables needed for the repita criteria table Transparency_Criteria &lt;- c(&quot;Study Purpose&quot;, &quot;Data Availability Statement&quot;, &quot;Data Location&quot;, &quot;Study Location&quot;, &quot;Author Review&quot;, &quot;Ethics Statement&quot;, &quot;Funding Statement&quot;, &quot;Code Availability&quot;) Definition &lt;- c(&quot;A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.&quot;, &quot;A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.&quot;, &quot;Where the article’s data can be accessed, either raw or processed.&quot;, &quot;Author has stated in the methods section where the study took place or the data’s country/region of origin.&quot;, &quot;The professionalism of the contact information that the author has provided in the manuscript.&quot;, &quot;A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.&quot;, &quot;A statement within the manuscript indicating whether or not the authors received funding for their research.&quot;, &quot;Authors have shared access to the most updated code that they used in their study, including code used for analysis.&quot;) Response_Type &lt;- c(&quot;Binary&quot;, &quot;Binary&quot;, &quot;Found Value&quot;, &quot;Binary;Found Value&quot;, &quot;Found Value&quot;, &quot;Binary&quot;, &quot;Binary&quot;, &quot;Binary&quot;) # Turn the variables into one dataframe repita_criteria &lt;- data.frame(Transparency_Criteria, Definition, Response_Type) # Use the dataframe to create the table repita_criteria %&gt;% kable(col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;Table 1.1 The repita criteria as given in portfolio assignment 1.2 in [lesson 1 of the reader](https://lesmaterialen.rstudio.hu.nl/workflows-reader/represintro.html).&quot;) (#tab:repita criteria)Table 1.1 The repita criteria as given in portfolio assignment 1.2 in lesson 1 of the reader. Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary;Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary 1.2.2 The article Plosone was used to find a suitable article. The found article is a primary article describing emperical scientific findings by Bruckner et al. (2022). The focus is on how microbiotas play a role in developmental programs. Altered microbiota composition appears to be linked to neurodevelopmental conditions such as autism spectrum disorder. One of the findings described how the microbiota can influence forebrain neurons. Forebrain neurons are required for normal social behavior and localization of forebrain microglia. The study uses zebrafish which were kept at 28°C with a 14/10 light/dark cycle. For the controlgroup conventionalized (CVZ) fish were used. The experimental condition consisted of germ-free (GF) fish. These fish were made germ-free and therefore did not have their microbiota. When the fish were older (7 dpf) they were inoculated with normal microbiota (ex germ-free or XGF) and their optomotor respons was tested. Once the fish were adults (14 dpf) their social behavior was tested. The social behavior was assessed through a dyad assay for postflexion larval and adult zebrafish. For each condition a pair of siblings were placed in isolated tanks and allowed to interact for 10 minutes via transparent tank walls. Social interaction was defined as the average relative distance from the divider and the percentage of time spent orienting at 45° to 90°. These parameters were measured and analyzed using computer vision software written in Python (available at https://github.com/stednitzs/daniopen). Compared to the CVZ siblings, the GF larvae spent significantly less time than in close proximity to and oriented at 45 to 90° to the stimulus fish. These results show that an intact microbiota is required early for later development of normal social behavior. Aside from the social behavior, the study also looked at optomotor responses. It is possible that the microbiota influences circuitry underlying the early vision and locomotion required for social behavior. To address this possibility, the vision and locomotion were assayed by comparing kinetics of the optomotor response to virtual motion in GF larvae and CVZ controls. Optomotor response was assessed using a “virtual reality” system for assessing zebrafish behavior, measuring swim response in larvae. A visual stimulus was projected on a screen underneath the dishes for 20 seconds and consisted of concentric rings moving toward the dish center, followed by a 20-second refractory period. Responses are the average of 46 to 59 stimulus trials per fish, presented over 1 hour. In this part of the study, no significant difference were found between the GF and CVZ fish. This suggests that the microbiota influences circuits specific to social behavior directly, rather than by modulating vision or locomotion. The results demonstrate that the microbiota influences zebrafish social behavior by stimulating microglial remodeling of forebrain circuits during early neurodevelopment. This conclusion suggests pathways for new interventions in multiple neurodevelopmental disorders. 1.2.3 The reproducibility To better understand the criteria (table 1.) they will be applied to the found article. The results can be found in table 1.2. While answering this assignment I noticed how this is not as black and white as I thought. For example with the study location. At first I thought it was stated, because University of Oregon came back quite a few times, looking at it more closely it was not clear if that was the actual study location. # Create the different variables needed for the article check table Score &lt;- c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;[Link](https://figshare.com/projects/Bruckner_et_al_Data/136756)&quot;, &quot;FALSE&quot;, &quot;Non professional&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;) Explanation &lt;- c(&quot;In the second to last paragraph of the introduction the study objective is described.&quot;, &quot;On the left-hand side of the webpage the \\&quot;Accesibble Data\\&quot; can be found&quot;, &quot;The data can be accessed through the given link, found under the \\&quot;Accesibble Data\\&quot; header.&quot;, &quot;The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. &quot;, &quot;Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given.&quot;, &quot;There is an ethics statement in the methods paragraph.&quot;, &quot;The funding is stated under the abstract.&quot;, &quot;The data analysis is explained but the code has not been shared.&quot;) # Turn the variables into one dataframe article_repita_check &lt;- data.frame(Transparency_Criteria, Score, Explanation) # Use the dataframe to create the table article_repita_check %&gt;% kable(col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;Table 1.2 The score for each repita criterium when looking at the research done by Bruckner et al.(2022)&quot;) (#tab:article check)Table 1.2 The score for each repita criterium when looking at the research done by Bruckner et al.(2022) Transparency Criteria Definition Response Type Study Purpose TRUE In the second to last paragraph of the introduction the study objective is described. Data Availability Statement TRUE On the left-hand side of the webpage the “Accesibble Data” can be found Data Location Link The data can be accessed through the given link, found under the “Accesibble Data” header. Study Location FALSE The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. Author Review Non professional Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given. Ethics Statement TRUE There is an ethics statement in the methods paragraph. Funding Statement TRUE The funding is stated under the abstract. Code Availability FALSE The data analysis is explained but the code has not been shared. 1.3 Open source code In this second part of assignment 1.2 we’ll be looking at the code from an existing study. The focus will be on understanding the code, noting what it tries to achieve, the readability, reproducibility, fixing errors, etc. 1.3.1 The article The article used for this assignment had to live up to a few criteria. First up, the data had to be analysed using R code. This code and the dataset had to be available. The chosen article is “Mental Health Impacts in Argentinean College Students During COVID-19 Quarantine” by López Steinmetz et al. (2021). The study aimed to analyze differences in mental health in college students from Argentina who were exposed to different spread-rates of COVID-19. They also wanted to analyze between group differences in mental health indicatores at four different quarantine sub-periods. To do this a cross-sectional design was used. The sample included 2687 college students and the data was collected online during the Argentinean quarantine. They used a one-way between-groups ANOVA with Tukey’s post hoc test for the analysis. The results showed that the center and most populated area only differed in psychological well-being and negative alcohol related consequences, but not in the other indicators. For the sub-periods there were differences in psychological well-being, social functioning and coping, psychological distress, and negative alcohol-related consequences. Negative alcohol-related consequences were the only MHS indicator improving over time. This worsened mean mental health suggests that quarantine and its extensions contribute to negative mental health impacts. 1.3.2 The code The code, as well as the dataset, for the study were made available through OSF. The original can be found here and the version with my adjustments can be found at the bottom of this page. The dataset and the code were added to the repository as well. After reading the code it appears to consist of a few parts. Reference to the manuscript Loading the data and packages Methods in general Sample size Distribution by sex, also noted in percentages Mean, median and stdev of the age of the students Distribution by province in percentages Methods for data analysis Test of skewness per mental health indicator Test or Kurtosis per mental health indicator with criteria and criteria reference Division based on region for first aim Division based on sub-periods for second aim Results for aim 1 (differences in MHS based on region) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per region Results for aim 2 (differences in MHS based on sub-period) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per sub-period Plot with means per sub-period and 95% confidence interval The code is used to make the data clear and tidy by first noting details like sample size, distributions in percentages and tests of skewness. Then it moves on to perform statistical tests for the two different aims of the study. It shows the significant differences and creates plots for the second aim. It was very easy to read the code, I would give it a 5/5 on readability. It was made especially easy by the clear headers and comments. 1.3.3 Excuting the code To further inspect the code we will execute it on the dataset given by the study. To this we change one word in the data set in order to link the dataset as it is downloaded to the project. To this the word “clipboard” under comment “Load the dataset” will be changed into raw-data/data012/dataset.xlsx, the location of the downloaded dataset. Errors will be noted and the code will be changed accordingly. 1.3.3.1 Changes and errors when executing The original code uses read.table and the clipboard to load the data. This can very quickly be done wrong. To fix this the readxl package was added and the excel file was added using table&lt;-read_excel(“data-raw/data012/dataset.xlsx”). “Error in model.frame.default(formula = table$PSYCH.WELLBEING ~ table$REGIONS,:invalid type (NULL) for variable ’table$PSYCH.WELLBEING_. In the table the column”PSYCH WELLBEING” uses a space instead of a period. This also happens for other column names. To change this colnames(table) &lt;- str_replace_all(colnames(table), ” “,”.”) was added before testing skewness. The rest of the code stayed the same. To make the knitting of this rmarkdown easier the code has been adjusted to only look at “PSYCH.WELLBEING” as an example. All other indicators would have been analysed in the exact same manner. 1.3.4 Effort scoring Thanks to the great readability and consistency of the code it was very easy to reproduce. The only issue was adjusting the columnnames to use periods instead of spaces which was easily fixed. Therefore the score is a 5/5 on reproducibility. Read the code including my adjustments # R Code for the manuscript entitled: # &quot;Mental health impacts in Argentinean college students during COVID-19 quarantine&quot;. # López Steinmetz L.C., Leyes C.A., Dutto Florio M.A., Fong S.B., López Steinmetz R.L. &amp; Godoy J.C. ########################################################################## ################## LOAD THE DATASET &amp; PACKAGES ########################### ########################################################################## # Load the dataset table&lt;-read_excel(&quot;data-raw/data012/dataset.xlsx&quot;) # Changed by Mirthe Klaassen for this portfolio assignment summary(table) ## SUB PERIODS IN PRE AND POST REGIONS PROVINCE ## Length:2687 Length:2687 Length:2687 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## SEX AGE PSYCH WELLBEING SOC FUNC AND COPING ## Length:2687 Min. :18.00 Min. :0.000 Min. :0.000 ## Class :character 1st Qu.:20.00 1st Qu.:2.000 1st Qu.:0.000 ## Mode :character Median :22.00 Median :3.000 Median :2.000 ## Mean :22.74 Mean :3.086 Mean :2.149 ## 3rd Qu.:24.00 3rd Qu.:5.000 3rd Qu.:4.000 ## Max. :39.00 Max. :6.000 Max. :6.000 ## K10 BDI STAIR YAACQ ISO ## Min. :10.0 Min. : 0 Min. : 1.00 Min. : 0.000 Min. : 1.00 ## 1st Qu.:19.0 1st Qu.:10 1st Qu.:21.00 1st Qu.: 0.000 1st Qu.:22.00 ## Median :25.0 Median :16 Median :29.00 Median : 3.000 Median :32.00 ## Mean :25.5 Mean :18 Mean :29.24 Mean : 3.699 Mean :34.56 ## 3rd Qu.:32.0 3rd Qu.:25 3rd Qu.:38.00 3rd Qu.: 6.000 3rd Qu.:46.00 ## Max. :49.0 Max. :57 Max. :58.00 Max. :21.000 Max. :82.00 # Load the packages: library(moments) library(gplots) library(readxl) ## Added by Mirthe Klaassen for this portfolio assignment ########################################################################## ###################### METHODS ########################################### ########################################################################## ###### SUB-TITLE: METHOD &gt; Sample and procedure # SAMPLE N = 2687 # Distribution by sex: table(table$SEX) ## ## man other woman ## 473 22 2192 # Absolute frequencies: Women = 2192, Men = 473, Other = 22 prop.table(table(table$SEX))*100 ## ## man other woman ## 17.603275 0.818757 81.577968 # Percentages: Women = 81.577968%, Men = 17.603275%, Other = 0.818757% # Central tendency measures by age (total sample) # mean mean(table$AGE) ## [1] 22.74023 # Mean age = 22.74023 # standard deviation sd(table$AGE) ## [1] 3.635612 # sd age = 3.635612 # median median(table$AGE) ## [1] 22 # median age = 22 # Distribution by provinces prop.table(table(table$PROVINCE))*100 ## ## CABA CBA JUJ PCIAB SAL STACR TDELF ## 11.9464086 39.0026051 6.6989207 31.9315221 7.1082992 0.9676219 2.3446223 # JUJ (JUJUY) = 6.6989207% # SAL (SALTA) = 7.1082992% # CBA (CÓRDOBA) = 39.0026051% # STACR (SANTA CRUZ) = 0.9676219% # TDELF (TIERRA DEL FUEGO) = 2.3446223% # CABA (CIUDAD AUTÓNOMA DE BUENOS AIRES) = 11.9464086% # PCIAB (PROVINCIA DE BUENOS AIRES) = 31.9315221% ###### SUB-TITLE: METHOD &gt; Data analysis # Adjustment of colnames to be the same as the names in the code == added by Mirthe Klaassen for this assignment colnames(table) &lt;- str_replace_all(colnames(table), &quot; &quot;, &quot;.&quot;) ### To test Skewness and Kurtosis # Criteria: range of acceptable values or near to (-3 and +3; Brown, 2006). # Reference: Brown T.A. (2006). Confirmatory factor analysis for applied research. New York: Guilford Press. # PSYCH.WELLBEING skewness(table$PSYCH.WELLBEING) ## [1] -0.05214941 # skewness PSYCH.WELLBEING = -0.05214941 kurtosis(table$PSYCH.WELLBEING) ## [1] 1.951112 # kurtosis PSYCH.WELLBEING = 1.951112 ### For analyses corresponding to the first aim, we divided the entire sample into four groups: table(table$REGIONS) ## ## CENTER MOST POPULATED NORTH SOUTH ## 1048 1179 371 89 # NORTH = 371 # CENTER = 1048 # SOUTH = 89 # MOST POPULATED = 1179 ### For analyses corresponding to the second aim, we divided the entire sample into four groups: table(table$SUB.PERIODS.IN.PRE.AND.POST) ## ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1508 525 364 ## 4. REMAINING WEEKS POST ## 290 # first week pre-quarantine extension (ONE WEEK PRE) = 1508 # second week pre-quarantine extension (TWO WEEK PRE) = 525 # first week post-quarantine extension (ONE WEEK POST) = 364 # remaining weeks post-quarantine extension (REMAINING WEEKS POST) = 290 ########################################################################## ###################### RESULTS ########################################### ########################################################################## ########################################################################## ####################### AIM 1 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four regions table$PSYCH.WELLBEING ## [1] 6 2 3 4 0 4 0 4 2 2 2 3 0 4 4 5 3 6 3 5 2 6 5 5 5 2 3 6 6 3 4 4 2 1 4 0 2 ## [38] 5 0 1 4 5 4 1 6 3 5 5 0 5 5 6 3 3 5 4 3 4 0 6 4 1 5 2 2 1 5 2 3 4 4 4 1 2 ## [75] 4 1 5 4 5 6 1 3 2 4 2 2 2 6 5 2 2 2 4 5 4 4 4 5 6 3 3 2 2 2 4 5 1 1 5 0 0 ## [112] 5 2 2 5 3 3 1 0 4 4 2 6 1 6 4 1 2 3 4 4 4 6 1 2 0 1 3 6 2 6 6 5 2 6 4 4 2 ## [149] 3 3 2 4 3 5 1 2 0 4 3 2 1 3 6 1 5 0 2 4 2 1 1 3 1 1 5 4 4 1 1 0 6 3 1 5 5 ## [186] 0 1 3 3 0 3 6 1 0 1 3 1 3 4 2 4 2 3 0 3 4 5 1 4 2 3 5 2 2 6 5 1 1 1 1 5 1 ## [223] 0 0 1 0 0 3 6 6 1 4 3 3 2 0 3 2 1 6 6 2 5 0 3 5 5 2 2 4 4 1 6 0 1 6 2 1 5 ## [260] 1 0 0 4 5 2 4 5 6 6 5 3 5 1 1 5 5 5 4 1 1 1 2 3 6 5 1 0 6 3 1 1 1 3 2 4 6 ## [297] 3 3 1 2 4 5 3 2 4 4 4 6 3 2 5 3 3 1 5 6 6 5 5 2 2 2 2 0 4 4 4 2 2 5 4 0 4 ## [334] 4 0 1 0 5 1 3 0 0 4 6 2 4 0 1 4 3 4 2 1 3 5 4 2 6 0 0 4 1 5 6 3 0 2 5 2 6 ## [371] 5 2 3 6 3 4 3 5 2 5 2 1 3 4 5 5 1 4 0 0 0 0 3 2 3 6 2 3 5 0 3 6 1 2 6 2 5 ## [408] 4 2 4 1 4 6 2 3 4 2 1 4 2 2 2 6 3 3 0 4 1 0 0 0 4 5 2 2 1 5 0 4 3 4 1 4 3 ## [445] 5 5 3 1 0 2 2 1 3 2 1 5 1 3 0 6 3 4 4 3 6 2 2 3 6 0 4 0 2 1 1 5 4 5 3 0 1 ## [482] 3 6 4 1 4 4 2 6 6 5 3 4 2 0 6 0 0 5 0 0 5 0 3 1 3 2 0 3 2 1 4 1 2 4 3 2 6 ## [519] 4 4 0 2 4 3 4 2 4 1 3 3 2 4 4 3 0 5 2 0 1 4 0 3 0 2 2 4 3 4 5 4 6 2 2 4 5 ## [556] 2 2 2 6 2 2 4 3 6 6 4 6 5 1 1 4 3 0 1 4 4 1 2 4 0 4 1 2 6 0 2 1 3 1 4 0 1 ## [593] 3 3 3 4 6 4 0 3 5 3 2 3 1 3 0 3 0 2 0 3 4 2 2 2 4 2 6 6 3 5 6 1 3 1 2 6 4 ## [630] 0 3 0 2 3 5 2 4 0 5 6 4 3 4 5 4 2 2 1 0 1 6 1 4 2 4 2 5 4 3 6 3 2 3 1 5 2 ## [667] 3 1 2 0 6 0 4 0 3 4 4 0 3 2 0 2 4 3 2 3 1 2 0 1 3 0 0 5 0 2 4 2 6 3 3 0 5 ## [704] 0 5 3 2 3 4 1 2 1 5 4 5 4 5 2 4 5 5 6 4 5 3 0 5 4 4 4 6 6 6 3 6 3 4 3 3 6 ## [741] 5 6 5 4 5 5 2 5 5 4 2 5 6 4 3 2 1 3 5 0 5 5 6 2 1 4 3 0 3 1 6 5 4 2 6 1 3 ## [778] 3 1 4 1 4 3 4 6 2 2 5 5 4 3 0 4 3 2 3 2 0 5 2 0 4 1 0 2 1 6 2 1 5 0 3 5 4 ## [815] 0 0 4 4 1 1 1 5 5 3 2 0 6 2 4 3 1 4 2 2 6 5 0 1 1 5 3 2 2 3 4 1 4 4 4 3 2 ## [852] 4 2 2 5 6 6 6 3 0 5 4 4 1 5 0 5 6 1 2 3 2 3 6 2 5 1 2 3 4 2 5 4 2 4 3 1 0 ## [889] 0 1 5 3 3 6 3 0 2 3 0 5 5 2 4 6 4 3 6 0 2 0 6 5 2 6 1 4 4 3 1 5 2 0 4 0 4 ## [926] 4 1 4 2 4 5 0 3 3 4 3 6 4 4 3 1 3 6 5 1 0 3 5 1 2 2 3 1 1 1 1 3 4 5 0 1 0 ## [963] 1 5 4 4 1 3 4 2 5 2 3 3 2 1 6 6 4 4 2 5 3 3 4 6 2 3 3 0 2 6 4 5 6 3 6 6 1 ## [1000] 5 2 4 1 1 5 1 3 2 5 3 1 0 5 1 5 3 5 4 2 1 2 2 6 0 3 0 4 6 4 3 0 3 3 3 1 1 ## [1037] 4 3 3 4 0 4 4 2 5 5 0 4 4 0 2 0 6 1 3 3 6 0 0 3 2 3 6 3 3 6 1 4 3 2 5 4 4 ## [1074] 1 5 2 3 2 2 5 5 5 4 5 6 3 3 4 3 6 2 6 5 5 1 0 3 3 1 6 2 4 5 5 3 4 4 0 3 0 ## [1111] 0 3 3 4 3 1 1 2 1 4 1 4 0 2 1 3 2 2 6 1 3 2 1 3 2 1 2 1 2 0 3 4 1 6 1 3 2 ## [1148] 5 2 4 1 3 1 0 0 2 1 3 4 0 0 4 1 5 4 3 2 4 5 4 5 5 3 3 6 1 0 4 2 3 4 2 2 0 ## [1185] 5 4 6 1 2 6 1 0 5 3 5 3 4 3 2 1 5 5 5 2 5 3 1 5 6 6 4 3 3 1 3 3 6 6 3 4 4 ## [1222] 2 3 0 4 2 0 4 4 1 2 2 2 3 1 0 1 6 5 1 5 3 6 6 1 4 1 6 4 1 1 4 2 2 3 5 0 1 ## [1259] 5 4 6 2 0 0 3 1 3 4 6 0 4 2 3 0 3 2 6 6 0 2 0 5 0 2 4 4 5 2 4 1 3 4 5 4 3 ## [1296] 6 3 4 0 3 4 4 2 0 4 2 5 3 5 1 0 6 0 0 2 6 2 0 1 1 3 1 0 4 5 3 1 2 4 3 2 1 ## [1333] 0 4 5 1 2 6 3 3 3 4 4 3 0 4 6 2 2 3 6 6 1 4 1 5 3 6 4 2 3 6 3 6 4 2 1 6 1 ## [1370] 3 2 1 1 4 3 1 4 4 5 3 6 0 5 1 4 5 0 0 1 3 2 0 1 2 2 3 5 3 4 3 6 2 4 0 0 0 ## [1407] 3 3 1 2 3 3 2 2 5 3 2 1 0 4 5 3 6 2 3 2 0 0 0 0 6 5 2 2 5 4 2 4 5 0 5 5 1 ## [1444] 2 1 2 6 2 4 0 6 0 3 4 3 6 0 2 3 2 4 1 4 0 2 5 1 4 4 2 2 2 4 3 0 6 3 0 3 5 ## [1481] 1 5 3 2 2 6 0 2 4 6 2 3 6 4 3 3 2 3 1 3 4 6 6 5 3 1 6 5 3 6 3 6 2 6 2 0 0 ## [1518] 4 2 1 2 3 1 0 5 2 2 3 3 6 4 1 5 3 2 3 2 5 1 6 3 1 0 3 6 2 2 4 6 6 4 0 1 5 ## [1555] 2 3 5 3 5 6 5 5 3 1 4 5 3 6 5 6 5 1 0 1 4 5 5 4 6 4 2 2 2 5 1 6 5 0 4 2 4 ## [1592] 5 4 3 3 4 0 3 2 0 3 1 4 0 6 4 2 5 2 1 3 2 2 3 1 4 1 1 5 6 6 5 5 3 6 1 0 3 ## [1629] 3 4 4 2 0 6 3 0 4 6 2 6 3 5 2 3 6 6 0 2 3 2 1 6 2 1 4 2 2 4 2 5 5 5 2 4 4 ## [1666] 5 1 0 2 3 5 4 2 6 2 4 1 3 2 2 4 1 3 5 3 3 4 5 6 3 6 4 1 1 4 6 3 2 5 3 3 0 ## [1703] 3 3 3 6 1 4 4 5 6 0 3 5 5 5 4 6 3 1 5 5 2 2 6 6 3 2 2 5 2 6 2 3 5 3 1 6 2 ## [1740] 6 5 5 6 1 6 1 2 3 1 3 4 5 4 1 4 3 2 5 5 3 3 1 1 2 3 5 6 2 4 4 4 1 1 6 2 4 ## [1777] 3 3 4 4 2 3 3 1 2 5 4 5 5 2 4 2 5 1 6 1 5 1 6 2 3 0 2 3 3 0 5 6 1 1 3 3 3 ## [1814] 3 4 4 2 5 3 4 4 2 5 2 0 2 6 0 5 5 6 1 4 5 0 5 5 5 5 1 3 1 2 4 5 4 2 6 3 4 ## [1851] 1 3 3 0 0 5 4 6 2 6 4 5 3 4 5 1 6 5 3 1 3 3 4 5 1 2 4 3 3 6 2 2 0 0 2 2 6 ## [1888] 3 6 3 6 4 0 4 5 3 3 1 0 5 4 2 4 4 2 4 4 0 4 5 4 0 2 1 2 1 1 2 0 3 0 6 2 0 ## [1925] 3 2 5 5 3 5 1 1 6 3 5 4 5 6 3 5 5 5 1 0 3 3 4 6 3 5 4 3 4 0 6 0 2 1 3 3 3 ## [1962] 3 4 2 0 4 4 6 5 1 3 0 2 3 5 3 2 3 3 1 4 5 3 3 2 3 6 4 2 1 6 0 4 2 0 4 5 2 ## [1999] 4 2 2 3 2 4 0 4 1 1 3 4 3 4 3 3 0 4 0 5 5 4 1 1 3 0 4 2 1 5 0 1 1 4 4 5 5 ## [2036] 4 3 1 5 3 1 6 4 5 4 0 5 3 2 6 2 6 6 5 1 1 0 3 2 2 4 2 0 3 3 3 5 3 1 5 6 3 ## [2073] 1 2 3 5 4 0 0 6 5 3 2 6 6 3 2 4 6 1 1 5 2 6 6 5 4 4 5 0 0 4 5 1 3 2 4 3 3 ## [2110] 4 6 6 3 0 2 4 6 0 1 0 5 1 5 0 3 3 6 5 4 6 5 5 3 5 2 1 6 0 3 0 4 6 4 2 2 2 ## [2147] 2 1 2 5 3 3 6 5 3 4 5 1 5 5 0 0 5 2 6 6 1 3 2 6 5 5 1 3 6 3 4 1 1 2 6 5 3 ## [2184] 2 5 4 5 4 4 1 5 5 6 6 6 5 2 2 3 3 6 2 1 1 5 3 5 0 3 6 4 3 0 5 5 1 5 6 1 6 ## [2221] 2 2 2 2 2 5 3 3 1 2 1 0 1 3 2 3 4 5 4 0 3 5 4 5 6 3 6 5 2 3 6 2 5 6 2 5 2 ## [2258] 4 5 6 5 3 2 0 0 6 2 1 1 5 6 4 4 6 3 1 5 6 2 2 5 4 3 2 6 0 2 4 1 0 4 6 3 5 ## [2295] 4 2 1 3 3 0 3 5 6 6 5 2 4 6 4 5 6 6 5 2 4 4 4 4 4 3 1 5 6 3 4 3 6 1 6 4 5 ## [2332] 1 3 5 3 2 3 2 3 3 3 5 0 4 4 4 6 6 5 3 0 4 2 1 5 4 5 3 5 4 5 6 0 4 2 5 2 5 ## [2369] 3 5 0 3 3 2 3 2 2 2 3 0 3 5 6 5 5 6 2 4 4 6 2 6 6 1 3 1 3 4 3 2 3 2 5 4 4 ## [2406] 6 4 5 2 3 6 1 2 2 5 1 1 6 3 4 2 5 5 4 4 2 3 0 5 2 5 1 1 1 2 6 1 1 4 5 3 3 ## [2443] 1 0 4 6 3 5 1 2 6 2 1 1 4 4 5 3 0 5 5 1 4 2 5 2 0 1 4 2 4 5 2 4 1 5 5 2 1 ## [2480] 6 1 6 1 3 1 2 4 5 0 3 4 5 3 5 1 4 0 0 6 4 5 6 0 4 4 2 5 4 2 4 6 3 2 2 2 0 ## [2517] 4 2 6 3 3 6 6 3 4 2 3 5 3 4 5 1 0 5 0 2 5 3 4 5 0 5 5 1 4 6 6 6 5 4 5 2 1 ## [2554] 5 1 0 4 5 4 4 6 3 3 6 2 4 5 2 6 2 2 6 2 4 3 6 1 3 3 4 6 2 5 5 6 0 2 2 5 3 ## [2591] 3 6 1 2 6 3 2 3 0 2 3 1 4 3 3 2 4 4 2 1 5 5 3 3 3 2 5 2 0 4 1 3 3 3 6 0 1 ## [2628] 1 4 1 2 1 2 6 6 2 2 3 5 2 2 1 4 2 5 4 4 6 4 4 0 4 5 4 5 5 5 6 5 5 5 3 5 3 ## [2665] 6 5 2 3 3 4 5 4 5 5 5 4 5 2 5 3 3 5 5 4 6 4 0 # PSYCHOLOGICAL WELL-BEING/DISCOMFORT (OF GENERAL HEALTH) anovaregpsychwellbeing &lt;- aov(table$PSYCH.WELLBEING~table$REGIONS) summary(anovaregpsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$REGIONS 3 46 15.340 4.569 0.00338 ** ## Residuals 2683 9008 3.358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovaregpsychwellbeing) TukeyHSD(anovaregpsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$REGIONS) ## ## $`table$REGIONS` ## diff lwr upr p adj ## MOST POPULATED-CENTER 0.28148855 0.08151381 0.4814633 0.0017158 ## NORTH-CENTER 0.22902049 -0.05554103 0.5135820 0.1636105 ## SOUTH-CENTER 0.17650527 -0.34355917 0.6965697 0.8191275 ## NORTH-MOST POPULATED -0.05246806 -0.33286583 0.2279297 0.9633105 ## SOUTH-MOST POPULATED -0.10498327 -0.62278119 0.4128146 0.9540137 ## SOUTH-NORTH -0.05251522 -0.60848288 0.5034524 0.9949688 plot(TukeyHSD(anovaregpsychwellbeing)) # significant differences # MOST POPULATED-CENTER p adj 0.0017158 #### MOST POPULATED mean = 3.206107, CENTER mean = 2.924618 tapply(table$PSYCH.WELLBEING,factor(table$REGIONS),mean) ## CENTER MOST POPULATED NORTH SOUTH ## 2.924618 3.206107 3.153639 3.101124 tapply(table$PSYCH.WELLBEING,factor(table$REGIONS),sd) ## CENTER MOST POPULATED NORTH SOUTH ## 1.836446 1.834613 1.838215 1.725774 ########################################################################## ####################### AIM 2 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four sub-periods of quarantine # PSYCHOLOGICAL WELL-BEING (OF GENERAL HEALTH) anovatemppsychwellbeing &lt;- aov(table$PSYCH.WELLBEING~table$SUB.PERIODS.IN.PRE.AND.POST) summary(anovatemppsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$SUB.PERIODS.IN.PRE.AND.POST 3 83 27.791 8.312 1.68e-05 *** ## Residuals 2683 8971 3.344 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovatemppsychwellbeing) TukeyHSD(anovatemppsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$SUB.PERIODS.IN.PRE.AND.POST) ## ## $`table$SUB.PERIODS.IN.PRE.AND.POST` ## diff lwr upr ## 2. TWO WEEK PRE-1. ONE WEEK PRE -0.1360136 -0.37421192 0.1021846 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2031072 -0.07139874 0.4776132 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.4771883 0.17578682 0.7785898 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.3391209 0.01851552 0.6597262 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.6132020 0.26928754 0.9571164 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2740811 -0.09590843 0.6440706 ## p adj ## 2. TWO WEEK PRE-1. ONE WEEK PRE 0.4571940 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2273681 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.0002823 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.0333276 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.0000283 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2264068 plot(TukeyHSD(anovatemppsychwellbeing)) # significant differences # 4. REMAINING WEEKS POST-1. ONE WEEK PRE p adj 0.0002823 # 3. ONE WEEK POST-2. TWO WEEK PRE p adj 0.0333276 # 4. REMAINING WEEKS POST-2. TWO WEEK PRE p adj 0.0000283 tapply(table$PSYCH.WELLBEING,factor(table$SUB.PERIODS.IN.PRE.AND.POST),mean) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 3.033156 2.897143 3.236264 ## 4. REMAINING WEEKS POST ## 3.510345 tapply(table$PSYCH.WELLBEING,factor(table$SUB.PERIODS.IN.PRE.AND.POST),sd) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1.823743 1.819344 1.886359 ## 4. REMAINING WEEKS POST ## 1.796256 # Figure S1: plotmeans(table$PSYCH.WELLBEING~table$SUB.PERIODS.IN.PRE.AND.POST, main=&quot;Fig. S1: Psychological well-being/discomfort by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Psychological well-being/discomfort&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) ########################################################################## ############################# THE END #################################### ########################################################################## "],["Workflows_Portfolio_2.html", "Chapter 2 Datamanagement", " Chapter 2 Datamanagement As part of assignment 2 from the DSFB2 Workflows course. For this assignment we focused on data management. This included topics like file naming, Guerilla analytics, project structures, checksums, etc. The Guerilla analytics framework is a tool to establish an efficient, trustworthy and reproducible datamanagement workflow. It is described by Enda Ridge in this booklet. The seven core principles are: Space is cheap, confusion is expensive Use simple, visual project structures and conventions Automate (everything - my addition) with program code Link stored data to data in the analytics environment to data in work products (literate programming with RMarkdown - my addition) Version control changes to data and analytics code (Git/Github.com) Consolidate team knowledge (agree on guidelines and stick to it as a team) Use code that runs from start to finish (literate programming with RMarkdown - my addition) In this assignment the focus was on principle 2, “Use simple, visual project structures and conventions”. This is important to avoid deep nesting of files and prepare for the way a project can evolve over time. If certain guidelines are not followed it will create problems, especially when handing a project over to someone else. The recommended guidelines are as follows: Create a separate folder for each analytics project. Keep the unit of a project small. Do not deeply nest folders (max 2-3 levels). Keep information about the data, close to the data. Store each dataset in its own sub-folder and create sub-folders within it for older versions. Do not change file names or move them! If you have to, record the change in the README. Do not manually edit data source files, always use code. In code, use relative paths (here::here etc) Use one R project per project and don’t reuse projects for other stuff. This principle was put into practice on a previous project, from subject DAUR2. At the time the guidelines had not been discussed so the initial structure was my own personal preference, without thinking about reproducibility. For this assignment the arrangement of the files and folders within that project have been adjusted. The directory tree is shown in figure 2.1. Since DAUR2 worked with datasets given by the school, those datasets had their own location on the shared server and were not downloaded. They would have been added to the ‘data-raw’ folder with a subfolder per dataset. Figure 2.1 Directory tree for subject DAUR2. "],["Workflows_Portfolio_3.html", "Chapter 3 Future planning", " Chapter 3 Future planning As part of assignment 3 from the DSFB2 Workflows course. In class we often focused on creating an image of what we want our future to look like. At points we are so busy studying, we lose track of why we are studying and what we are working towards. In this assignment I answered a few questions given by the teachers. With these questions I tried to look ahead and take concrete steps towards my future. Where do I want to be in ~2 years time? In two years I will have finished my studies. Which means I completed the specialisation I’m doing currently, a minor at the professorship and a graduationproject/internship outside of school. I hope to have also started my first job in the field of Data Science or Bioinformatics, with a focus on one of my interests. This is where it gets tricky because I have a lot of interests (maybe too many for my own good). The first ones that come to mind are systembiology, structural biology, biocomplexity, statistics, forensic research and I could go on but will try to keep it short. I would also like my work to reduce the need for animaltesting. How am I doing now with respect to this goal? Aside from specializing in Data Science I am trying to filter my interests. I can’t do all the things at once so I want to start narrowing my view when it comes to my career. This is very difficult for me. Luckily, the teachers are a great help and I’m getting a clearer picture of the best route to take. In my minor I’ll be focusing on applying a probabilistic approach to the risk-evaluation of toxins within an existing pipeline. Right now I think my graduationproject will focus on something along the lines of structural biochemistry. What would be the next skill to learn? Something about how software is used to describe protein structure and folding. Bas van Gestel suggested I look at AlphaFold. They have the open source code available on github and it would be useful to learn more about it. Make a planning on how to start learning this new skill. I have blocked out a couple of days before winterbreak which I will use to work on this assignment. I will dive into the code and see if I can replicate some of it. The result of this can be seen under chapter 9 ‘Free Assignment’. "],["Workflows_Portfolio_5.html", "Chapter 4 Groupproject 4.1 Technology to reduce animaltesting 4.2 ONTOX 4.3 SYSREV 4.4 SBtab and Phymdos 4.5 The Goal", " Chapter 4 Groupproject As part of assignment 5 from the DSFB2 Workflows course. Alongside the subject of Workflows we will be working in groups of three students on a project. With this project we will learn about agile methods of collaboration, apply our data science skills, create a working product and present this product. In our case we are working on a project revolving around ONTOX with Marc Teunis as our cliënt. 4.1 Technology to reduce animaltesting A goal I have within my career is reducing the need for animaltesting in biomedical research. Within the field of animaltesting there’s a focus on replacement, reduction and refinement, as described by Simmonds (2018). In the past replacement focussed on insentient material and animal species “lower” on the pylogenic scale. In recent years the use of computer software as a replacement is becoming more and more relevant (Grafström et al. 2015). Reduction focuses on properly designing the research so less animals are used and to prevent the need for more animals than initially expected. This becomes complicated when using less animals for the inital research may create the need for more animals in repetitive or supporting studies. In this case using software would not only reduce the amount of animals needed, it would also make reproduction a lot easier. The third R, refinement, is all about reducing the amount of discomfort the animals go through. This can include training the animals to get used to the treatment with positive reinforcement or the use of anesthesia (Simmonds 2018). This takes more time and resources which can be prevented by using computer software instead. One project with a focus on reducing the need for animals in human risk assessment of chemicals is ONTOX. 4.2 ONTOX In their own words “The vision of the ONTOX consortium is to provide a functional and sustainable solution for advancing human risk assessment of chemicals without the use of animals in line with the principles of 21st century toxicity testing and next generation risk assessment” (“ONTOX Project,” n.d.). One of the ways to achieve this is by collecting and combining existing data to be used in the risk assessment. Platforms like SysRev are able to extract useful data from articles. This data consists of metabolic pathways and interactions between compounds. To make the data useful for computerbased analysis it is formatted into what is called an SBtab file. This file, in turn, can be used to create visualisations of the metabolic pathways or it can be turned into other formats like SBML. Different apps (i.e. Phymdos) and functions have been created to execute these possibilities. 4.3 SYSREV Our current knowledge within the biomedical industry consists of big and complex datasets. Combining the datasets from multiple sources can help with identifying clusters and correlation between datasets, as well as developing predictive models (Ristevski and Chen 2018). The amount of existing literature and data is hard to grasp. One way to make this easier is by using software platforms like SysRev. SysRev is a collaborative platform for the extraction of data from documents (“Built for Data Miners | Sysrev,” n.d.). It speeds up the extraction of useful data from documents by presenting it in a shorter format. This researcher will be able to go over more data in the same amount of time. Existing data about risk assessment on humans may be more accurate than performing the same risk assessment on a new samplegroup of animals. As shown by Perel et al. (2007) the treatment effects in animal experiments can differ greatly from the ones in clinical trials. Simon Festing, executive director of RDS, once stated “Animals are better used for understanding disease mechanisms and potential new treatments, rather than predicting what will happen in humans,” (“The Trouble with Animal Models,” n.d.). In order to predict risk in humans it is important to look at our specific biological systems instead of other organisms. Aside from performing clinical trials the existing data should be used more optimally. 4.4 SBtab and Phymdos One of the possible output formats used by SysRev is called SBtab. This format is a set of syntax rules, created to simplify data processing, and can be interconverted with the well known SBML format (“SBtab - Standardised Data Tables for Systems Biology,” n.d.). One app which can perform this conversion is Phymdos. The app was created by D. Roodzant for University of Applied Sciences Utrecht. It can take SBtab files, SBML files or extract them directly from SysRev. It allows the user to create SBML style physiological maps in the desired format. This app demonstrates the range of possiblities with this type of data. 4.5 The Goal The current process of extracting and analyzing SBtab files is only applicable to single files. Looking ahead, ONTOX strives to improve human risk assessment by combining multiple sources for the same goal. In order to use this existing process for the bigger picture, it should be combining data instead of looking at one particular set. This is why Marc Teunis asked us to make an R package with the following functions: Extract the SBtab data from SysRev for one or more articles. Merge multiple SBtab files into one new file. Create a graph, visualizing the data based on the merged SBtab file. Convert the SBtab file to another format like SBML. "],["Workflows_Portfolio_7.html", "Chapter 5 Relational data 5.1 Connecting to DBeaver 5.2 Data inspection 5.3 Joining datasets 5.4 Descriptive statistics 5.5 Visualisations", " Chapter 5 Relational data As part of assignment 7 from the DSFB2 Workflows course. So far I have been working with R, BASH, HTML and CSS. Another important language to understand is SQL. With this assignment I’ll focus on learning the basics of SQL and applying it to relational data. To start the dataframes will be created and tidied up. # Importing the flu dataframe flu_df &lt;- read_csv(&quot;data-raw/data070/flu_data.csv&quot;, skip=11) ## First 11 rows are metadata # Making the flu dataframe tidy flu_df_tidy &lt;- flu_df %&gt;% pivot_longer(cols = c(2:ncol(flu_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Importing the dengue dataframe dengue_df &lt;- read_csv(&quot;data-raw/data070/dengue_data.csv&quot;, skip=11) ## First 11 rows are metadata # Making the dengue dataframe tidy dengue_df_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = c(2:ncol(dengue_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # The third dataframe is the gapminder dataframe from the dslabs package # This dataframe is already tidy To make the data more relational a couple things have to be changed. The columns describing the countries and dates should be comparable across the different dataframes so they can be joined later. This was achieved with the following code. # All &#39;country&#39; columns should be the same in type, class and values # For flu and dengue they&#39;re characters, for gapminder it&#39;s a factor # Turning the &#39;country&#39; column of both dengue and flu into a factor flu_df_tidy$country &lt;- as.factor(flu_df_tidy$country) dengue_df_tidy$country &lt;- as.factor(dengue_df_tidy$country) # All &#39;date&#39; columns should be the same in type, class and values # For flu and dengue the dates are specified to the day instead of the year # We&#39;ll create a new &#39;year&#39; column for both which matches the column of gapminder flu_df_tidy &lt;- flu_df_tidy %&gt;% mutate(year = substr(flu_df_tidy$Date, start=1, stop=4)) dengue_df_tidy &lt;- dengue_df_tidy %&gt;% mutate(year = substr(dengue_df_tidy$Date, start=1, stop=4)) # Now they&#39;re characters, they should be integers flu_df_tidy$year &lt;- as.integer(flu_df_tidy$year) dengue_df_tidy$year &lt;- as.integer(dengue_df_tidy$year) The dataframes were exported to .csv and .rds for later use. # Export the three dataframes as csv write.csv(flu_df_tidy, &quot;data/data070/flu.csv&quot;, row.names=FALSE) write.csv(dengue_df_tidy, &quot;data/data070/dengue.csv&quot;, row.names=FALSE) write.csv(gapminder, &quot;data/data070/gapminder.csv&quot;, row.names=FALSE) # Export the three dataframes as rds saveRDS(flu_df_tidy, file=&quot;data/data070/flu.rds&quot;) saveRDS(dengue_df_tidy, file=&quot;data/data070/dengue.rds&quot;) saveRDS(gapminder, file=&quot;data/data070/gapminder.rds&quot;) 5.1 Connecting to DBeaver To move from R to SQL (PostgreSQL) a database has been created in DBeaver called ‘workflowsdb’. The three R dataframes were imported into the database using RPostgreSQL. The tables were inserted into the hierarchy of this database as shown in figure 4.1. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;insertpassword&quot;) # Password changed after connecting for privacy reasons # Using the DBI library dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbWriteTable(con, &quot;flu&quot;, flu_df_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_df_tidy) Figure 4.1. The hierarchy of the workflowsdb database including the three tables. 5.2 Data inspection The data was inspected using the SQL editor. A script was created for the inspection of the gapminder, flu and dengue data. R can also be used for the same inspections. SQL script Gapminder -- Checking the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;gapminder&#39;; -- Checking the timeframe in which the data has been collected select distinct year from gapminder order by year desc; -- from 1960 to 2016 -- Checking the countries with the highest and lowest average population over this timeframe select country, AVG (population):: NUMERIC(11,0) from gapminder group by country order by avg desc; -- China has the highest with around 1,000,000,000 and Greenland the lowest with around 51,000 -- Checking the countries with the highest and lowest average life expectancy per year over this timeframe select country, AVG (life_expectancy):: NUMERIC(6,2) from gapminder where life_expectancy is not null group by country order by avg desc; -- Iceland has the highest with 78.17 and Central Africa Republic the lowest with 46.31 -- Checking how many countries are noted in the data select count(distinct country) from gapminder; -- 185 different countries SQL script flu data inspection -- First we check the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;flu&#39;; -- We check in which timeframe the data has been collected select distinct year from flu order by year desc; -- from 2002 to 2015 AVG (cases):: NUMERIC(6,1) from flu group by country order by avg desc; -- South Africa had the most with 2698.8 on average, Sweden the least 5.5 -- We check how many countries are noted in the data and if they are spelled correctly select distinct country from flu; -- 29 different countries, all spelled correctly SQL script dengue data inspection -- First we check the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;dengue&#39;; -- We check in which timeframe the data has been collected select distinct year from dengue order by year desc; -- from 2002 to 2015 -- We check which countries had the most and least dengue cases on average over this timeframe select country, AVG (cases):: NUMERIC(6,4) from dengue group by country order by avg desc; -- Venezuela had the most with 0.2774 on average, Bolivia the least with 0.0447 -- We check how many countries are noted in the data and if they are spelled correctly select distinct country from dengue; -- 10 different countries, all spelled correctly R script data inspection # Checking the columnnames and columntypes str(gapminder_df) ## &#39;data.frame&#39;: 10545 obs. of 9 variables: ## $ country : Factor w/ 185 levels &quot;Albania&quot;,&quot;Algeria&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ year : int 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... ## $ infant_mortality: num 115.4 148.2 208 NA 59.9 ... ## $ life_expectancy : num 62.9 47.5 36 63 65.4 ... ## $ fertility : num 6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ... ## $ population : num 1636054 11124892 5270844 54681 20619075 ... ## $ gdp : num NA 1.38e+10 NA NA 1.08e+11 ... ## $ continent : Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 4 1 1 2 2 3 2 5 4 3 ... ## $ region : Factor w/ 22 levels &quot;Australia and New Zealand&quot;,..: 19 11 10 2 15 21 2 1 22 21 ... str(flu_df_tidy) ## tibble [17,266 × 4] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:17266], format: &quot;2002-12-29&quot; &quot;2002-12-29&quot; ... ## $ country: Factor w/ 29 levels &quot;Argentina&quot;,&quot;Australia&quot;,..: 6 19 6 19 6 9 19 6 9 19 ... ## $ cases : num [1:17266] 174 329 162 315 174 1 314 162 0 267 ... ## $ year : int [1:17266] 2002 2002 2003 2003 2003 2003 2003 2003 2003 2003 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:1845] 1 2 3 4 5 7 8 9 10 11 ... ## ..- attr(*, &quot;names&quot;)= chr [1:1845] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... str(dengue_df_tidy) ## tibble [6,263 × 4] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:6263], format: &quot;2002-12-29&quot; &quot;2002-12-29&quot; ... ## $ country: Factor w/ 10 levels &quot;Argentina&quot;,&quot;Bolivia&quot;,..: 2 3 4 5 8 2 3 4 5 8 ... ## $ cases : num [1:6263] 0.101 0.073 0.062 0.101 0.059 0.143 0.098 0.047 0.039 0.059 ... ## $ year : int [1:6263] 2002 2002 2002 2002 2002 2003 2003 2003 2003 2003 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:327] 1 6 7 9 10 11 16 17 19 20 ... ## ..- attr(*, &quot;names&quot;)= chr [1:327] &quot;1&quot; &quot;6&quot; &quot;7&quot; &quot;9&quot; ... # Checking the timeframes paste(&quot;From&quot;, min(unique(gapminder_df$year)), &quot;to&quot;, max(unique(gapminder_df$year))) ## [1] &quot;From 1960 to 2016&quot; paste(&quot;From&quot;, min(unique(flu_df_tidy$year)), &quot;to&quot;, max(unique(flu_df_tidy$year))) ## [1] &quot;From 2002 to 2015&quot; paste(&quot;From&quot;, min(unique(dengue_df_tidy$year)), &quot;to&quot;, max(unique(dengue_df_tidy$year))) ## [1] &quot;From 2002 to 2015&quot; # Checking the highest and lowest average population in the gapminder data options(scipen=999) gapminder_pop &lt;- gapminder_df %&gt;% group_by(country) %&gt;% summarize(average = mean(population, na.rm = TRUE)) gapminder_pop %&gt;% filter(average == max(gapminder_pop$average) | average == min(gapminder_pop$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 China 1068197798. ## 2 Greenland 50984. # Checking the highest and lowest average life expectancy in the gapminder data gapminder_life &lt;- gapminder_df %&gt;% group_by(country) %&gt;% summarize(average = mean(life_expectancy, na.rm = TRUE)) gapminder_life %&gt;% filter(average == max(gapminder_life$average) | average == min(gapminder_life$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 Central African Republic 46.3 ## 2 Iceland 78.2 # Checking the most and least average flu cases in the flu data flu_cases &lt;- flu_df_tidy %&gt;% group_by(country) %&gt;% summarize(average = mean(cases, na.rm = TRUE)) flu_cases %&gt;% filter(average == max(flu_cases$average) | average == min(flu_cases$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 South Africa 2699. ## 2 Sweden 5.55 # Checking the most and least average dengue cases in the dengue data dengue_cases &lt;- dengue_df_tidy %&gt;% group_by(country) %&gt;% summarize(average = mean(cases, na.rm = TRUE)) dengue_cases %&gt;% filter(average == max(dengue_cases$average) | average == min(dengue_cases$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 Bolivia 0.0447 ## 2 Venezuela 0.277 # Checking how many countries took part length(unique(gapminder_df$country)) # 185 countries ## [1] 185 length(unique(flu_df_tidy$country)) # 29 countries ## [1] 29 length(unique(dengue_df_tidy$country)) # 10 countries ## [1] 10 5.3 Joining datasets The three datasets have two variables in common, country and year. Both the flu and dengue datasets have multiple observations per country per year. Sadly, the flu and dengue databases are very unclear. Since the data is old and the links in the metadata are no longer working it is hard to estimate the details of this data. Per how many inhabitants are these numbers? What were the criteria to be considered a case? Other factors, like the option to get a diagnosis, should also be considered. To combine the datasets the average amount of weekly cases per year per country was used. This was a better option than total average per year because not every country has data for every week of each year, therefore taking the sum would not be accurate. The dengue dataset has the least amount of countries. When joining the data only the countries seen in all three datasets will be kept, these are Argentina, Bolivia, Brazil and Mexico. Click here for the full SQL script written to join the three datasets based on country and year. The joined table has been imported into R using the following code. gapminder_flu_dengue &lt;- read_csv(&quot;data/data070/gapminder_flu_dengue.csv&quot;) 5.4 Descriptive statistics With this new joined table we can perform some descriptive statistics. For example, looking at the mean per country per variable, the highest/lowest values and the median. # Get the mean and stdev values per country gfd_population_summ &lt;- gapminder_flu_dengue %&gt;% group_by(country)%&gt;% summarize(average = mean(population), max = max(population), min = min(population), median = median(population)) gfd_life_summ &lt;- gapminder_flu_dengue %&gt;% group_by(country)%&gt;% summarize(average = mean(life_expectancy), max = max(life_expectancy), min = min(life_expectancy), median = median(life_expectancy)) gfd_gdp_summ &lt;- gapminder_flu_dengue %&gt;% group_by(country)%&gt;% summarize(average = mean(gdp, na.rm = TRUE), max = max(gdp, na.rm = TRUE), min = min(gdp, na.rm = TRUE), median = median(gdp, na.rm = TRUE)) gfd_flu_deng_summ &lt;- gapminder_flu_dengue %&gt;% group_by(country)%&gt;% summarize(avg_flu = mean(flu_cases), max_flu = max(flu_cases), min_flu = min(flu_cases), median_flu = median(flu_cases), avg_dengue = mean(dengue_cases), max_dengue = max(dengue_cases), min_dengue = min(dengue_cases), median_dengue = median(dengue_cases)) 5.5 Visualisations We can also create visualizations to get an overall look of how the countries compare to each other and how the numbers changed through the years. 5.5.1 Dengue cases In figure 4.2 a scatterplot shows the average amount of weekly dengue cases per year with a different color for each country. In 2009 three of the four countries had a steep increase in cases compared to the years before. It is noteworthy that the guidelines for the classification of dengue were revised in 2009 by the World Health Organization (2009). This revision is considered an improvement but, being less stringent, will have increased the amount of diagnoses (van de Weg et al. 2012). # Visualising the weekly dengue cases per country per year in a scatterplot ggplot(gapminder_flu_dengue, aes(x=year, y=dengue_cases, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Dengue cases&quot;, y = &quot;Average dengue cases per week&quot;, x = &quot;Year&quot;, caption = &quot;Figure 4.2. Scatterplot to show the average amount of weekly dengue cases per year per country.&quot;) + scale_x_continuous(breaks = round(seq(min(gapminder_flu_dengue$year), max(gapminder_flu_dengue$year), by = 1),1)) + theme_minimal() After seeing this graph I have decided to remake it without the joined table. Joining the tables has decreased the amount of countries in the table since 6 of the 10 countries in the dengue database were not seen in the flu database. It would be a waste of information not to make this graph with all the data I currently have access to. In figure 4.3 the new scatterplot is shown. Here we see how the added countries did not show a significant increase in cases in 2009, unlike what the previous graph showed. Letting out this information by joining the databases with other could therefore have been misleading. # The tidy dengue data will be used and modified to get the average amount of weekly cases per year dengue_graph &lt;- dengue_df_tidy %&gt;% group_by(year, country) %&gt;% summarise(avg_cases = mean(cases)) view(dengue_graph) # Visualising the weekly dengue cases per country per year in a scatterplot ggplot(dengue_graph, aes(x=year, y=avg_cases, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Dengue cases&quot;, y = &quot;Average dengue cases per week&quot;, x = &quot;Year&quot;, caption = &quot;Figure 4.3. Scatterplot to show the average amount of weekly dengue cases per year per country.&quot;) + scale_x_continuous(breaks = round(seq(min(gapminder_flu_dengue$year), max(gapminder_flu_dengue$year), by = 1),1)) + theme_minimal() 5.5.2 Flu cases To prevent the misleading visualizations, seen for the dengue cases, a new joined table was made to analyse the flu cases. This time only the flu and gapminder data have been joined using this SQL script. The table was imported to R and the descriptive statistics were executed again, this time based on the the continent options(scipen=999) gf_continent &lt;- read_csv(&quot;data/data070/gapminder_flu_continent.csv&quot;) # In 2002 not all countries were noted so this year will be left out of the analysis gf_continent &lt;- gf_continent %&gt;% filter(year != 2002) # Get the average value per continent per variable gf_continent_summ &lt;- gf_continent %&gt;% group_by(continent)%&gt;% summarize(population = mean(population), life_expectancy = mean(life_expectancy), flu_cases = mean(flu_cases), gdp = mean(gdp, na.rm=TRUE)) # Make it useable for the bar graph gf_continent_bar &lt;- gf_continent_summ %&gt;% mutate(&quot;Population&quot; = round(population/max(population)*100, 1), &quot;Weekly flu cases&quot; = round(flu_cases/max(flu_cases)*100, 1)) gf_continent_bar &lt;- gf_continent_bar %&gt;% select(&quot;continent&quot;, &quot;Population&quot;, &quot;Weekly flu cases&quot;) gf_continent_bar &lt;- gf_continent_bar %&gt;% pivot_longer(cols = c(&quot;Population&quot;, &quot;Weekly flu cases&quot;), names_to = &quot;Statistics&quot;, values_to = &quot;Percentage&quot;) Because of the incomplete metadata of the flu database the relation between the population and the amount of weekly flu cases should first be looked at. This can give us an insight to wether the values for flu cases are per a certain amount of inhabitants. The bar graph (shown in figure 4.4) shows how a smaller population compared to other continents also results in a smaller amount of weekly flu cases. This indicates the data in the flu database has not been normalized for the number of inhabitants. Africa is an outlier in this with relatively more weekly flu cases than Asia but a smaller population. # Make a bar graph with the relative average population and weekly flu cases ggplot(gf_continent_bar, aes(x=reorder(continent, -Percentage), y=Percentage, fill=Statistics)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge()) + labs(title = &quot;Population and flu cases&quot;, y = &quot;Percentage&quot;, x = NULL, caption = &quot;Figure 4.4. The relative average population and weekly flu cases from 2003 to 2015 per continent.&quot;) + theme_minimal() For further analysis a scatterplot has been made, shown in figure 4.5. The sudden increase in 2009, only to go back down again in 2010, is noteworthy. In 2009 there was a pandemic called the Swine Flu which might be the cause of this sudden jump in flu cases for all continents (Akin and Gözel 2020). This also explains why Oceania was affected less, as it is more secluded from other continents being surrounded by ocean. # Visualising the weekly dengue cases per continent per year in a scatterplot ggplot(gf_continent, aes(x=year, y=flu_cases, group = continent, color = continent)) + geom_point() + geom_line() + labs(title = &quot;Weekly flu cases across the world&quot;, y = &quot;Weekly flu cases&quot;, x = &quot;Year&quot;, caption = &quot;Figure 4.5. Scatterplot to show the average amount of weekly flu cases per year per continent.&quot;) + scale_x_continuous(breaks = round(seq(min(gf_continent$year), max(gf_continent$year), by = 1),1)) + theme_minimal() "],["Workflows_Portfolio_8.html", "Chapter 6 R packages", " Chapter 6 R packages As part of assignment 8 from the DSFB2 Workflows course. "],["Workflows_Portfolio_9.html", "Chapter 7 Parameters", " Chapter 7 Parameters As part of assignment 9 from the DSFB2 Workflows course. To be added. "],["Workflows_Portfolio_Free.html", "Chapter 8 Free assignment", " Chapter 8 Free assignment To be added. "],["Workflows_Portfolio_References.html", "Chapter 9 References", " Chapter 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
