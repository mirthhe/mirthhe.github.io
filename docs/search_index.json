[["index.html", "Curriculum Vitae Contact Profile Job experience Education Skills Volunteering Languages", " Curriculum Vitae Contact Utrecht, The Netherlands mirtheklaassen@outlook.com Linkedin Instagram Profile I’m goal-oriented, curious, analytical and versatile with a passion for molecular biology and data science. I want to apply my skills in unraveling patterns and pathways in the field of bioinformatics. Along with combining my wide range of interests this would give me the opportunity of contributing to the development of big data analysis and support biomedical research. Job experience Management Assistent, Fitness Factory, Utrecht June 2017 - Oktober 2018 My responsibilities included the remodelling projects, contact with externals, hiring and teaching new employees, basic accountancy, inventory management and technical maintenance. Also active as spinning-instructor. Examtrainer Math, Lyceo, Utrecht March 2018 - September 2018 Sales employee, Lens Utrecht, Utrecht May 2018 - August 2019 Management Assistant, TrainMore Black Label, Utrecht August 2019 - September 2020 My responsibilities included directing the sales-team, technical maintenance, inventory management, member administration, contacting debtors/debt collection agencies and contact with externals. Also active as kickboxing- and HIIT-instructor. Webdeveloper, WisMon, Utrecht March 2021 - September 2021 Sales employee, Pipoos, Utrecht September 2022 - Present Education Communication &amp; Multimedia design, Avans, Den Bosch September 2015 - December 2017 With a personal focus on front-end webdevelopment and datavisualisation Optometry, University of Applied Sciences, Utrecht September 2018 - Augustus 2019 Biology &amp; Medical Labresearch, University of Applied Sciences, Utrecht September 2020 - Present GPA of 4.0 Honours star in genetic recombination and transfection Specialization in Data Science for Biology Skills Informatics: Basic R, basic BASH, HTML and CSS Design: Adobe Photoshop, Adobe Illustrator and Adobe InDesign Volunteering Design-expert, Innovation in a Week, Veghel July 2016 Won the ‘Teamwork-award’ Student Association Uerius Brabus, Den Bosch October 2015 - December 2016 Active as co-founder, captain of the PR committee and active in the committee for external contacts Languages Dutch: native speaker English: level C1 "],["Workflows_Portfolio_1.html", "Chapter 1 Reproducible Research 1.1 Excel’s downsides 1.2 Repita criteria 1.3 Open source code", " Chapter 1 Reproducible Research As part of assignment 1 from the DSFB2 Workflows course. 1.1 Excel’s downsides 1.1.1 Inspecting an Excel file As part of assignment 1.1 I was asked to review this file, by opening the file in Excel. The experimental condition ‘ControlVehicleA’ stood out to me. It appears to be the same as the ControlPositive but with less Ethanol. I’m guessing this shows the experimental conditions without the experimental compounds which would mean the compounds have been diluted in ethanol. There are also extra sheets with lists of inputs, logs of changes and input examples. I’m not sure what the difference in purpose is between the ExpLookup sheet and the Input example sheet. Three different compounds have been tested, 2,6-diisopropylnaphthalene, decane and naphthalene. The positive control for this experiment uses 1.5% ethanol in S-medium. The negative control for this experiment uses just S-medium without an added compound. Next up, the file was opened in R and the data types per column were checked, these seemed to differ from the expectations. # Read the excel file CE.LIQ.FLOW.062_Tidydata &lt;- read_excel( &quot;data-raw/data011/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) The column RawData was recognized as a double while I’d expect an integer, compName was a character while I expected a factor and compConcentration was also character while it should be a double. It had not been assigned correctly. For further analysis the types were changed to the correct ones using the following code. # The compConcentration has to be turned into a numeric value CE.LIQ.FLOW.062_Tidydata$compConcentration &lt;- as.double(CE.LIQ.FLOW.062_Tidydata$compConcentration) # The RawData has to be turned into an integer CE.LIQ.FLOW.062_Tidydata$RawData &lt;- as.integer(CE.LIQ.FLOW.062_Tidydata$RawData) # Turn the data into a tibble CE.LIQ.FLOW.062_tbl &lt;- as.tibble(CE.LIQ.FLOW.062_Tidydata) # The compName&#39;s have to be turned into a factor # Create a vector with the levels compName_factor &lt;- c(&quot;2,6-diisopropylnaphthalene&quot;, &quot;decane&quot;, &quot;naphthalene&quot;, &quot;Ethanol&quot;, &quot;S-medium&quot;) # Use the levels to create the factor CE.LIQ.FLOW.062_tbl$compName &lt;- factor(CE.LIQ.FLOW.062_tbl$compName, levels = compName_factor) # Check if it worked # str(CE.LIQ.FLOW.062_tbl) # it did! 1.1.2 Creating the scatterplot Now that the columntypes are corrected the data can be used to create a scatterplot. The assignment said to put the compConcentration on the x-axis, the DataRaw counts on the y-axis and to assign a colour to each level in compName. It also asked to assign a different symbol to each level in the expType variable. Since the compUnit of the controls are not nM but percentages, they have been added as dashed lines instead of measured values. This means the different shapes for expType variables are not neccessary since only “experiment” is shown out of the possible ExpType values. In the assignment the initial scatterplot was followed by a question about the x-axis, “When creating the plot under C), what happened with the ordering of the x-axis labels. Explain why this happens.”. The compConcentration had already been changed to the correct type in the previous code chunk, so nothing went wrong when creating the scatterplot. Had the type not been changed beforehand, the x-axis would have been in alphabetical orders because they were seen as characters instead of doubles. The x-axis is shown with a log10 transformation and slight jitter has been added for better readability. A few details were added on top of the given assignment. This included each compound getting it’s own colour, the standard deviations being added as errorbars and the control values being added as dashed lines for comparison. The resulting plot is shown in figure 1.1. # Get the average counts per compound per concentration, keep the expType variable too CE.LIQ_summ &lt;- CE.LIQ.FLOW.062_tbl %&gt;% group_by(compName, compConcentration, expType) %&gt;% summarize(mean_counts = mean(RawData, na.rm = TRUE), stdev_counts = sd(RawData, na.rm = TRUE)) # Create a tibble containing only the data used for the graph CE.LIQ_summ_exp &lt;- CE.LIQ_summ %&gt;% filter(expType == &quot;experiment&quot;) # Create the scatterplot CE.LIQ_summ_exp %&gt;% ggplot(aes(x = log10(compConcentration) # log10 as suggested in question F , y = mean_counts)) + geom_smooth(aes(group=compName, color = compName), span= .5, size=0.2) + geom_point(aes(color = compName), position = graph_jitter(0.2)) + geom_errorbar(aes(ymin=mean_counts-stdev_counts, ymax=mean_counts+stdev_counts, color = compName), width=.1, position = graph_jitter(0.2)) + labs(title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count&quot;, x = &quot;Compound concentration in log10(nM)&quot;, caption = graph_cap(&quot;Figure 1.1 Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations.&quot;)) + geom_hline(yintercept = 85.9, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control negative geom_text(aes(0,85.9, label = &quot;Control negative&quot;, vjust=-0.3), size = 3.5) + geom_hline(yintercept = 49.4, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control positive geom_text(aes(-3.5,49.4, label = &quot;Control positive&quot;, vjust=-0.3), size = 3.5) + theme_minimal() 1.1.3 Normalize for the negative control In order to see how much the offspring count improved or decreased relatively to the baseline (in this case, the negative control) the data must be normalized. To do this we adjust the negative control to a value of 1 and adjust the other values in the same way. In this case that means dividing by 85.9. The scatterplot was created again with the new normalized data, further settings stayed the same, the result is shown in figure 1.2. # Check the mean value of controlNegative # view(CE.LIQ_summ) # It&#39;s 85.9 # Add the column with normalized average counts CE.LIQ_summ_norm &lt;- CE.LIQ_summ_exp %&gt;% mutate(norm_counts = mean_counts/85.9, norm_stdev = stdev_counts/85.9) CE.LIQ_summ_norm %&gt;% ggplot(aes(x = log10(compConcentration), # log10 as requested in question F y = norm_counts)) + geom_smooth(aes(group = compName, color = compName), span=.5, size = 0.2) + geom_point(aes(color = compName), position = graph_jitter(0.04)) + geom_errorbar(aes(ymin=norm_counts-norm_stdev, ymax=norm_counts+norm_stdev, color = compName), width=.1, position = graph_jitter(0.04)) + labs(title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count relative to negative control&quot;, x = &quot;Compound concentration in log10(nM)&quot;, caption = graph_cap(&quot;Figure 1.2 Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations. Normalized to negative control == 1.&quot;)) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, size = 0.2) + # Show control negative geom_text(aes(0,1, label = &quot;Control negative&quot;, vjust=-0.3), size = 3.5) + geom_hline(yintercept = 0.57508731, linetype = &quot;dashed&quot;, size = 0.2) + # Show control positive geom_text(aes(-3.5,0.57508731, label = &quot;Control positive&quot;, vjust=-0.3), size = 3.5) + theme_minimal() 1.1.4 Statistical analysis In order to learn wether there is indeed an effect of the different compounds a few tests need to be performed. Starting with the Shapiro-Wilk test to check the normality of the data and a Levene’s test to check for equality of variance. Followed by an ANOVA with the different concentrations as added variable. To see the actual difference, the analysis should be finished with a Post hoc test like Tukey. 1.2 Repita criteria 1.2.1 Criteria for reproducibility This first part of assignment 1.2 will look at ‘Repita’ criteria. These criteria are used to check for the reproducibility of scientific research articles. In table 1.1 the different criteria are shown with a definition and the type of response it calls for. In order to further understand and apply these criteria, a research article will be analysed to check if this article follows the ‘Repita’ guidelines as shown in table 1.1. # Create the different variables needed for the repita criteria table Transparency_Criteria &lt;- c(&quot;Study Purpose&quot;, &quot;Data Availability Statement&quot;, &quot;Data Location&quot;, &quot;Study Location&quot;, &quot;Author Review&quot;, &quot;Ethics Statement&quot;, &quot;Funding Statement&quot;, &quot;Code Availability&quot;) Definition &lt;- c(&quot;A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.&quot;, &quot;A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.&quot;, &quot;Where the article’s data can be accessed, either raw or processed.&quot;, &quot;Author has stated in the methods section where the study took place or the data’s country/region of origin.&quot;, &quot;The professionalism of the contact information that the author has provided in the manuscript.&quot;, &quot;A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.&quot;, &quot;A statement within the manuscript indicating whether or not the authors received funding for their research.&quot;, &quot;Authors have shared access to the most updated code that they used in their study, including code used for analysis.&quot;) Response_Type &lt;- c(&quot;Binary&quot;, &quot;Binary&quot;, &quot;Found Value&quot;, &quot;Binary;Found Value&quot;, &quot;Found Value&quot;, &quot;Binary&quot;, &quot;Binary&quot;, &quot;Binary&quot;) # Turn the variables into one dataframe repita_criteria &lt;- data.frame(Transparency_Criteria, Definition, Response_Type) # Use the dataframe to create the table repita_criteria %&gt;% kable(col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;Table 1.1 The repita criteria as given in portfolio assignment 1.2 in [lesson 1 of the reader](https://lesmaterialen.rstudio.hu.nl/workflows-reader/represintro.html).&quot;) (#tab:repita criteria)Table 1.1 The repita criteria as given in portfolio assignment 1.2 in lesson 1 of the reader. Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary;Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary 1.2.2 The article Plosone was used to find a suitable article. The found article is a primary article describing emperical scientific findings by Bruckner et al. (2022). The focus is on how microbiotas play a role in developmental programs. Altered microbiota composition appears to be linked to neurodevelopmental conditions such as autism spectrum disorder. One of the findings described how the microbiota can influence forebrain neurons. Forebrain neurons are required for normal social behavior and localization of forebrain microglia. The study uses zebrafish which were kept at 28°C with a 14/10 light/dark cycle. For the controlgroup conventionalized (CVZ) fish were used. The experimental condition consisted of germ-free (GF) fish. These fish were made to be germ-free and therefore did not have their microbiota. When the fish were older (7 dpf) they were inoculated with normal microbiota (making them ex germ-free or XGF) and their optomotor respons was tested. Once the fish were adults (14 dpf) their social behavior was tested. The social behavior was assessed through a dyad assay for postflexion larval and adult zebrafish. For each condition a pair of siblings were placed in isolated tanks and allowed to interact for 10 minutes via transparent tank walls. Social interaction was defined as the average relative distance from the divider and the percentage of time spent orienting at 45° to 90°. These parameters were measured and analyzed using computer vision software written in Python (available at https://github.com/stednitzs/daniopen). Compared to the CVZ siblings, the GF larvae spent significantly less time in close proximity to and oriented at 45 to 90°. These results show that an intact microbiota is required during the early life-stages for later development of normal social behavior. Aside from the social behavior, the study also looked at optomotor responses. It is possible that the microbiota influences circuitry underlying the early vision and locomotion required for social behavior. To address this possibility, the vision and locomotion were assayed by comparing kinetics of the optomotor response to virtual motion in GF larvae and CVZ controls. Optomotor response was assessed using a “virtual reality” system for assessing zebrafish behavior, measuring swim response in larvae. A visual stimulus was projected on a screen underneath the dishes for 20 seconds and consisted of concentric rings moving toward the dish center, followed by a 20-second refractory period. Responses are the average of 46 to 59 stimulus trials per fish, presented over 1 hour. In this part of the study, no significant difference were found between the GF and CVZ fish. This suggests that the microbiota influences circuits specific to social behavior directly, rather than by modulating vision or locomotion. The results demonstrate that the microbiota influences zebrafish social behavior by stimulating microglial remodeling of forebrain circuits during early neurodevelopment. This conclusion suggests pathways for new interventions in multiple neurodevelopmental disorders. 1.2.3 The reproducibility To better understand the criteria (table 1.1) they will be applied to the found article. The results can be found in table 1.2. While answering this assignment I noticed how this is not as black and white as I thought. For example with the study location. At first I thought it was stated, because University of Oregon came back quite a few times, looking at it more closely it was not clear if that was the actual study location. # Create the different variables needed for the article check table Score &lt;- c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;[Link](https://figshare.com/projects/Bruckner_et_al_Data/136756)&quot;, &quot;FALSE&quot;, &quot;Non professional&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;) Explanation &lt;- c(&quot;In the second to last paragraph of the introduction the study objective is described.&quot;, &quot;On the left-hand side of the webpage the \\&quot;Accesibble Data\\&quot; can be found&quot;, &quot;The data can be accessed through the given link, found under the \\&quot;Accesibble Data\\&quot; header.&quot;, &quot;The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. &quot;, &quot;Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given.&quot;, &quot;There is an ethics statement in the methods paragraph.&quot;, &quot;The funding is stated under the abstract.&quot;, &quot;The data analysis is explained but the code has not been shared.&quot;) # Turn the variables into one dataframe article_repita_check &lt;- data.frame(Transparency_Criteria, Score, Explanation) # Use the dataframe to create the table article_repita_check %&gt;% kable(col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;Table 1.2 The score for each repita criterium when looking at the research done by Bruckner et al.(2022)&quot;) (#tab:article check)Table 1.2 The score for each repita criterium when looking at the research done by Bruckner et al.(2022) Transparency Criteria Definition Response Type Study Purpose TRUE In the second to last paragraph of the introduction the study objective is described. Data Availability Statement TRUE On the left-hand side of the webpage the “Accesibble Data” can be found Data Location Link The data can be accessed through the given link, found under the “Accesibble Data” header. Study Location FALSE The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. Author Review Non professional Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given. Ethics Statement TRUE There is an ethics statement in the methods paragraph. Funding Statement TRUE The funding is stated under the abstract. Code Availability FALSE The data analysis is explained but the code has not been shared. 1.3 Open source code In this second part of assignment 1.2 we’ll be looking at the code from an existing study. The focus will be on understanding the code, noting what it tries to achieve, the readability, reproducibility, fixing errors, etc. 1.3.1 The article The article used for this assignment had to live up to a few criteria. First up, the data had to be analysed using R code. This code and the dataset had to be available. The chosen article is “Mental Health Impacts in Argentinean College Students During COVID-19 Quarantine” by López Steinmetz et al. (2021). The study aimed to analyze differences in mental health in college students who were exposed to different spread-rates of COVID-19. They also wanted to analyze the between group differences in mental health indicators at four different quarantine sub-periods. To do this a cross-sectional design was used. The sample included 2687 Argentinean college students and the data was collected online during the Argentinean quarantine. They used a one-way between-groups ANOVA with Tukey’s post hoc test for the analysis. The results showed that the center and most populated area only differed in psychological well-being and negative alcohol related consequences, but not in the other indicators. For the sub-periods there were differences in psychological well-being, social functioning and coping, psychological distress and negative alcohol-related consequences. Negative alcohol-related consequences were the only MHS indicator improving over time. This worsened mental health suggests that quarantine and its extensions contribute to negative mental health impacts. 1.3.2 The code The code, as well as the dataset, for the study were made available through OSF. The original can be found here and the version with my adjustments can be found at the bottom of this page. The dataset and the code were added to the repository as well. After reading the code it appears to consist of a few parts. Reference to the manuscript Loading the data and packages Methods in general Sample size Distribution by sex, also noted in percentages Mean, median and stdev of the age of the students Distribution by province in percentages Methods for data analysis Test of skewness per mental health indicator Test or Kurtosis per mental health indicator with criteria and criteria reference Division based on region for first aim Division based on sub-periods for second aim Results for aim 1 (differences in MHS based on region) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per region Results for aim 2 (differences in MHS based on sub-period) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per sub-period Plot with means per sub-period and 95% confidence interval The code is used to make the data clear and tidy by first noting details like sample size, distributions in percentages and tests of skewness. Then it moves on to perform statistical tests for the two different aims of the study. It shows the significant differences and creates plots for the second aim. It was very easy to read the code, I would give it a 5/5 on readability. It was made especially easy by the clear headers and comments. 1.3.3 Excuting the code To further inspect the code it will be executed on the dataset given by the study. To do this one word in the code was changed in order to link the dataset. The word “clipboard” under comment “Load the dataset” was changed into raw-data/data012/dataset.xlsx, the location of the downloaded dataset. Errors will be noted and the code will be changed accordingly. 1.3.3.1 Changes and errors when executing The original code uses read.table and the clipboard to load the data. This can very quickly be done wrong. To fix this the readxl package was added and the excel file was added using table&lt;-read_excel(“data-raw/data012/dataset.xlsx”). “Error in model.frame.default(formula = table$PSYCH.WELLBEING ~ table$REGIONS,:invalid type (NULL) for variable ’table$PSYCH.WELLBEING_. In the table the column”PSYCH WELLBEING” uses a space instead of a period. This also happens for other column names. To change this colnames(table) &lt;- str_replace_all(colnames(table), ” “,”.”) was added before testing skewness. The rest of the code stayed the same. To make the knitting of this rmarkdown easier the code has been adjusted to only look at “PSYCH.WELLBEING” as an example. All other indicators would have been analysed in the exact same manner. 1.3.4 Effort scoring Thanks to the great readability and consistency of the code it was very easy to reproduce. The only issue was adjusting the columnnames to use periods instead of spaces which was easily fixed. Therefore the score is a 5/5 on reproducibility. Read the code including my adjustments # R Code for the manuscript entitled: # &quot;Mental health impacts in Argentinean college students during COVID-19 quarantine&quot;. # López Steinmetz L.C., Leyes C.A., Dutto Florio M.A., Fong S.B., López Steinmetz R.L. &amp; Godoy J.C. ########################################################################## ################## LOAD THE DATASET &amp; PACKAGES ########################### ########################################################################## # Load the dataset table&lt;-read_excel(&quot;data-raw/data012/dataset.xlsx&quot;) # Changed by Mirthe Klaassen for this portfolio assignment summary(table) ## SUB PERIODS IN PRE AND POST REGIONS PROVINCE ## Length:2687 Length:2687 Length:2687 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## SEX AGE PSYCH WELLBEING SOC FUNC AND COPING ## Length:2687 Min. :18.00 Min. :0.000 Min. :0.000 ## Class :character 1st Qu.:20.00 1st Qu.:2.000 1st Qu.:0.000 ## Mode :character Median :22.00 Median :3.000 Median :2.000 ## Mean :22.74 Mean :3.086 Mean :2.149 ## 3rd Qu.:24.00 3rd Qu.:5.000 3rd Qu.:4.000 ## Max. :39.00 Max. :6.000 Max. :6.000 ## K10 BDI STAIR YAACQ ISO ## Min. :10.0 Min. : 0 Min. : 1.00 Min. : 0.000 Min. : 1.00 ## 1st Qu.:19.0 1st Qu.:10 1st Qu.:21.00 1st Qu.: 0.000 1st Qu.:22.00 ## Median :25.0 Median :16 Median :29.00 Median : 3.000 Median :32.00 ## Mean :25.5 Mean :18 Mean :29.24 Mean : 3.699 Mean :34.56 ## 3rd Qu.:32.0 3rd Qu.:25 3rd Qu.:38.00 3rd Qu.: 6.000 3rd Qu.:46.00 ## Max. :49.0 Max. :57 Max. :58.00 Max. :21.000 Max. :82.00 # Load the packages: library(moments) library(gplots) library(readxl) ## Added by Mirthe Klaassen for this portfolio assignment ########################################################################## ###################### METHODS ########################################### ########################################################################## ###### SUB-TITLE: METHOD &gt; Sample and procedure # SAMPLE N = 2687 # Distribution by sex: table(table$SEX) ## ## man other woman ## 473 22 2192 # Absolute frequencies: Women = 2192, Men = 473, Other = 22 prop.table(table(table$SEX))*100 ## ## man other woman ## 17.603275 0.818757 81.577968 # Percentages: Women = 81.577968%, Men = 17.603275%, Other = 0.818757% # Central tendency measures by age (total sample) # mean mean(table$AGE) ## [1] 22.74023 # Mean age = 22.74023 # standard deviation sd(table$AGE) ## [1] 3.635612 # sd age = 3.635612 # median median(table$AGE) ## [1] 22 # median age = 22 # Distribution by provinces prop.table(table(table$PROVINCE))*100 ## ## CABA CBA JUJ PCIAB SAL STACR TDELF ## 11.9464086 39.0026051 6.6989207 31.9315221 7.1082992 0.9676219 2.3446223 # JUJ (JUJUY) = 6.6989207% # SAL (SALTA) = 7.1082992% # CBA (CÓRDOBA) = 39.0026051% # STACR (SANTA CRUZ) = 0.9676219% # TDELF (TIERRA DEL FUEGO) = 2.3446223% # CABA (CIUDAD AUTÓNOMA DE BUENOS AIRES) = 11.9464086% # PCIAB (PROVINCIA DE BUENOS AIRES) = 31.9315221% ###### SUB-TITLE: METHOD &gt; Data analysis # Adjustment of colnames to be the same as the names in the code == added by Mirthe Klaassen for this assignment colnames(table) &lt;- str_replace_all(colnames(table), &quot; &quot;, &quot;.&quot;) ### To test Skewness and Kurtosis # Criteria: range of acceptable values or near to (-3 and +3; Brown, 2006). # Reference: Brown T.A. (2006). Confirmatory factor analysis for applied research. New York: Guilford Press. # PSYCH.WELLBEING skewness(table$PSYCH.WELLBEING) ## [1] -0.05214941 # skewness PSYCH.WELLBEING = -0.05214941 kurtosis(table$PSYCH.WELLBEING) ## [1] 1.951112 # kurtosis PSYCH.WELLBEING = 1.951112 ### For analyses corresponding to the first aim, we divided the entire sample into four groups: table(table$REGIONS) ## ## CENTER MOST POPULATED NORTH SOUTH ## 1048 1179 371 89 # NORTH = 371 # CENTER = 1048 # SOUTH = 89 # MOST POPULATED = 1179 ### For analyses corresponding to the second aim, we divided the entire sample into four groups: table(table$SUB.PERIODS.IN.PRE.AND.POST) ## ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1508 525 364 ## 4. REMAINING WEEKS POST ## 290 # first week pre-quarantine extension (ONE WEEK PRE) = 1508 # second week pre-quarantine extension (TWO WEEK PRE) = 525 # first week post-quarantine extension (ONE WEEK POST) = 364 # remaining weeks post-quarantine extension (REMAINING WEEKS POST) = 290 ########################################################################## ###################### RESULTS ########################################### ########################################################################## ########################################################################## ####################### AIM 1 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four regions table$PSYCH.WELLBEING ## [1] 6 2 3 4 0 4 0 4 2 2 2 3 0 4 4 5 3 6 3 5 2 6 5 5 5 2 3 6 6 3 4 4 2 1 4 0 2 ## [38] 5 0 1 4 5 4 1 6 3 5 5 0 5 5 6 3 3 5 4 3 4 0 6 4 1 5 2 2 1 5 2 3 4 4 4 1 2 ## [75] 4 1 5 4 5 6 1 3 2 4 2 2 2 6 5 2 2 2 4 5 4 4 4 5 6 3 3 2 2 2 4 5 1 1 5 0 0 ## [112] 5 2 2 5 3 3 1 0 4 4 2 6 1 6 4 1 2 3 4 4 4 6 1 2 0 1 3 6 2 6 6 5 2 6 4 4 2 ## [149] 3 3 2 4 3 5 1 2 0 4 3 2 1 3 6 1 5 0 2 4 2 1 1 3 1 1 5 4 4 1 1 0 6 3 1 5 5 ## [186] 0 1 3 3 0 3 6 1 0 1 3 1 3 4 2 4 2 3 0 3 4 5 1 4 2 3 5 2 2 6 5 1 1 1 1 5 1 ## [223] 0 0 1 0 0 3 6 6 1 4 3 3 2 0 3 2 1 6 6 2 5 0 3 5 5 2 2 4 4 1 6 0 1 6 2 1 5 ## [260] 1 0 0 4 5 2 4 5 6 6 5 3 5 1 1 5 5 5 4 1 1 1 2 3 6 5 1 0 6 3 1 1 1 3 2 4 6 ## [297] 3 3 1 2 4 5 3 2 4 4 4 6 3 2 5 3 3 1 5 6 6 5 5 2 2 2 2 0 4 4 4 2 2 5 4 0 4 ## [334] 4 0 1 0 5 1 3 0 0 4 6 2 4 0 1 4 3 4 2 1 3 5 4 2 6 0 0 4 1 5 6 3 0 2 5 2 6 ## [371] 5 2 3 6 3 4 3 5 2 5 2 1 3 4 5 5 1 4 0 0 0 0 3 2 3 6 2 3 5 0 3 6 1 2 6 2 5 ## [408] 4 2 4 1 4 6 2 3 4 2 1 4 2 2 2 6 3 3 0 4 1 0 0 0 4 5 2 2 1 5 0 4 3 4 1 4 3 ## [445] 5 5 3 1 0 2 2 1 3 2 1 5 1 3 0 6 3 4 4 3 6 2 2 3 6 0 4 0 2 1 1 5 4 5 3 0 1 ## [482] 3 6 4 1 4 4 2 6 6 5 3 4 2 0 6 0 0 5 0 0 5 0 3 1 3 2 0 3 2 1 4 1 2 4 3 2 6 ## [519] 4 4 0 2 4 3 4 2 4 1 3 3 2 4 4 3 0 5 2 0 1 4 0 3 0 2 2 4 3 4 5 4 6 2 2 4 5 ## [556] 2 2 2 6 2 2 4 3 6 6 4 6 5 1 1 4 3 0 1 4 4 1 2 4 0 4 1 2 6 0 2 1 3 1 4 0 1 ## [593] 3 3 3 4 6 4 0 3 5 3 2 3 1 3 0 3 0 2 0 3 4 2 2 2 4 2 6 6 3 5 6 1 3 1 2 6 4 ## [630] 0 3 0 2 3 5 2 4 0 5 6 4 3 4 5 4 2 2 1 0 1 6 1 4 2 4 2 5 4 3 6 3 2 3 1 5 2 ## [667] 3 1 2 0 6 0 4 0 3 4 4 0 3 2 0 2 4 3 2 3 1 2 0 1 3 0 0 5 0 2 4 2 6 3 3 0 5 ## [704] 0 5 3 2 3 4 1 2 1 5 4 5 4 5 2 4 5 5 6 4 5 3 0 5 4 4 4 6 6 6 3 6 3 4 3 3 6 ## [741] 5 6 5 4 5 5 2 5 5 4 2 5 6 4 3 2 1 3 5 0 5 5 6 2 1 4 3 0 3 1 6 5 4 2 6 1 3 ## [778] 3 1 4 1 4 3 4 6 2 2 5 5 4 3 0 4 3 2 3 2 0 5 2 0 4 1 0 2 1 6 2 1 5 0 3 5 4 ## [815] 0 0 4 4 1 1 1 5 5 3 2 0 6 2 4 3 1 4 2 2 6 5 0 1 1 5 3 2 2 3 4 1 4 4 4 3 2 ## [852] 4 2 2 5 6 6 6 3 0 5 4 4 1 5 0 5 6 1 2 3 2 3 6 2 5 1 2 3 4 2 5 4 2 4 3 1 0 ## [889] 0 1 5 3 3 6 3 0 2 3 0 5 5 2 4 6 4 3 6 0 2 0 6 5 2 6 1 4 4 3 1 5 2 0 4 0 4 ## [926] 4 1 4 2 4 5 0 3 3 4 3 6 4 4 3 1 3 6 5 1 0 3 5 1 2 2 3 1 1 1 1 3 4 5 0 1 0 ## [963] 1 5 4 4 1 3 4 2 5 2 3 3 2 1 6 6 4 4 2 5 3 3 4 6 2 3 3 0 2 6 4 5 6 3 6 6 1 ## [1000] 5 2 4 1 1 5 1 3 2 5 3 1 0 5 1 5 3 5 4 2 1 2 2 6 0 3 0 4 6 4 3 0 3 3 3 1 1 ## [1037] 4 3 3 4 0 4 4 2 5 5 0 4 4 0 2 0 6 1 3 3 6 0 0 3 2 3 6 3 3 6 1 4 3 2 5 4 4 ## [1074] 1 5 2 3 2 2 5 5 5 4 5 6 3 3 4 3 6 2 6 5 5 1 0 3 3 1 6 2 4 5 5 3 4 4 0 3 0 ## [1111] 0 3 3 4 3 1 1 2 1 4 1 4 0 2 1 3 2 2 6 1 3 2 1 3 2 1 2 1 2 0 3 4 1 6 1 3 2 ## [1148] 5 2 4 1 3 1 0 0 2 1 3 4 0 0 4 1 5 4 3 2 4 5 4 5 5 3 3 6 1 0 4 2 3 4 2 2 0 ## [1185] 5 4 6 1 2 6 1 0 5 3 5 3 4 3 2 1 5 5 5 2 5 3 1 5 6 6 4 3 3 1 3 3 6 6 3 4 4 ## [1222] 2 3 0 4 2 0 4 4 1 2 2 2 3 1 0 1 6 5 1 5 3 6 6 1 4 1 6 4 1 1 4 2 2 3 5 0 1 ## [1259] 5 4 6 2 0 0 3 1 3 4 6 0 4 2 3 0 3 2 6 6 0 2 0 5 0 2 4 4 5 2 4 1 3 4 5 4 3 ## [1296] 6 3 4 0 3 4 4 2 0 4 2 5 3 5 1 0 6 0 0 2 6 2 0 1 1 3 1 0 4 5 3 1 2 4 3 2 1 ## [1333] 0 4 5 1 2 6 3 3 3 4 4 3 0 4 6 2 2 3 6 6 1 4 1 5 3 6 4 2 3 6 3 6 4 2 1 6 1 ## [1370] 3 2 1 1 4 3 1 4 4 5 3 6 0 5 1 4 5 0 0 1 3 2 0 1 2 2 3 5 3 4 3 6 2 4 0 0 0 ## [1407] 3 3 1 2 3 3 2 2 5 3 2 1 0 4 5 3 6 2 3 2 0 0 0 0 6 5 2 2 5 4 2 4 5 0 5 5 1 ## [1444] 2 1 2 6 2 4 0 6 0 3 4 3 6 0 2 3 2 4 1 4 0 2 5 1 4 4 2 2 2 4 3 0 6 3 0 3 5 ## [1481] 1 5 3 2 2 6 0 2 4 6 2 3 6 4 3 3 2 3 1 3 4 6 6 5 3 1 6 5 3 6 3 6 2 6 2 0 0 ## [1518] 4 2 1 2 3 1 0 5 2 2 3 3 6 4 1 5 3 2 3 2 5 1 6 3 1 0 3 6 2 2 4 6 6 4 0 1 5 ## [1555] 2 3 5 3 5 6 5 5 3 1 4 5 3 6 5 6 5 1 0 1 4 5 5 4 6 4 2 2 2 5 1 6 5 0 4 2 4 ## [1592] 5 4 3 3 4 0 3 2 0 3 1 4 0 6 4 2 5 2 1 3 2 2 3 1 4 1 1 5 6 6 5 5 3 6 1 0 3 ## [1629] 3 4 4 2 0 6 3 0 4 6 2 6 3 5 2 3 6 6 0 2 3 2 1 6 2 1 4 2 2 4 2 5 5 5 2 4 4 ## [1666] 5 1 0 2 3 5 4 2 6 2 4 1 3 2 2 4 1 3 5 3 3 4 5 6 3 6 4 1 1 4 6 3 2 5 3 3 0 ## [1703] 3 3 3 6 1 4 4 5 6 0 3 5 5 5 4 6 3 1 5 5 2 2 6 6 3 2 2 5 2 6 2 3 5 3 1 6 2 ## [1740] 6 5 5 6 1 6 1 2 3 1 3 4 5 4 1 4 3 2 5 5 3 3 1 1 2 3 5 6 2 4 4 4 1 1 6 2 4 ## [1777] 3 3 4 4 2 3 3 1 2 5 4 5 5 2 4 2 5 1 6 1 5 1 6 2 3 0 2 3 3 0 5 6 1 1 3 3 3 ## [1814] 3 4 4 2 5 3 4 4 2 5 2 0 2 6 0 5 5 6 1 4 5 0 5 5 5 5 1 3 1 2 4 5 4 2 6 3 4 ## [1851] 1 3 3 0 0 5 4 6 2 6 4 5 3 4 5 1 6 5 3 1 3 3 4 5 1 2 4 3 3 6 2 2 0 0 2 2 6 ## [1888] 3 6 3 6 4 0 4 5 3 3 1 0 5 4 2 4 4 2 4 4 0 4 5 4 0 2 1 2 1 1 2 0 3 0 6 2 0 ## [1925] 3 2 5 5 3 5 1 1 6 3 5 4 5 6 3 5 5 5 1 0 3 3 4 6 3 5 4 3 4 0 6 0 2 1 3 3 3 ## [1962] 3 4 2 0 4 4 6 5 1 3 0 2 3 5 3 2 3 3 1 4 5 3 3 2 3 6 4 2 1 6 0 4 2 0 4 5 2 ## [1999] 4 2 2 3 2 4 0 4 1 1 3 4 3 4 3 3 0 4 0 5 5 4 1 1 3 0 4 2 1 5 0 1 1 4 4 5 5 ## [2036] 4 3 1 5 3 1 6 4 5 4 0 5 3 2 6 2 6 6 5 1 1 0 3 2 2 4 2 0 3 3 3 5 3 1 5 6 3 ## [2073] 1 2 3 5 4 0 0 6 5 3 2 6 6 3 2 4 6 1 1 5 2 6 6 5 4 4 5 0 0 4 5 1 3 2 4 3 3 ## [2110] 4 6 6 3 0 2 4 6 0 1 0 5 1 5 0 3 3 6 5 4 6 5 5 3 5 2 1 6 0 3 0 4 6 4 2 2 2 ## [2147] 2 1 2 5 3 3 6 5 3 4 5 1 5 5 0 0 5 2 6 6 1 3 2 6 5 5 1 3 6 3 4 1 1 2 6 5 3 ## [2184] 2 5 4 5 4 4 1 5 5 6 6 6 5 2 2 3 3 6 2 1 1 5 3 5 0 3 6 4 3 0 5 5 1 5 6 1 6 ## [2221] 2 2 2 2 2 5 3 3 1 2 1 0 1 3 2 3 4 5 4 0 3 5 4 5 6 3 6 5 2 3 6 2 5 6 2 5 2 ## [2258] 4 5 6 5 3 2 0 0 6 2 1 1 5 6 4 4 6 3 1 5 6 2 2 5 4 3 2 6 0 2 4 1 0 4 6 3 5 ## [2295] 4 2 1 3 3 0 3 5 6 6 5 2 4 6 4 5 6 6 5 2 4 4 4 4 4 3 1 5 6 3 4 3 6 1 6 4 5 ## [2332] 1 3 5 3 2 3 2 3 3 3 5 0 4 4 4 6 6 5 3 0 4 2 1 5 4 5 3 5 4 5 6 0 4 2 5 2 5 ## [2369] 3 5 0 3 3 2 3 2 2 2 3 0 3 5 6 5 5 6 2 4 4 6 2 6 6 1 3 1 3 4 3 2 3 2 5 4 4 ## [2406] 6 4 5 2 3 6 1 2 2 5 1 1 6 3 4 2 5 5 4 4 2 3 0 5 2 5 1 1 1 2 6 1 1 4 5 3 3 ## [2443] 1 0 4 6 3 5 1 2 6 2 1 1 4 4 5 3 0 5 5 1 4 2 5 2 0 1 4 2 4 5 2 4 1 5 5 2 1 ## [2480] 6 1 6 1 3 1 2 4 5 0 3 4 5 3 5 1 4 0 0 6 4 5 6 0 4 4 2 5 4 2 4 6 3 2 2 2 0 ## [2517] 4 2 6 3 3 6 6 3 4 2 3 5 3 4 5 1 0 5 0 2 5 3 4 5 0 5 5 1 4 6 6 6 5 4 5 2 1 ## [2554] 5 1 0 4 5 4 4 6 3 3 6 2 4 5 2 6 2 2 6 2 4 3 6 1 3 3 4 6 2 5 5 6 0 2 2 5 3 ## [2591] 3 6 1 2 6 3 2 3 0 2 3 1 4 3 3 2 4 4 2 1 5 5 3 3 3 2 5 2 0 4 1 3 3 3 6 0 1 ## [2628] 1 4 1 2 1 2 6 6 2 2 3 5 2 2 1 4 2 5 4 4 6 4 4 0 4 5 4 5 5 5 6 5 5 5 3 5 3 ## [2665] 6 5 2 3 3 4 5 4 5 5 5 4 5 2 5 3 3 5 5 4 6 4 0 # PSYCHOLOGICAL WELL-BEING/DISCOMFORT (OF GENERAL HEALTH) anovaregpsychwellbeing &lt;- aov(table$PSYCH.WELLBEING~table$REGIONS) summary(anovaregpsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$REGIONS 3 46 15.340 4.569 0.00338 ** ## Residuals 2683 9008 3.358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovaregpsychwellbeing) TukeyHSD(anovaregpsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$REGIONS) ## ## $`table$REGIONS` ## diff lwr upr p adj ## MOST POPULATED-CENTER 0.28148855 0.08151381 0.4814633 0.0017158 ## NORTH-CENTER 0.22902049 -0.05554103 0.5135820 0.1636105 ## SOUTH-CENTER 0.17650527 -0.34355917 0.6965697 0.8191275 ## NORTH-MOST POPULATED -0.05246806 -0.33286583 0.2279297 0.9633105 ## SOUTH-MOST POPULATED -0.10498327 -0.62278119 0.4128146 0.9540137 ## SOUTH-NORTH -0.05251522 -0.60848288 0.5034524 0.9949688 plot(TukeyHSD(anovaregpsychwellbeing)) # significant differences # MOST POPULATED-CENTER p adj 0.0017158 #### MOST POPULATED mean = 3.206107, CENTER mean = 2.924618 tapply(table$PSYCH.WELLBEING,factor(table$REGIONS),mean) ## CENTER MOST POPULATED NORTH SOUTH ## 2.924618 3.206107 3.153639 3.101124 tapply(table$PSYCH.WELLBEING,factor(table$REGIONS),sd) ## CENTER MOST POPULATED NORTH SOUTH ## 1.836446 1.834613 1.838215 1.725774 ########################################################################## ####################### AIM 2 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four sub-periods of quarantine # PSYCHOLOGICAL WELL-BEING (OF GENERAL HEALTH) anovatemppsychwellbeing &lt;- aov(table$PSYCH.WELLBEING~table$SUB.PERIODS.IN.PRE.AND.POST) summary(anovatemppsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$SUB.PERIODS.IN.PRE.AND.POST 3 83 27.791 8.312 1.68e-05 *** ## Residuals 2683 8971 3.344 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovatemppsychwellbeing) TukeyHSD(anovatemppsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$SUB.PERIODS.IN.PRE.AND.POST) ## ## $`table$SUB.PERIODS.IN.PRE.AND.POST` ## diff lwr upr ## 2. TWO WEEK PRE-1. ONE WEEK PRE -0.1360136 -0.37421192 0.1021846 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2031072 -0.07139874 0.4776132 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.4771883 0.17578682 0.7785898 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.3391209 0.01851552 0.6597262 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.6132020 0.26928754 0.9571164 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2740811 -0.09590843 0.6440706 ## p adj ## 2. TWO WEEK PRE-1. ONE WEEK PRE 0.4571940 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2273681 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.0002823 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.0333276 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.0000283 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2264068 plot(TukeyHSD(anovatemppsychwellbeing)) # significant differences # 4. REMAINING WEEKS POST-1. ONE WEEK PRE p adj 0.0002823 # 3. ONE WEEK POST-2. TWO WEEK PRE p adj 0.0333276 # 4. REMAINING WEEKS POST-2. TWO WEEK PRE p adj 0.0000283 tapply(table$PSYCH.WELLBEING,factor(table$SUB.PERIODS.IN.PRE.AND.POST),mean) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 3.033156 2.897143 3.236264 ## 4. REMAINING WEEKS POST ## 3.510345 tapply(table$PSYCH.WELLBEING,factor(table$SUB.PERIODS.IN.PRE.AND.POST),sd) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1.823743 1.819344 1.886359 ## 4. REMAINING WEEKS POST ## 1.796256 # Figure S1: plotmeans(table$PSYCH.WELLBEING~table$SUB.PERIODS.IN.PRE.AND.POST, main=&quot;Fig. S1: Psychological well-being/discomfort by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Psychological well-being/discomfort&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) ########################################################################## ############################# THE END #################################### ########################################################################## "],["Workflows_Portfolio_2.html", "Chapter 2 Datamanagement", " Chapter 2 Datamanagement As part of assignment 2 from the DSFB2 Workflows course. This assignment was all about different principles and focuspoints within data management. This included topics like file naming, Guerilla analytics, project structures, checksums, etc. The Guerilla analytics framework is a tool to establish an efficient, trustworthy and reproducible datamanagement workflow. It is described by Enda Ridge in this booklet. The seven core principles are: Space is cheap, confusion is expensive Use simple, visual project structures and conventions Automate (everything - my addition) with program code Link stored data to data in the analytics environment to data in work products (literate programming with RMarkdown - my addition) Version control changes to data and analytics code (Git/Github.com) Consolidate team knowledge (agree on guidelines and stick to it as a team) Use code that runs from start to finish (literate programming with RMarkdown - my addition) The execution of this assignment was focused on principle 2, “Use simple, visual project structures and conventions”. This is important to avoid deep nesting of files and to prepare for the way a project can evolve over time. If certain guidelines are not followed it will create problems, especially when handing a project over to someone else. The recommended guidelines are as follows: Create a separate folder for each analytics project. Keep the unit of a project small. Do not deeply nest folders (max 2-3 levels). Keep information about the data, close to the data. Store each dataset in its own sub-folder and create sub-folders within it for older versions. Do not change file names or move them! If you have to, record the change in the README. Do not manually edit data source files, always use code. In code, use relative paths (here::here etc) Use one R project per project and don’t reuse projects for other stuff. This principle was put into practice on a previous project, from subject DAUR2. At the time the guidelines had not been discussed so the initial structure was my own personal preference, without thinking about reproducibility. For this assignment the arrangement of the files and folders within that project have been adjusted. The directory tree is shown in figure 2.1. Since DAUR2 worked with datasets given by the school, those datasets had their own location on the shared server and were not downloaded. They would have been added to the ‘data-raw’ folder with a numbered subfolder per dataset. Figure 2.1 Directory tree for subject DAUR2. "],["Workflows_Portfolio_3.html", "Chapter 3 Future planning", " Chapter 3 Future planning As part of assignment 3 from the DSFB2 Workflows course. The future of our careers was often discussed inclass. At points we are so busy studying, we lose track of why we are studying and what we are working towards. In this assignment few questions given by the teachers will be answered. With these questions I tried to look ahead and take concrete steps towards my future. Where do I want to be in ~2 years time? In two years I will have finished my studies. Which means I completed the specialisation I’m doing currently, a minor at the professorship and a graduationproject/internship outside of school. I hope to have also started my first job in the field of Data Science or Bioinformatics, with a focus on one of my interests. This is where it gets tricky because I have a lot of interests (maybe too many for my own good). The first ones that come to mind are systembiology, structural biology, biocomplexity and statistics. I would also like my work to reduce the need for animaltesting. How am I doing now with respect to this goal? Aside from specializing in Data Science I am trying to filter my interests. I can’t do all the things at once so I want to start narrowing my view when it comes to my career. This is very difficult for me. Luckily, the teachers are a great help and I’m getting a clearer picture of the best route to take. In my minor I’ll be focusing on applying a probabilistic approach to the risk-evaluation of toxins within an existing pipeline. Right now I think my graduationproject will focus on something along the lines of structural biochemistry. What would be the next skill to learn? Something about how software is used to describe protein structure and folding. Bas van Gestel suggested I look at AlphaFold. They have the open source code available on github and it would be useful to learn more about it. Make a planning on how to start learning this new skill. I have blocked out a couple of days before winterbreak which I will use to work on this assignment. I will dive into the code and see if I can replicate some of it. The result of this can be seen under chapter 9 ‘Free Assignment’. "],["Workflows_Portfolio_5.html", "Chapter 4 Groupproject 4.1 Technology to reduce animaltesting 4.2 ONTOX 4.3 SYSREV 4.4 SBtab and Phymdos 4.5 The Goal", " Chapter 4 Groupproject As part of assignment 5 from the DSFB2 Workflows course. Alongside the subject of Workflows there will be a groupproject with two other students. This project will be a great way to learn about agile methods of collaboration, applying data science skills, creating a working product and presenting this product. In this case the project revolves around ONTOX with Marc Teunis as our cliënt. 4.1 Technology to reduce animaltesting A goal within my career is reducing the need for animaltesting in biomedical research. Within the field of animaltesting there’s a focus on replacement, reduction and refinement, as described by Simmonds (2018). In the past replacement focussed on insentient material and animal species “lower” on the pylogenic scale. In recent years the use of computer software as a replacement is becoming more and more relevant (Grafström et al. 2015). Reduction focuses on properly designing the research so the least amount of animals are used while also preventing the need for more animals at a later stage. This becomes complicated when using less animals for the initial research may create the need for more animals in repetitive or supporting studies. In this case using software would not only reduce the amount of animals needed, it would also make reproduction a lot easier. The third R, refinement, is all about reducing the amount of discomfort the animals go through. This can include training the animals to get used to the treatment with positive reinforcement or the use of anesthesia (Simmonds 2018). This requires more time and resources which can be prevented by using computer software. One project with a focus on reducing the need for animals in human risk assessment of chemicals is the ONTOX project. 4.2 ONTOX In their own words “The vision of the ONTOX consortium is to provide a functional and sustainable solution for advancing human risk assessment of chemicals without the use of animals in line with the principles of 21st century toxicity testing and next generation risk assessment” (“ONTOX Project,” n.d.). One of the ways to achieve this is by collecting and combining existing data to be used in the risk assessment. Platforms like SysRev are able to extract useful data from articles. This data consists of metabolic pathways and interactions between compounds. To make the data useful for computerbased analysis it is formatted into what is called an SBtab file. This file, in turn, can be used to create visualisations of the metabolic pathways or it can be turned into other formats like SBML. Different apps (i.e. Phymdos) and functions have been created to execute these possibilities. 4.3 SYSREV Our current knowledge within the biomedical industry consists of big and complex datasets. Combining the datasets from multiple sources can help with identifying clusters and correlation between datasets, as well as developing predictive models (Ristevski and Chen 2018). The amount of existing literature and data is hard to grasp. One way to make this easier is by using software platforms like SysRev. SysRev is a collaborative platform for the extraction of data from documents (“Built for Data Miners | Sysrev,” n.d.). It extracts only the useful data from documents and presents it in a short and clear format. The researcher will be able to go over more relevant data in a shorter amount of time. Existing data about risk assessment on humans may be more accurate than performing the same risk assessment on a new samplegroup of animals. As shown by Perel et al. (2007) the treatment effects in animal experiments can differ greatly from the treatment effects in clinical trials. Simon Festing, executive director of RDS, once stated “Animals are better used for understanding disease mechanisms and potential new treatments, rather than predicting what will happen in humans,” (“The Trouble with Animal Models,” n.d.). In order to predict risk in humans it is important to look at our specific biological systems instead of other organisms. Aside from performing clinical trials the existing data can and should be used more optimally. 4.4 SBtab and Phymdos One of the possible short and clear output formats used by SysRev is called SBtab. This format is a set of syntax rules, created to simplify data processing, and can be interconverted with the well known SBML format (“SBtab - Standardised Data Tables for Systems Biology,” n.d.). One app which can perform this conversion is Phymdos. The app was created by D. Roodzant for University of Applied Sciences Utrecht. It can take SBtab files, SBML files or extract them directly from SysRev. It allows the user to create SBML style physiological maps in the desired format. This app demonstrates the range of possiblities with this type of data. 4.5 The Goal The current process of extracting and analyzing SBtab files is only applicable to single files. Looking ahead, ONTOX strives to improve human risk assessment by combining multiple sources for the same goal. In order to use this existing process for the bigger picture, it should be combining data instead of looking at one particular set. This is why Marc Teunis asked us to make an R package with the following functions: Extract the SBtab data from SysRev for one or more articles. Merge multiple SBtab files into one new file. Create a graph, visualizing the data based on the merged SBtab file. Convert the SBtab file to another format like SBML. The merging of multiple files will make finding patterns or relations between existing data easier and faster. It will speed up the research process by predicting which patterns or relations are worth focusing on. "],["Workflows_Portfolio_7.html", "Chapter 5 Relational data 5.1 Connecting to DBeaver 5.2 Data inspection 5.3 Joining datasets 5.4 Descriptive statistics 5.5 Visualisations", " Chapter 5 Relational data As part of assignment 7 from the DSFB2 Workflows course. So far the subjects have been about R, BASH, HTML and CSS. Another important language to understand is SQL. With this assignment the focus is on learning the basics of SQL and applying it to relational data. To start, the dataframes will be created, using the files provided in the assignment, and tidied up. # Importing the flu dataframe ---- flu_df &lt;- read_csv(&quot;data-raw/data070/flu_data.csv&quot;, skip=11) ## First 11 rows are metadata # Making the flu dataframe tidy ---- flu_df_tidy &lt;- flu_df %&gt;% pivot_longer(cols = c(2:ncol(flu_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # Importing the dengue dataframe ---- dengue_df &lt;- read_csv(&quot;data-raw/data070/dengue_data.csv&quot;, skip=11) ## First 11 rows are metadata # Making the dengue dataframe tidy ---- dengue_df_tidy &lt;- dengue_df %&gt;% pivot_longer(cols = c(2:ncol(dengue_df)), names_to = &#39;country&#39;, values_to = &#39;cases&#39;) %&gt;% na.omit() # The third dataframe is the gapminder dataframe from the dslabs package # This dataframe is already tidy To make the data more relational a couple things have to be changed. The columns describing the countries and dates should be comparable across the different dataframes so they can be joined later. This was achieved with the following code. # All &#39;country&#39; columns should be the same in type, class and values # For flu and dengue they&#39;re characters, for gapminder it&#39;s a factor # Turning the &#39;country&#39; column of both dengue and flu into a factor flu_df_tidy$country &lt;- as.factor(flu_df_tidy$country) dengue_df_tidy$country &lt;- as.factor(dengue_df_tidy$country) # All &#39;date&#39; columns should be the same in type, class and values # For flu and dengue the dates are specified to the day instead of the year # We&#39;ll create a new &#39;year&#39; column for both which matches the column of gapminder flu_df_tidy &lt;- flu_df_tidy %&gt;% mutate(year = substr(flu_df_tidy$Date, start=1, stop=4)) dengue_df_tidy &lt;- dengue_df_tidy %&gt;% mutate(year = substr(dengue_df_tidy$Date, start=1, stop=4)) # Now they&#39;re characters, they should be integers flu_df_tidy$year &lt;- as.integer(flu_df_tidy$year) dengue_df_tidy$year &lt;- as.integer(dengue_df_tidy$year) The dataframes were also exported to .csv and .rds for later use. # Export the dataframes as csv and rds save_csv_rds(flu_df_tidy, &quot;data/data070/flu&quot;) save_csv_rds(dengue_df_tidy, &quot;data/data070/dengue&quot;) save_csv_rds(gapminder, &quot;data/data070/gapminder&quot;) 5.1 Connecting to DBeaver To move from R to SQL (PostgreSQL) a database has been created in DBeaver called ‘workflowsdb’. The three R dataframes were imported into the database using RPostgreSQL. The tables were inserted into the hierarchy of this database as shown in figure 4.1. con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;workflowsdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;insertpassword&quot;) # Password changed after connecting for privacy reasons # Using the DBI library dbWriteTable(con, &quot;gapminder&quot;, gapminder) dbWriteTable(con, &quot;flu&quot;, flu_df_tidy) dbWriteTable(con, &quot;dengue&quot;, dengue_df_tidy) Figure 4.1. The hierarchy of the workflowsdb database including the three tables. 5.2 Data inspection The data was inspected using the SQL editor. A script was created for the inspection of the gapminder, flu and dengue data. R can also be used for the same inspections as shown below. SQL script Gapminder -- Checking the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;gapminder&#39;; -- Checking the timeframe in which the data has been collected select distinct year from gapminder order by year desc; -- from 1960 to 2016 -- Checking the countries with the highest and lowest average population select country, AVG (population):: NUMERIC(11,0) from gapminder group by country order by avg desc; -- China has the highest with around 1,000,000,000 -- Greenland the lowest with around 51,000 -- Checking the countries with the highest and lowest average life expectancy select country, AVG (life_expectancy):: NUMERIC(6,2) from gapminder where life_expectancy is not null group by country order by avg desc; -- Iceland has the highest with 78.17 -- Central Africa Republic the lowest with 46.31 -- Checking how many countries are noted in the data select count(distinct country) from gapminder; -- 185 different countries SQL script flu data inspection -- First we check the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;flu&#39;; -- We check in which timeframe the data has been collected select distinct year from flu order by year desc; -- from 2002 to 2015 AVG (cases):: NUMERIC(6,1) from flu group by country order by avg desc; -- South Africa had the most with 2698.8 on average -- Sweden the least 5.5 -- We check how many countries are noted in the data and if they are spelled correctly select distinct country from flu; -- 29 different countries, all spelled correctly SQL script dengue data inspection -- First we check the columnnames and types select column_name, data_type from information_schema.columns where table_schema = &#39;public&#39; and table_name = &#39;dengue&#39;; -- We check in which timeframe the data has been collected select distinct year from dengue order by year desc; -- from 2002 to 2015 -- We check which countries had the most and least dengue cases on average over this timeframe select country, AVG (cases):: NUMERIC(6,4) from dengue group by country order by avg desc; -- Venezuela had the most with 0.2774 on average -- Bolivia the least with 0.0447 -- We check how many countries are noted in the data and if they are spelled correctly select distinct country from dengue; -- 10 different countries, all spelled correctly R script data inspection # Checking the columnnames and columntypes str(gapminder_df) ## &#39;data.frame&#39;: 10545 obs. of 9 variables: ## $ country : Factor w/ 185 levels &quot;Albania&quot;,&quot;Algeria&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ year : int 1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ... ## $ infant_mortality: num 115.4 148.2 208 NA 59.9 ... ## $ life_expectancy : num 62.9 47.5 36 63 65.4 ... ## $ fertility : num 6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ... ## $ population : num 1636054 11124892 5270844 54681 20619075 ... ## $ gdp : num NA 1.38e+10 NA NA 1.08e+11 ... ## $ continent : Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 4 1 1 2 2 3 2 5 4 3 ... ## $ region : Factor w/ 22 levels &quot;Australia and New Zealand&quot;,..: 19 11 10 2 15 21 2 1 22 21 ... str(flu_df_tidy) ## tibble [17,266 × 4] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:17266], format: &quot;2002-12-29&quot; &quot;2002-12-29&quot; ... ## $ country: Factor w/ 29 levels &quot;Argentina&quot;,&quot;Australia&quot;,..: 6 19 6 19 6 9 19 6 9 19 ... ## $ cases : num [1:17266] 174 329 162 315 174 1 314 162 0 267 ... ## $ year : int [1:17266] 2002 2002 2003 2003 2003 2003 2003 2003 2003 2003 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:1845] 1 2 3 4 5 7 8 9 10 11 ... ## ..- attr(*, &quot;names&quot;)= chr [1:1845] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... str(dengue_df_tidy) ## tibble [6,263 × 4] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:6263], format: &quot;2002-12-29&quot; &quot;2002-12-29&quot; ... ## $ country: Factor w/ 10 levels &quot;Argentina&quot;,&quot;Bolivia&quot;,..: 2 3 4 5 8 2 3 4 5 8 ... ## $ cases : num [1:6263] 0.101 0.073 0.062 0.101 0.059 0.143 0.098 0.047 0.039 0.059 ... ## $ year : int [1:6263] 2002 2002 2002 2002 2002 2003 2003 2003 2003 2003 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:327] 1 6 7 9 10 11 16 17 19 20 ... ## ..- attr(*, &quot;names&quot;)= chr [1:327] &quot;1&quot; &quot;6&quot; &quot;7&quot; &quot;9&quot; ... # Checking the timeframes paste(&quot;From&quot;, min(unique(gapminder_df$year)), &quot;to&quot;, max(unique(gapminder_df$year))) ## [1] &quot;From 1960 to 2016&quot; paste(&quot;From&quot;, min(unique(flu_df_tidy$year)), &quot;to&quot;, max(unique(flu_df_tidy$year))) ## [1] &quot;From 2002 to 2015&quot; paste(&quot;From&quot;, min(unique(dengue_df_tidy$year)), &quot;to&quot;, max(unique(dengue_df_tidy$year))) ## [1] &quot;From 2002 to 2015&quot; # Checking the highest and lowest average population in the gapminder data options(scipen=999) gapminder_pop &lt;- gapminder_df %&gt;% group_by(country) %&gt;% summarize(average = mean(population, na.rm = TRUE)) gapminder_pop %&gt;% filter(average == max(gapminder_pop$average) | average == min(gapminder_pop$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 China 1068197798. ## 2 Greenland 50984. # Checking the highest and lowest average life expectancy in the gapminder data gapminder_life &lt;- gapminder_df %&gt;% group_by(country) %&gt;% summarize(average = mean(life_expectancy, na.rm = TRUE)) gapminder_life %&gt;% filter(average == max(gapminder_life$average) | average == min(gapminder_life$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 Central African Republic 46.3 ## 2 Iceland 78.2 # Checking the most and least average flu cases in the flu data flu_cases &lt;- flu_df_tidy %&gt;% group_by(country) %&gt;% summarize(average = mean(cases, na.rm = TRUE)) flu_cases %&gt;% filter(average == max(flu_cases$average) | average == min(flu_cases$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 South Africa 2699. ## 2 Sweden 5.55 # Checking the most and least average dengue cases in the dengue data dengue_cases &lt;- dengue_df_tidy %&gt;% group_by(country) %&gt;% summarize(average = mean(cases, na.rm = TRUE)) dengue_cases %&gt;% filter(average == max(dengue_cases$average) | average == min(dengue_cases$average)) ## # A tibble: 2 × 2 ## country average ## &lt;fct&gt; &lt;dbl&gt; ## 1 Bolivia 0.0447 ## 2 Venezuela 0.277 # Checking how many countries took part length(unique(gapminder_df$country)) # 185 countries ## [1] 185 length(unique(flu_df_tidy$country)) # 29 countries ## [1] 29 length(unique(dengue_df_tidy$country)) # 10 countries ## [1] 10 5.3 Joining datasets The three datasets have two variables in common, country and year. Both the flu and dengue datasets have multiple observations within the same year. Sadly, the flu and dengue databases are very unclear. Since the data is old and the links provided in the metadata are no longer working it is hard to estimate the details of this data. Per how many inhabitants are the cases? What were the criteria to be considered a case? Other factors, like the option to get a diagnosis within that region, should also be considered. To combine the datasets the average amount of weekly cases per year per country was used. This was a better option than total average per year because not every country has data for every week of each year, therefore taking the sum would not be accurate. The dengue dataset incorporates the least amount of countries. When joining the data only the countries seen in all three datasets will be kept, these are Argentina, Bolivia, Brazil and Mexico. Therefore, joining the three datasets would waste a lot of data. Since dengue and flu will be looked at separately those two datasets will not be joined. Two joined tables have been made, one joining the flu and gapminder data and one joining the dengue and gapminder data. Click here for the full SQL script written to join the dengue and gapminder datasets based on country and year. This SQL script was used to combine the flu and gapminder datasets, this time with a focus on continents instead of countries. The joined tables were imported into R using the following code. gd_country &lt;- read_csv(&quot;data/data070/gapminder_dengue_country.csv&quot;) gf_continent &lt;- read_csv(&quot;data/data070/gapminder_flu_continent.csv&quot;) 5.4 Descriptive statistics With these new joined tables we can perform some descriptive statistics. The cases have not been normalized for the population so it will be normalized to reflect the amount of cases per 100.000 inhabitants instead. Then the average amount of cases and standard deviation will be determined for each country/continent. After using the shapiro-wilk test both databases appeared to have at least a few non-normally distributed variables, 5 out of 10 for the dengue database and 3 out of 5 for the flu database. The Levene’s test showed that neither database had an equality of variance between countries/continents. Because of this (lack of normality and no equality of variance) the Kruskal-Wallis test was used to check for significant differences. Both databases brought back a p-value of .0000. The Dunn test was used as Post Hoc test to see where the statistically significant difference came from. The script used to find descriptive statistics # Normalize the cases for population ---- gd_norm_country &lt;- gd_country %&gt;% mutate(cases_per_100000 = cases/population*100000) gf_norm_continent &lt;- gf_continent %&gt;% mutate(cases_per_100000 = cases/population*100000) # Mean and stdev per country/continent ---- gd_summ &lt;- gd_norm_country %&gt;% group_by(country)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) gf_summ &lt;- gf_norm_continent %&gt;% group_by(continent)%&gt;% summarize(avg_population = round(mean(population), 0), sd_population = round(sd(population), 0), avg_cases_per_100000 = mean(cases_per_100000), sd_cases_per_100000 = sd(cases_per_100000)) # To check the normality, dataframes are made with the countries/continents as columns dengue_country &lt;- gd_norm_country %&gt;% select(country, cases_per_100000, year) %&gt;% pivot_wider(names_from = country, values_from = cases_per_100000) %&gt;% filter(year != 2002) flu_continent &lt;- gf_norm_continent %&gt;% select(continent, cases_per_100000, year) %&gt;% pivot_wider(names_from = continent, values_from = cases_per_100000) %&gt;% filter(year != 2002) # Shapiro wilk for dengue ---- stats_dengue_country &lt;- data.frame(country = colnames(dengue_country[,2:length(colnames(dengue_country))]), pvalue_shap = round(c(shapiro.test(dengue_country$Venezuela)$p.value, shapiro.test(dengue_country$Thailand)$p.value, shapiro.test(dengue_country$Singapore)$p.value, shapiro.test(dengue_country$Philippines)$p.value, shapiro.test(dengue_country$Mexico)$p.value, shapiro.test(dengue_country$Indonesia)$p.value, shapiro.test(dengue_country$India)$p.value, shapiro.test(dengue_country$Brazil)$p.value, shapiro.test(dengue_country$Bolivia)$p.value, shapiro.test(dengue_country$Argentina)$p.value), 4)) # Shapiro wilk for flu ---- stats_flu_continent &lt;- data.frame(continent = colnames(flu_continent[,2:length(colnames(flu_continent))]), pvalue_shap = round(c(shapiro.test(flu_continent$Oceania)$p.value, shapiro.test(flu_continent$Europe)$p.value, shapiro.test(flu_continent$Asia)$p.value, shapiro.test(flu_continent$Americas)$p.value, shapiro.test(flu_continent$Africa)$p.value), 4)) # Normality check ---- stats_dengue_country &lt;- stats_dengue_country %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) # 5 out of 10 are normally distributed stats_flu_continent &lt;- stats_flu_continent %&gt;% mutate(normal_dis = pvalue_shap&gt;0.05) # 3 out of 5 are normally distributed # Levene test ---- leveneTest(cases_per_100000 ~ country, data = gd_norm_country)[1,3] # P-value of .0000 ## [1] 0.000000328782 leveneTest(cases_per_100000 ~ continent, data = gf_norm_continent)[1,3] # P-value of .0015 ## [1] 0.001455027 # The variances are not equal # Kruskal Wallis test ---- kruskal.test(cases_per_100000 ~ country, data = gd_norm_country) ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by country ## Kruskal-Wallis chi-squared = 110.54, df = 9, p-value &lt; ## 0.00000000000000022 # P-value of .0000, there is statistically significant difference kruskal.test(cases_per_100000 ~ continent, data = gf_norm_continent) ## ## Kruskal-Wallis rank sum test ## ## data: cases_per_100000 by continent ## Kruskal-Wallis chi-squared = 51.754, df = 4, p-value = 0.0000000001553 # P-value of .0000, there is statistically significant difference # Post-Hoc Dunn test ---- dunn_dengue_country &lt;- dunnTest(cases_per_100000 ~ country, data = gd_norm_country, method = &quot;holm&quot;)$res dunn_flu_continent &lt;- dunnTest(cases_per_100000 ~ continent, data = gf_norm_continent, method = &quot;holm&quot;)$res # Check for significant differences ---- dunn_dengue_country &lt;- dunn_dengue_country %&gt;% mutate(different = P.adj&lt;0.05) dunn_flu_continent &lt;- dunn_flu_continent %&gt;% mutate(different = P.adj&lt;0.05) # Note the significant differences between countries/continents diff_dengue_country &lt;- dunn_dengue_country %&gt;% filter(different == TRUE) diff_flu_continent &lt;- dunn_flu_continent %&gt;% filter(different == TRUE) # Note the comparable countries/continents nondiff_dengue_country &lt;- dunn_dengue_country %&gt;% filter(different == FALSE) nondiff_flu_continent &lt;- dunn_flu_continent %&gt;% filter(different == FALSE) Seperate tables were made to show the combinations with significant difference and without. For the dengue data, 26 out of 45 combinations between the 10 countries were not significantly different in amount of dengue cases per week per 100.000 inhabitants. For the flu data, 3 out of 10 combinations between the 5 continents were not significantly different in amount of flu cases per week per 100.000 inhabitants. 5.5 Visualisations We can also create varying visualizations to get an overall look of how the countries/continents compare to each other and how the numbers changed through the years. 5.5.1 Dengue cases In figure 4.2 a scatterplot shows the average amount of weekly dengue cases per year with a different color for each country. Singapore has a clear higher number of cases for each year, except 2009. Singapore’s climate is a natural breeding ground for the dengue-carrying Aedes mosquitoes, which prefer a tropical climate according to Khetarpal and Khanna (2016). By the end of 2016 27% of outbreak-associated dengue cases reported in literature had been in Singapore, second to China with 27.9% (Guo et al. 2017). # Visualising the weekly dengue cases per country per year in a scatterplot ggplot(gd_norm_country, aes(x=year, y=cases_per_100000, group = country, color = country)) + geom_point() + geom_line() + labs(title = &quot;Dengue cases&quot;, y = &quot;Average dengue cases per week&quot;, x = &quot;Year&quot;, caption =graph_cap(&quot;Figure 4.2. Scatterplot to show the average amount of weekly dengue cases per 100.000 inhabitants per year per country.&quot;)) + scale_x_continuous(breaks = round(seq(min(gapminder_flu_dengue$year), max(gapminder_flu_dengue$year), by = 1),1)) + theme_minimal() 5.5.2 Flu cases For the flu cases we will be comparing continents instead of countries, first comparing the average amount of weekly flu cases per 100.000 inhabitants per continent per year. In figure 4.3 a bar graph shows the average per continent with the standard deviation as error bars. Africa has a clear higher average of 5.2 per 100.000 inhabitants per week. In less tropical continents, like Europe and North America, most flu cases occur in the colder months. In more tropical continents, like Africa, they occur all throughout the year (Yazdanbakhsh and Kremsner 2009). Multiple factors like, access to health care, poor nutrition, chronic infections and indoor air pollution could contribute to the increase in flu cases (Ortiz et al. 2012). # Make a bar graph with the relative average population and weekly flu cases ggplot(gf_summ, aes(x=reorder(continent, -avg_cases_per_100000), y=avg_cases_per_100000, fill=continent)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge(), show.legend = FALSE) + geom_errorbar(aes(ymin=avg_cases_per_100000-sd_cases_per_100000, ymax=avg_cases_per_100000+sd_cases_per_100000), width=.2) + labs(title = &quot;Flu cases per continent&quot;, y = &quot;Flu cases per week per 100.000 inhabitants&quot;, x = NULL, caption = graph_cap(&quot;Figure 4.3. The average amount of weekly flu cases per 100.000 inhabitants per continent from approx. 2003 to 2015.&quot;)) + theme_minimal() For further analysis a scatterplot has been made, shown in figure 4.4. One points which stands out for all continents except Oceania is the increase in 2009, only to go back down again in 2010. In 2009 there was a pandemic called the Swine Flu which might be the cause of this sudden jump in flu cases for all continents (Akin and Gözel 2020). This also explains why Oceania was affected less, as it is separated from other continents by oceans. # Visualising the weekly dengue cases per continent per year in a scatterplot ggplot(gf_norm_continent, aes(x=year, y=cases_per_100000, group = continent, color = continent)) + geom_point() + geom_line() + labs(title = &quot;Weekly flu cases across the world&quot;, y = &quot;Weekly flu cases&quot;, x = &quot;Year&quot;, caption = &quot;Figure 4.4. Scatterplot to show the average amount of weekly flu cases per year per continent.&quot;) + scale_x_continuous(breaks = round(seq(min(gf_norm_continent$year), max(gf_norm_continent$year), by = 1),1)) + theme_minimal() "],["Workflows_Portfolio_8.html", "Chapter 6 R packages 6.1 The Demo 6.2 RMarkdown driven development 6.3 Package “portrobbo”", " Chapter 6 R packages As part of assignment 8 from the DSFB2 Workflows course. For this assignment the previous assignments will be analysed to check for any repetitive code. This code will be turned into functions, combined into my first R package. 6.1 The Demo The Whole Game demo made by Hadley Wickham was followed to start creating the R package. This demo was a complete guide through setting up the package, connecting it to Git and writing the first function. It went over useful packages like roxygen2 and testthat. A short summary of packages and functions was made based on what I learned in the demo for later use. R package building cheatsheet Useful libraries: library(devtools) library(usethis) library(roxygen2) library(testthat) How roxygen2 works: Put cursor in function Code -&gt; insert roxygen skeleton/li&gt;&gt; Fill in necessary details Important functions: create_package(“~/path/to/package/packaganame”) – Creates package use_git() – Connects to git use_r(“filename”) – Opens/creates script in R/ load_all() – Loads all functions without using globalenv exists(“function”, where = globalenv(), inherits = FALSE) – Checks if function exists in globalenv check() – Checks entire package for errors/warnings/notes use_mit_license() – Sets license to MIT document() – Creates/updates the manuals in man/ based on roxygen – Updates the NAMESPACE file install() – Installs the package use_testthat() – Declares our intent to write tests – Creates test directories use_test(“functionname”) – Opens/create a testfile test() – Performs all tests use_package(“packagename”) – Add a package to import in DESCRIPTION rename_files(“oldname”, “newname”) – Rename files use_readme_rmd() – Writes README template – Adds readme to the ignore file build_readme() – Updates the readme.md file based on the readme.rmd 6.2 RMarkdown driven development In the Workflows course the use of “Start with RMarkdown” was emphasized. This style of creating an R package does not start from zero like the demo suggested. It is based on a pre-written RMarkdown and the scripts that came with it. There are a lot of advantages to creating a complete all-in-one package when sharing a RMarkdown based analysis for others to reproduce. Usually, this sharing happens through an RMarkdown, some functions, the tidy version of the data, the steps used to get it tidy, the analysis itself and the used packages. These are multiple separate elements for someone to go over before they can reproduce the analysis. It can be made a lot more efficient by taking a few extra steps to create an R package from the RMarkdown. This way the elements are combined in one unit which is easily shared and can be used for multiple purposes and on different datasets. 6.3 Package “portrobbo” For this assignment we were asked to create a package which supports our portfolio. This will make the RMarkdowns more fluid and reduce duplicated code. It can also be used for later analysis. 6.3.1 Picking a name To find an available but still meaningful name I wanted to combine the word ‘portfolio’ with my partner’s name. My partner happens to work on ships so the name portrobbo (combining the words ‘portfolio’, ‘port’ and his name) came to mind. After checking the availability I decided to use that as the name for my first R package. After setting up the project for the package and the github repository it was ready for it’s first function. 6.3.2 The functions After going over my written code, it was difficult to find duplications. For most assignments no code was written, except for assignment 1 and assignment 7. In the end, three functions have been made which shorten the code slightly and will be useful for future projects. For a longer explanation and manual, visit the package’s github repository. save_csv_rds() -&gt; saves a dataframe in .csv and .rds format graph_cap() -&gt; wraps a string at 80 characters so it fits well in the graphs format graph_jitter -&gt; shortens the positon_jitter function to only have one number as input More functions will be added as the portfolio develops and expands. "],["Workflows_Portfolio_9.html", "Chapter 7 Parameters 7.1 Inspecting the data 7.2 Creating the graphs 7.3 Checking parameters", " Chapter 7 Parameters As part of assignment 9 from the DSFB2 Workflows course. After learning about reproducibility and creating packages, parameters are an important factor in creating an analysis. Working with parameters will make it easier to re-use or apply the analysis to different datasets or different categories within the same dataset. In this assignment data about COviD19 cases and COVID19-related deaths will be used. The data was taken from “Data on the Daily Number of New Reported COVID-19 Cases and Deaths by EU/EEA Country” (2022) and the parameters have been set in the YAML of the index page as shown in figure 7.1. First the analysis will be executed with the parameters as they are in figure 7.1. Later the analysis will be repeated with different inputs to show the functionality of the parameters. 7.1 Inspecting the data Before using the parameters it is important to know what data we are working with. covid19 &lt;- read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/nationalcasedeath_eueea_daily_ei/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) str(covid19) # There&#39;s 11 columns, all either integer or character # Inspecting the countries uniq_val(covid19$countriesAndTerritories) # 30 different countries took part uniq_val(covid19$geoId) == 30 # Each country has it&#39;s own geoID uniq_val(covid19$countryterritoryCode) == 30 # Each country has it&#39;s own territory code unique(covid19$continentExp) # All countries are European # Inspecting the timeline min(covid19$dateRep) # Data was taken from 01/01/2020 max(covid19$dateRep) # Until 31/12/2021 7.2 Creating the graphs Two graphs will be made, one using the column ‘cases’ and one using the column ‘deaths’. The other inputs, like timeframe and country, will be parameterized. To make this easy the parameters will only be used in the first step, to extract the useful data from the main dataset. After this, only that extracted data will be used without referencing the parameters again. The four parameters used are “country”, “year”, “fromMonth” and “untilMonth”. # The original dataset will be filtered using the parameters covid19_cases_graph &lt;- covid19 %&gt;% filter(countriesAndTerritories == params$country, year == params$year, month == c(params$fromMonth:params$untilMonth)) # To use the dateRep column as the x-axis in the graphs, it&#39;s class will be changed to &#39;date&#39; covid19_cases_graph$dateRep &lt;- as.Date(covid19_cases_graph$dateRep, &quot;%d/%m/%y&quot;) Now that only the data specified by the parameters has been extracted, it can be used for the graphs. It is important to refer to the newly created dataset, not just for the values themselves but also when writing captions or labels. This was achieved using the paste function to adjust the strings to the extracted data. The first graph looks at the COVID19 cases and can be seen in figure 7.1, the second graph is the same except it looks at COVID19 related deaths and can be seen in ifgure 7.2. # Creating the graph with covid cases cases &lt;- ggplot(covid19_cases_graph, aes(x=dateRep, y=cases)) + geom_point() + geom_line() + labs(y = &quot;Cases&quot;, x = NULL) + theme_minimal() # Creating the graph with covid related deaths deaths &lt;- ggplot(covid19_cases_graph, aes(x=dateRep, y=deaths)) + geom_point() + geom_line() + labs(y = &quot;Related deaths&quot;, x = NULL, caption = paste0(&quot;Figure 7.1. Scatterplot to show the COVID19 cases and related deaths in &quot;, covid19_cases_graph$countriesAndTerritories[1], &quot; from &quot;, min(covid19_cases_graph$dateRep), &quot; until &quot;, max(covid19_cases_graph$dateRep), &quot;.&quot;)) + theme_minimal() # Combining the plots in one grid plots &lt;- plot_grid(cases, deaths, labels = NULL, ncol=1, align = &quot;v&quot;) # Creating the title for the grid title &lt;- ggdraw() + draw_label(paste0(&quot;COVID19 in &quot;, covid19_cases_graph$countriesAndTerritories[1]), fontface = &#39;bold&#39;, x = 0, hjust = 0) + theme( plot.margin = margin(0, 0, 0, 22) ) # Final grid plot_grid(title, plots, ncol = 1, rel_heights = c(0.1, 1) ) 7.3 Checking parameters Since the graphs are made using the parameters, it can easily be changed to show different data. Below are a few examples of other inputs for the parameters, with a screenshot of the resulting graphs. Germany from August 2020 until December 2020 Austria for all of 2021 Italy for February 2020 until May 2020 "],["Workflows_Portfolio_Free.html", "Chapter 8 Free assignment 8.1 Data about 3D protein structures 8.2 Structure determination software", " Chapter 8 Free assignment As part of assignment 3.2 from the DSFB2 Workflows course. In order to prepare for the future and find direction in my career I have decided to point this free assignment at structural biology. There are two paths within three-dimensional structural bioinformatics, one focusing on the evolutionary history and the other on the physical interactions (Jumper et al. 2021). Since I am most interested in the latter, I will be doing the following: Research databases and files with protein structure data Research protein folding and it’s current predictability Dive into AlphaFold and it’s software Learn and implement some of the coding done by AlphaFold myself Reflect and look forward 8.1 Data about 3D protein structures In the past decade the use of high-throughput technologies within the biomedical field has vastly increased. Studies about genomics, transcriptomics and proteomics are supported by more and more databases and software tools. There’s a large variety of protein classes for bioinformatics databases like sequences, enzyme/pathway, chemistry or 3D structures. (Chen, Huang, and Wu 2017) 8.1.1 Protein Data Bank Archive When it comes to databases for 3D protein structures, the worldwide PDB (Protein Data Bank archive) was announced in 2003 (Berman, Henrick, and Nakamura 2003). This international collaboration includes PDBe for Europe, PDBj for Japan, RCSB PDB (Research Collaboratory for Structural Bioinformatics) and BMRB (Biological Magnetic Resonance Bank). The archive offers three formats per file, legacy PDB, PDBx/mmCIF and PDBML. As of December 1st 2015 there are 113,971 biological macromolecular structures in the wwPDB database including 37,049 distinct protein sequences, 30,099 structures of human sequences, 8,096 Nucleic Acid containing structures. (Chen, Huang, and Wu 2017) 8.1.2 PDBx/MMcif format One of the file formats used and implemented by wwPDB is PDBx/MMcif. It comes from the CIF (Crystallopgrahic Information File) format which was developed for small crystallographic molecules (Kramer Green, n.d.). As it expanded it also included macromolecules and the name was changed to MMcif. It’s strengths lie in it’s ability to archive structural models of any size and being fully machine-readable. With all it’s advantages compared to other formats, like legacy PDB, PDBx/MMcif became the main format of use for wwPDB in 2007. In 2011 it was also adopted by multiple major software developers. (Burley et al. 2017) 8.1.3 Visualising protein structure There are multiple types of software for the visualization of protein structures, like Jmol, Chimera and OpenRasMol. It is also possible to visualise the structure using R, for example with the package r3dmol. 8.2 Structure determination software CCP4 and Phenix "],["Workflows_Portfolio_References.html", "Chapter 9 References", " Chapter 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
