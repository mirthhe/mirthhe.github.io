[["index.html", "Curriculum Vitae Contact Profile Job experience Education Skills Volunteering Languages", " Read more Inhoud Curriculum Vitae Contact Utrecht, The Netherlands mirtheklaassen@outlook.com Linkedin Instagram Profile I’m goal-oriented, curious, analytical and versatile with a passion for molecular biology and data science. I want to apply my skills in unraveling patterns and pathways in the field of bioinformatics. Along with combining my wide range of interests this would give me the opportunity of contributing to the development of big data analysis and support biomedical research. Job experience Management Assistent, Fitness Factory, Utrecht June 2017 - Oktober 2018 My responsibilities included the remodelling projects, contact with externals, hiring and teaching new employees, basic accountancy, inventory management and technical maintenance. Also active as spinning-instructor Examtrainer Math, Lyceo, Utrecht March 2018 - September 2018 Sales employee, Lens Utrecht, Utrecht May 2018 - August 2019 Management Assistant, TrainMore Black Label, Utrecht August 2019 - September 2020 My responsibilities included directing the sales-team, technical maintenance, inventory management, member administration, contacting debtors/debt collection agencies, contact with externals, contactperson for employees and helping the manager when necessary. Webdeveloper, WisMon, Utrecht March 2021 - September 2021 Sales employee, Pipoos, Utrecht September 2022 - Present Education Communication &amp; Multimedia design, Avans, Den Bosch September 2015 - December 2017 With a personal focus on webdesign and datavisualisation Optometry, University of Applied Sciences, Utrecht September 2018 - Augustus 2019 Biology &amp; Medical Labresearch, University of Applied Sciences, Utrecht September 2020 - Present GPA of 4.0 Honours star in genetic recombination and transfection Specialization in Data Science for Biology Skills Design: Adobe Photoshop, Adobe Illustrator and Adobe InDesign Informatics: Basic R, basic BASH, HTML and CSS Volunteering Design-expert, Innovation in a Week, Veghel July 2016 Won the ‘Teamwork-award’ Student Association Uerius Brabus, Den Bosch October 2015 - December 2016 Active as co-founder, captain of the PR committee and active in the committee for external contacts Languages Dutch: native speaker English: level C1 "],["Workflows_Portfolio_1.html", "Chapter 1 Reproducible Research 1.1 Excel’s downsides 1.2 Repita criteria 1.3 Open source code", " Chapter 1 Reproducible Research As part of assignment 1 from the DSFB2 Workflows course. 1.1 Excel’s downsides 1.1.1 Inspecting an Excel file As part of assignment 1.1 I was asked to review this file, by opening the file in Excel. The experimental condition ‘ControlVehicleA’ stood out to me. It appears to be the same as the ControlPositive but with less Ethanol. I’m guessing this shows the experimental conditions without the experimental compounds which would mean the compounds have been diluted in ethanol. There are also extra sheets with lists of inputs, logs of changes and input examples. I’m not sure what the difference in purpose is between the ExpLookup sheet and the Input example sheet. Aside from that three compounds have been tested, 2,6-diisopropylnaphthalene, decane and naphthalene. The positive control for this experiment uses 1.5% ethanol in S-medium.. The negative control for this experiment uses just S-medium without an added compound. Next up, the file was opened in R and the data types per column were checked, these seemed to differ from the expectations. # Read the excel file CE.LIQ.FLOW.062_Tidydata &lt;- read_excel( &quot;data-raw/data011/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;) RawData was double while I’d expect an integer, compName was character while I expected a factor and compConcentration was character while it should be a double. It had not been assigned correctly. For further analysis the types were changed to the correct ones. # The compConcentration has to be turned into a numeric value CE.LIQ.FLOW.062_Tidydata$compConcentration &lt;- as.double(CE.LIQ.FLOW.062_Tidydata$compConcentration) # The RawData has to be turned into an integer CE.LIQ.FLOW.062_Tidydata$RawData &lt;- as.integer(CE.LIQ.FLOW.062_Tidydata$RawData) # Turn the data into a tibble CE.LIQ.FLOW.062_tbl &lt;- as.tibble(CE.LIQ.FLOW.062_Tidydata) # The compName&#39;s have to be turned into a factor # Create a vector with the levels compName_factor &lt;- c(&quot;2,6-diisopropylnaphthalene&quot;, &quot;decane&quot;, &quot;naphthalene&quot;, &quot;Ethanol&quot;, &quot;S-medium&quot;) # Use the levels to create the factor CE.LIQ.FLOW.062_tbl$compName &lt;- factor(CE.LIQ.FLOW.062_tbl$compName, levels = compName_factor) # Check if it worked CE.LIQ.FLOW.062_tbl # it did! ## # A tibble: 360 × 34 ## plateRow plateCo…¹ vialNr dropC…² expType expRe…³ expName expDate ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 NA NA 1 a experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 2 NA NA 1 b experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 3 NA NA 1 c experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 4 NA NA 1 d experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 5 NA NA 1 e experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 6 NA NA 2 a experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 7 NA NA 2 b experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 8 NA NA 2 c experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 9 NA NA 2 d experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## 10 NA NA 2 e experi… 3 CE.LIQ… 2020-11-30 00:00:00 ## # … with 350 more rows, 26 more variables: expResearcher &lt;chr&gt;, expTime &lt;dbl&gt;, ## # expUnit &lt;chr&gt;, expVolumeCounted &lt;dbl&gt;, RawData &lt;int&gt;, compCASRN &lt;chr&gt;, ## # compName &lt;fct&gt;, compConcentration &lt;dbl&gt;, compUnit &lt;chr&gt;, ## # compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;, ## # elegansInput &lt;dbl&gt;, bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, ## # bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, ## # bacterialVolUnit &lt;chr&gt;, incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, … 1.1.2 Creating the scatterplot Now that the types are correct the data can be used to create a scatterplot. The assignment asked to put the compConcentration on the x-axis, the DataRaw counts on the y-axis and to assign a colour to each level in compName. It also asked to assign a different symbol to each level in the expType variable. Since the compUnit of the controls are not nM but percentages, they have been added as dashed lines. This means the different shapes for expType variables are not neccessary since only “experiment” is shown as ExpType. In the assignment the initial scatterplot was followed by a question about the x-axis, “When creating the plot under C), what happened with the ordering of the x-axis labels. Explain why this happens.”. The compConcentration has already been changed to the correct type in the previous code chunk, so nothing went wrong when creating the scatterplot. Had the type not been changed beforehand, the x-axis would have been in random orders because they were seen as characters instead of doubles. The x-axis is also shown with a log10 transformation and slight jitter has been added for better readability. A few details were added outside of the given assignment. Each compound got it’s own colour, the stdev’s have been added as errorbars and the control values have been added as dashed lines for comparison. The resulting plot is shown in figure 1. # Get the average counts per compound per concentration, keep the expType variable too CE.LIQ_summ &lt;- CE.LIQ.FLOW.062_tbl %&gt;% group_by(compName, compConcentration, expType) %&gt;% summarize(mean_counts = mean(RawData, na.rm = TRUE), stdev_counts = sd(RawData, na.rm = TRUE),) # Create a tibble containing only the data used for the graph CE.LIQ_summ_exp &lt;- CE.LIQ_summ %&gt;% filter(expType == &quot;experiment&quot;) # Create the scatterplot CE.LIQ_summ_exp %&gt;% ggplot(aes(x = log10(compConcentration) # log10 as suggested in question F , y = mean_counts)) + geom_smooth(aes(group=compName, color = compName), span= .5) + geom_point(aes(color = compName), position = position_jitter(width = 0.2, height = 0.2, seed = 123)) + geom_errorbar(aes(ymin=mean_counts-stdev_counts, ymax=mean_counts+stdev_counts, color = compName), width=.1, position = position_jitter( width = 0.2, height = 0.2, seed = 123)) + labs(title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count&quot;, x = &quot;Compound concentration in log10(nM)&quot;, caption = str_wrap(&quot;Figure 1. Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations.&quot;, 80)) + geom_hline(yintercept = 85.9, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control negative geom_text(aes(0,85.9, label = &quot;Control negative&quot;, vjust=-0.3), size = 3.5) + geom_hline(yintercept = 49.4, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control positive geom_text(aes(-3.5,49.4, label = &quot;Control positive&quot;, vjust=-0.3), size = 3.5) + theme_minimal() 1.1.3 Normalize for the negative control In order to see how much the offspring count improved or decreased relatively to the negative control basline the data must be normalized. To do this we adjust the negative control to a value of 1 and adjust the other values in the same way. In this case that means dividing by 85.9. The scatterplot was created again with the new normalized data, further settings stayed the same, the result is shown in figure 2. # Check the mean value of controlNegative # view(CE.LIQ_summ) # It&#39;s 85.9 # Add the column with normalized average counts CE.LIQ_summ_norm &lt;- CE.LIQ_summ_exp %&gt;% mutate(norm_counts = mean_counts/85.9, norm_stdev = stdev_counts/85.9) CE.LIQ_summ_norm %&gt;% ggplot(aes(x = log10(compConcentration), # log10 as requested in question F y = norm_counts)) + geom_smooth(aes(group=compName, color = compName), span= .5) + geom_point(aes(color = compName), position = position_jitter(width = 0.15, height = 0.15, seed = 123)) + geom_errorbar(aes(ymin=norm_counts-norm_stdev, ymax=norm_counts+norm_stdev, color = compName), width=.1, position = position_jitter(width = 0.15, height = 0.15, seed = 123)) + labs(title = str_wrap(&quot;Mean offspring count of C. Elegans with added compounds in varying concentrations&quot;, 70), y = &quot;Offspring count relative to negative control&quot;, x = &quot;Compound concentration in log10(nM)&quot;, caption = str_wrap(&quot;Figure 2. Scatterplot with trendlines showing the mean offspring count of C. Elegans with three different compounds at varying concentrations. Normalized to negative control == 1.&quot;, 70)) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control negative geom_text(aes(0,1, label = &quot;Control negative&quot;, vjust=-0.3), size = 3.5) + geom_hline(yintercept = 0.57508731, linetype = &quot;dashed&quot;, size = 0.2) + # Shows control positive geom_text(aes(-3.5,0.57508731, label = &quot;Control positive&quot;, vjust=-0.3), size = 3.5) + theme_minimal() 1.1.4 Statistical analysis In order to learn wether there is indeed an effect of the different compounds a few tests need to be performed. Starting with the Shapiro-Wilk test to check the normality of the data. Followed by an ANOVA with the different concentrations as added variable. To see the actual difference, the analysis finished with a Post hoc test like Tukey. 1.2 Repita criteria 1.2.1 Criteria for reproducibility In this first part of assignment 1.2 we will look at ‘Repita’ criteria. The criteria are used to check for the reproducibility of scientific research articles. In table 1 the different criteria are shown with a definition and the type of response it calls for. In order to further understand and apply these criteria I will look for a research article and check if this article follows the ‘Repita’ guidelines as shown in table 1. # Create the different variables needed for the repita criteria table Transparency_Criteria &lt;- c(&quot;Study Purpose&quot;, &quot;Data Availability Statement&quot;, &quot;Data Location&quot;, &quot;Study Location&quot;, &quot;Author Review&quot;, &quot;Ethics Statement&quot;, &quot;Funding Statement&quot;, &quot;Code Availability&quot;) Definition &lt;- c(&quot;A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.&quot;, &quot;A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.&quot;, &quot;Where the article’s data can be accessed, either raw or processed.&quot;, &quot;Author has stated in the methods section where the study took place or the data’s country/region of origin.&quot;, &quot;The professionalism of the contact information that the author has provided in the manuscript.&quot;, &quot;A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.&quot;, &quot;A statement within the manuscript indicating whether or not the authors received funding for their research.&quot;, &quot;Authors have shared access to the most updated code that they used in their study, including code used for analysis.&quot;) Response_Type &lt;- c(&quot;Binary&quot;, &quot;Binary&quot;, &quot;Found Value&quot;, &quot;Binary;Found Value&quot;, &quot;Found Value&quot;, &quot;Binary&quot;, &quot;Binary&quot;, &quot;Binary&quot;) # Turn the variables into one dataframe repita_criteria &lt;- data.frame(Transparency_Criteria, Definition, Response_Type) # Use the dataframe to create the table repita_criteria %&gt;% kable(col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;Table 1. The repita criteria as given in portfolio assignment 1.2 in [lesson 1 of the reader](https://lesmaterialen.rstudio.hu.nl/workflows-reader/represintro.html).&quot;) (#tab:repita criteria)Table 1. The repita criteria as given in portfolio assignment 1.2 in lesson 1 of the reader. Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary;Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary 1.2.2 The article Plosone was used to find a suitable article. The found article is a primary article describing emperical scientific findings by Bruckner et al. (2022). The focus is on how microbiotas play a role in developmental programs. Altered microbiota composition appears to be linked to neurodevelopmental conditions such as autism spectrum disorder. One of the findings described how the microbiota can influence forebrain neurons. Forebrain neurons are required for normal social behavior and localization of forebrain microglia. The study uses zebrafish which were kept at 28°C with a 14/10 light/dark cycle. For the controlgroup conventionalized (CVZ) fish were used. The experimental condition consisted of germ-free (GF) fish. These fish were made germ-free and therefore did not have their microbiota. When the fish were older (7 dpf) they were inoculated with normal microbiota (ex germ-free or XGF) and their optomotor respons was tested. Once the fish were adults (14 dpf) their social behavior was tested. The social behavior was assessed through a dyad assay for postflexion larval and adult zebrafish. For each condition a pair of siblings were placed in isolated tanks and allowed to interact for 10 minutes via transparent tank walls. Social interaction was defined as the average relative distance from the divider and the percentage of time spent orienting at 45° to 90°. These parameters were measured and analyzed using computer vision software written in Python (available at https://github.com/stednitzs/daniopen). Compared to the CVZ siblings, the GF larvae spent significantly less time than in close proximity to and oriented at 45 to 90° to the stimulus fish. These results show that an intact microbiota is required early for later development of normal social behavior. Aside from the social behavior, the study also looked at optomotor responses. It is possible that the microbiota influences circuitry underlying the early vision and locomotion required for social behavior. To address this possibility, the vision and locomotion were assayed by comparing kinetics of the optomotor response to virtual motion in GF larvae and CVZ controls. Optomotor response was assessed using a “virtual reality” system for assessing zebrafish behavior, measuring swim response in larvae. A visual stimulus was projected on a screen underneath the dishes for 20 seconds and consisted of concentric rings moving toward the dish center, followed by a 20-second refractory period. Responses are the average of 46 to 59 stimulus trials per fish, presented over 1 hour. In this part of the study, no significant difference were found between the GF and CVZ fish. This suggests that the microbiota influences circuits specific to social behavior directly, rather than by modulating vision or locomotion. The results demonstrate that the microbiota influences zebrafish social behavior by stimulating microglial remodeling of forebrain circuits during early neurodevelopment. This conclusion suggests pathways for new interventions in multiple neurodevelopmental disorders. 1.2.3 The reproducibility To better understand the criteria (table 1.) they will be applied to the found article. The results can be found in table 2. While answering this assignment I noticed how this is not as black and white as I thought. For example with the study location. At first I thought it was stated, because University of Oregon came back quite a few times, looking at it more closely it was not clear if that was the actual study location. # Create the different variables needed for the article check table Score &lt;- c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;[Link](https://figshare.com/projects/Bruckner_et_al_Data/136756)&quot;, &quot;FALSE&quot;, &quot;Non professional&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;) Explanation &lt;- c(&quot;In the second to last paragraph of the introduction the study objective is described.&quot;, &quot;On the left-hand side of the webpage the \\&quot;Accesibble Data\\&quot; can be found&quot;, &quot;The data can be accessed through the given link, found under the \\&quot;Accesibble Data\\&quot; header.&quot;, &quot;The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. &quot;, &quot;Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given.&quot;, &quot;There is an ethics statement in the methods paragraph.&quot;, &quot;The funding is stated under the abstract.&quot;, &quot;The data analysis is explained but the code has not been shared.&quot;) # Turn the variables into one dataframe article_repita_check &lt;- data.frame(Transparency_Criteria, Score, Explanation) # Use the dataframe to create the table article_repita_check %&gt;% kable(col.names = gsub(&quot;_&quot;, &quot; &quot;, names(repita_criteria)), caption = &quot;Table 2. The score for each repita criterium when looking at the research done by Bruckner et al.(2022)&quot;) (#tab:article check)Table 2. The score for each repita criterium when looking at the research done by Bruckner et al.(2022) Transparency Criteria Definition Response Type Study Purpose TRUE In the second to last paragraph of the introduction the study objective is described. Data Availability Statement TRUE On the left-hand side of the webpage the “Accesibble Data” can be found Data Location Link The data can be accessed through the given link, found under the “Accesibble Data” header. Study Location FALSE The origin of the zebrafish has been given. The workplace of the authors has also been given (University of Oregon) but the location where the research has been performed is not speficially stated anywhere in the article. Author Review Non professional Only the email adresses of P. Washbourne and J.S. Eisen were given. Both were part of the funding acquisition. The contact information of the researchers has not been given. Ethics Statement TRUE There is an ethics statement in the methods paragraph. Funding Statement TRUE The funding is stated under the abstract. Code Availability FALSE The data analysis is explained but the code has not been shared. 1.3 Open source code In this second part of assignment 1.2 we’ll be looking at the code from an existing study. The focus will be on understanding the code, noting what it tries to achieve, the readability, reproducibility, fixing errors, etc. 1.3.1 The article The article used for this assignment had to live up to a few criteria. First up, the data had to be analysed using R code. This code and the dataset had to be available. The chosen article is “Mental Health Impacts in Argentinean College Students During COVID-19 Quarantine” by López Steinmetz et al. (2021). The study aimed to analyze differences in mental health in college students from Argentina who were exposed to different spread-rates of COVID-19. They also wanted to analyze between group differences in mental health indicatores at four different quarantine sub-periods. To do this a cross-sectional design was used. The sample included 2687 college students and the data was collected online during the Argentinean quarantine. They used a one-way between-groups ANOVA with Tukey’s post hoc test for the analysis. The results showed that the center and most populated area only differed in psychological well-being and negative alcohol related consequences, but not in the other indicators. For the sub-periods there were differences in psychological well-being, social functioning and coping, psychological distress, and negative alcohol-related consequences. Negative alcohol-related consequences were the only MHS indicator improving over time. This worsened mean mental health suggests that quarantine and its extensions contribute to negative mental health impacts. 1.3.2 The code The code, as well as the dataset, for the study were made available through OSF. The original can be found here and the version with my adjustments can be found at the bottom of this page. The dataset and the code were added to the repository as well. After reading the code it appears to consist of a few parts. Reference to the manuscript Loading the data and packages Methods in general Sample size Distribution by sex, also noted in percentages Mean, median and stdev of the age of the students Distribution by province in percentages Methods for data analysis Test of skewness per mental health indicator Test or Kurtosis per mental health indicator with criteria and criteria reference Division based on region for first aim Division based on sub-periods for second aim Results for aim 1 (differences in MHS based on region) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per region Results for aim 2 (differences in MHS based on sub-period) Anova per mental health indicator Summary Plot TukeyHSD test Plot of Tukey HSD Significant differences Mean and stdev for indicator per sub-period Plot with means per sub-period and 95% confidence interval The code is used to make the data clear and tidy by first noting details like sample size, distributions in percentages and tests of skewness. Then it moves on to perform statistical tests for the two different aims of the study. It shows the significant differences and creates plots for the second aim. It was very easy to read the code, I would give it a 5/5 on readability. It was made especially easy by the clear headers and comments. 1.3.3 Excuting the code To further inspect the code we will execute it on the dataset given by the study. To this we change one word in the data set in order to link the dataset as it is downloaded to the project. To this the word “clipboard” under comment “Load the dataset” will be changed into raw-data/data012/dataset.xlsx, the location of the downloaded dataset. Errors will be noted below the code and changed. 1.3.3.1 Changes and errors when executing The original code uses read.table and the clipboard to load the data. This can very quickly be done wrong. To fix this the readxl package was added and the excel file was added using table&lt;-read_excel(“data-raw/data012/dataset.xlsx”). “Error in model.frame.default(formula = table$PSYCH.WELLBEING ~ table$REGIONS,:invalid type (NULL) for variable ’table$PSYCH.WELLBEING_. In the table the column”PSYCH WELLBEING” uses a space instead of a period. This also happens for other column names. To change this colnames(table) &lt;- str_replace_all(colnames(table), ” “,”.”) was added before testing skewness. The rest of the code stayed the same. To make the knitting of this rmarkdown easier the code has been adjusted to only look at “PSYCH.WELLBEING” as an example. All other indicators would have been analysed in the exact same manner. 1.3.4 Effort scoring Thanks to the great readability and consistency of the code it was very easy to reproduce. The only issue was adjusting the columnnames to use periods instead of spaces which was easily fixed. Therefore the score is a 5/5 on reproducibility. Read the code including my adjustments # R Code for the manuscript entitled: # &quot;Mental health impacts in Argentinean college students during COVID-19 quarantine&quot;. # López Steinmetz L.C., Leyes C.A., Dutto Florio M.A., Fong S.B., López Steinmetz R.L. &amp; Godoy J.C. ########################################################################## ################## LOAD THE DATASET &amp; PACKAGES ########################### ########################################################################## # Load the dataset table&lt;-read_excel(&quot;data-raw/data012/dataset.xlsx&quot;) # Changed by Mirthe Klaassen for this portfolio assignment summary(table) ## SUB PERIODS IN PRE AND POST REGIONS PROVINCE ## Length:2687 Length:2687 Length:2687 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## SEX AGE PSYCH WELLBEING SOC FUNC AND COPING ## Length:2687 Min. :18.00 Min. :0.000 Min. :0.000 ## Class :character 1st Qu.:20.00 1st Qu.:2.000 1st Qu.:0.000 ## Mode :character Median :22.00 Median :3.000 Median :2.000 ## Mean :22.74 Mean :3.086 Mean :2.149 ## 3rd Qu.:24.00 3rd Qu.:5.000 3rd Qu.:4.000 ## Max. :39.00 Max. :6.000 Max. :6.000 ## K10 BDI STAIR YAACQ ISO ## Min. :10.0 Min. : 0 Min. : 1.00 Min. : 0.000 Min. : 1.00 ## 1st Qu.:19.0 1st Qu.:10 1st Qu.:21.00 1st Qu.: 0.000 1st Qu.:22.00 ## Median :25.0 Median :16 Median :29.00 Median : 3.000 Median :32.00 ## Mean :25.5 Mean :18 Mean :29.24 Mean : 3.699 Mean :34.56 ## 3rd Qu.:32.0 3rd Qu.:25 3rd Qu.:38.00 3rd Qu.: 6.000 3rd Qu.:46.00 ## Max. :49.0 Max. :57 Max. :58.00 Max. :21.000 Max. :82.00 # Load the packages: library(moments) library(gplots) library(readxl) ## Added by Mirthe Klaassen for this portfolio assignment ########################################################################## ###################### METHODS ########################################### ########################################################################## ###### SUB-TITLE: METHOD &gt; Sample and procedure # SAMPLE N = 2687 # Distribution by sex: table(table$SEX) ## ## man other woman ## 473 22 2192 # Absolute frequencies: Women = 2192, Men = 473, Other = 22 prop.table(table(table$SEX))*100 ## ## man other woman ## 17.603275 0.818757 81.577968 # Percentages: Women = 81.577968%, Men = 17.603275%, Other = 0.818757% # Central tendency measures by age (total sample) # mean mean(table$AGE) ## [1] 22.74023 # Mean age = 22.74023 # standard deviation sd(table$AGE) ## [1] 3.635612 # sd age = 3.635612 # median median(table$AGE) ## [1] 22 # median age = 22 # Distribution by provinces prop.table(table(table$PROVINCE))*100 ## ## CABA CBA JUJ PCIAB SAL STACR TDELF ## 11.9464086 39.0026051 6.6989207 31.9315221 7.1082992 0.9676219 2.3446223 # JUJ (JUJUY) = 6.6989207% # SAL (SALTA) = 7.1082992% # CBA (CÓRDOBA) = 39.0026051% # STACR (SANTA CRUZ) = 0.9676219% # TDELF (TIERRA DEL FUEGO) = 2.3446223% # CABA (CIUDAD AUTÓNOMA DE BUENOS AIRES) = 11.9464086% # PCIAB (PROVINCIA DE BUENOS AIRES) = 31.9315221% ###### SUB-TITLE: METHOD &gt; Data analysis # Adjustment of colnames to be the same as the names in the code == added by Mirthe Klaassen for this assignment colnames(table) &lt;- str_replace_all(colnames(table), &quot; &quot;, &quot;.&quot;) ### To test Skewness and Kurtosis # Criteria: range of acceptable values or near to (-3 and +3; Brown, 2006). # Reference: Brown T.A. (2006). Confirmatory factor analysis for applied research. New York: Guilford Press. # PSYCH.WELLBEING skewness(table$PSYCH.WELLBEING) ## [1] -0.05214941 # skewness PSYCH.WELLBEING = -0.05214941 kurtosis(table$PSYCH.WELLBEING) ## [1] 1.951112 # kurtosis PSYCH.WELLBEING = 1.951112 ### For analyses corresponding to the first aim, we divided the entire sample into four groups: table(table$REGIONS) ## ## CENTER MOST POPULATED NORTH SOUTH ## 1048 1179 371 89 # NORTH = 371 # CENTER = 1048 # SOUTH = 89 # MOST POPULATED = 1179 ### For analyses corresponding to the second aim, we divided the entire sample into four groups: table(table$SUB.PERIODS.IN.PRE.AND.POST) ## ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1508 525 364 ## 4. REMAINING WEEKS POST ## 290 # first week pre-quarantine extension (ONE WEEK PRE) = 1508 # second week pre-quarantine extension (TWO WEEK PRE) = 525 # first week post-quarantine extension (ONE WEEK POST) = 364 # remaining weeks post-quarantine extension (REMAINING WEEKS POST) = 290 ########################################################################## ###################### RESULTS ########################################### ########################################################################## ########################################################################## ####################### AIM 1 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four regions table$PSYCH.WELLBEING ## [1] 6 2 3 4 0 4 0 4 2 2 2 3 0 4 4 5 3 6 3 5 2 6 5 5 5 2 3 6 6 3 4 4 2 1 4 0 2 ## [38] 5 0 1 4 5 4 1 6 3 5 5 0 5 5 6 3 3 5 4 3 4 0 6 4 1 5 2 2 1 5 2 3 4 4 4 1 2 ## [75] 4 1 5 4 5 6 1 3 2 4 2 2 2 6 5 2 2 2 4 5 4 4 4 5 6 3 3 2 2 2 4 5 1 1 5 0 0 ## [112] 5 2 2 5 3 3 1 0 4 4 2 6 1 6 4 1 2 3 4 4 4 6 1 2 0 1 3 6 2 6 6 5 2 6 4 4 2 ## [149] 3 3 2 4 3 5 1 2 0 4 3 2 1 3 6 1 5 0 2 4 2 1 1 3 1 1 5 4 4 1 1 0 6 3 1 5 5 ## [186] 0 1 3 3 0 3 6 1 0 1 3 1 3 4 2 4 2 3 0 3 4 5 1 4 2 3 5 2 2 6 5 1 1 1 1 5 1 ## [223] 0 0 1 0 0 3 6 6 1 4 3 3 2 0 3 2 1 6 6 2 5 0 3 5 5 2 2 4 4 1 6 0 1 6 2 1 5 ## [260] 1 0 0 4 5 2 4 5 6 6 5 3 5 1 1 5 5 5 4 1 1 1 2 3 6 5 1 0 6 3 1 1 1 3 2 4 6 ## [297] 3 3 1 2 4 5 3 2 4 4 4 6 3 2 5 3 3 1 5 6 6 5 5 2 2 2 2 0 4 4 4 2 2 5 4 0 4 ## [334] 4 0 1 0 5 1 3 0 0 4 6 2 4 0 1 4 3 4 2 1 3 5 4 2 6 0 0 4 1 5 6 3 0 2 5 2 6 ## [371] 5 2 3 6 3 4 3 5 2 5 2 1 3 4 5 5 1 4 0 0 0 0 3 2 3 6 2 3 5 0 3 6 1 2 6 2 5 ## [408] 4 2 4 1 4 6 2 3 4 2 1 4 2 2 2 6 3 3 0 4 1 0 0 0 4 5 2 2 1 5 0 4 3 4 1 4 3 ## [445] 5 5 3 1 0 2 2 1 3 2 1 5 1 3 0 6 3 4 4 3 6 2 2 3 6 0 4 0 2 1 1 5 4 5 3 0 1 ## [482] 3 6 4 1 4 4 2 6 6 5 3 4 2 0 6 0 0 5 0 0 5 0 3 1 3 2 0 3 2 1 4 1 2 4 3 2 6 ## [519] 4 4 0 2 4 3 4 2 4 1 3 3 2 4 4 3 0 5 2 0 1 4 0 3 0 2 2 4 3 4 5 4 6 2 2 4 5 ## [556] 2 2 2 6 2 2 4 3 6 6 4 6 5 1 1 4 3 0 1 4 4 1 2 4 0 4 1 2 6 0 2 1 3 1 4 0 1 ## [593] 3 3 3 4 6 4 0 3 5 3 2 3 1 3 0 3 0 2 0 3 4 2 2 2 4 2 6 6 3 5 6 1 3 1 2 6 4 ## [630] 0 3 0 2 3 5 2 4 0 5 6 4 3 4 5 4 2 2 1 0 1 6 1 4 2 4 2 5 4 3 6 3 2 3 1 5 2 ## [667] 3 1 2 0 6 0 4 0 3 4 4 0 3 2 0 2 4 3 2 3 1 2 0 1 3 0 0 5 0 2 4 2 6 3 3 0 5 ## [704] 0 5 3 2 3 4 1 2 1 5 4 5 4 5 2 4 5 5 6 4 5 3 0 5 4 4 4 6 6 6 3 6 3 4 3 3 6 ## [741] 5 6 5 4 5 5 2 5 5 4 2 5 6 4 3 2 1 3 5 0 5 5 6 2 1 4 3 0 3 1 6 5 4 2 6 1 3 ## [778] 3 1 4 1 4 3 4 6 2 2 5 5 4 3 0 4 3 2 3 2 0 5 2 0 4 1 0 2 1 6 2 1 5 0 3 5 4 ## [815] 0 0 4 4 1 1 1 5 5 3 2 0 6 2 4 3 1 4 2 2 6 5 0 1 1 5 3 2 2 3 4 1 4 4 4 3 2 ## [852] 4 2 2 5 6 6 6 3 0 5 4 4 1 5 0 5 6 1 2 3 2 3 6 2 5 1 2 3 4 2 5 4 2 4 3 1 0 ## [889] 0 1 5 3 3 6 3 0 2 3 0 5 5 2 4 6 4 3 6 0 2 0 6 5 2 6 1 4 4 3 1 5 2 0 4 0 4 ## [926] 4 1 4 2 4 5 0 3 3 4 3 6 4 4 3 1 3 6 5 1 0 3 5 1 2 2 3 1 1 1 1 3 4 5 0 1 0 ## [963] 1 5 4 4 1 3 4 2 5 2 3 3 2 1 6 6 4 4 2 5 3 3 4 6 2 3 3 0 2 6 4 5 6 3 6 6 1 ## [1000] 5 2 4 1 1 5 1 3 2 5 3 1 0 5 1 5 3 5 4 2 1 2 2 6 0 3 0 4 6 4 3 0 3 3 3 1 1 ## [1037] 4 3 3 4 0 4 4 2 5 5 0 4 4 0 2 0 6 1 3 3 6 0 0 3 2 3 6 3 3 6 1 4 3 2 5 4 4 ## [1074] 1 5 2 3 2 2 5 5 5 4 5 6 3 3 4 3 6 2 6 5 5 1 0 3 3 1 6 2 4 5 5 3 4 4 0 3 0 ## [1111] 0 3 3 4 3 1 1 2 1 4 1 4 0 2 1 3 2 2 6 1 3 2 1 3 2 1 2 1 2 0 3 4 1 6 1 3 2 ## [1148] 5 2 4 1 3 1 0 0 2 1 3 4 0 0 4 1 5 4 3 2 4 5 4 5 5 3 3 6 1 0 4 2 3 4 2 2 0 ## [1185] 5 4 6 1 2 6 1 0 5 3 5 3 4 3 2 1 5 5 5 2 5 3 1 5 6 6 4 3 3 1 3 3 6 6 3 4 4 ## [1222] 2 3 0 4 2 0 4 4 1 2 2 2 3 1 0 1 6 5 1 5 3 6 6 1 4 1 6 4 1 1 4 2 2 3 5 0 1 ## [1259] 5 4 6 2 0 0 3 1 3 4 6 0 4 2 3 0 3 2 6 6 0 2 0 5 0 2 4 4 5 2 4 1 3 4 5 4 3 ## [1296] 6 3 4 0 3 4 4 2 0 4 2 5 3 5 1 0 6 0 0 2 6 2 0 1 1 3 1 0 4 5 3 1 2 4 3 2 1 ## [1333] 0 4 5 1 2 6 3 3 3 4 4 3 0 4 6 2 2 3 6 6 1 4 1 5 3 6 4 2 3 6 3 6 4 2 1 6 1 ## [1370] 3 2 1 1 4 3 1 4 4 5 3 6 0 5 1 4 5 0 0 1 3 2 0 1 2 2 3 5 3 4 3 6 2 4 0 0 0 ## [1407] 3 3 1 2 3 3 2 2 5 3 2 1 0 4 5 3 6 2 3 2 0 0 0 0 6 5 2 2 5 4 2 4 5 0 5 5 1 ## [1444] 2 1 2 6 2 4 0 6 0 3 4 3 6 0 2 3 2 4 1 4 0 2 5 1 4 4 2 2 2 4 3 0 6 3 0 3 5 ## [1481] 1 5 3 2 2 6 0 2 4 6 2 3 6 4 3 3 2 3 1 3 4 6 6 5 3 1 6 5 3 6 3 6 2 6 2 0 0 ## [1518] 4 2 1 2 3 1 0 5 2 2 3 3 6 4 1 5 3 2 3 2 5 1 6 3 1 0 3 6 2 2 4 6 6 4 0 1 5 ## [1555] 2 3 5 3 5 6 5 5 3 1 4 5 3 6 5 6 5 1 0 1 4 5 5 4 6 4 2 2 2 5 1 6 5 0 4 2 4 ## [1592] 5 4 3 3 4 0 3 2 0 3 1 4 0 6 4 2 5 2 1 3 2 2 3 1 4 1 1 5 6 6 5 5 3 6 1 0 3 ## [1629] 3 4 4 2 0 6 3 0 4 6 2 6 3 5 2 3 6 6 0 2 3 2 1 6 2 1 4 2 2 4 2 5 5 5 2 4 4 ## [1666] 5 1 0 2 3 5 4 2 6 2 4 1 3 2 2 4 1 3 5 3 3 4 5 6 3 6 4 1 1 4 6 3 2 5 3 3 0 ## [1703] 3 3 3 6 1 4 4 5 6 0 3 5 5 5 4 6 3 1 5 5 2 2 6 6 3 2 2 5 2 6 2 3 5 3 1 6 2 ## [1740] 6 5 5 6 1 6 1 2 3 1 3 4 5 4 1 4 3 2 5 5 3 3 1 1 2 3 5 6 2 4 4 4 1 1 6 2 4 ## [1777] 3 3 4 4 2 3 3 1 2 5 4 5 5 2 4 2 5 1 6 1 5 1 6 2 3 0 2 3 3 0 5 6 1 1 3 3 3 ## [1814] 3 4 4 2 5 3 4 4 2 5 2 0 2 6 0 5 5 6 1 4 5 0 5 5 5 5 1 3 1 2 4 5 4 2 6 3 4 ## [1851] 1 3 3 0 0 5 4 6 2 6 4 5 3 4 5 1 6 5 3 1 3 3 4 5 1 2 4 3 3 6 2 2 0 0 2 2 6 ## [1888] 3 6 3 6 4 0 4 5 3 3 1 0 5 4 2 4 4 2 4 4 0 4 5 4 0 2 1 2 1 1 2 0 3 0 6 2 0 ## [1925] 3 2 5 5 3 5 1 1 6 3 5 4 5 6 3 5 5 5 1 0 3 3 4 6 3 5 4 3 4 0 6 0 2 1 3 3 3 ## [1962] 3 4 2 0 4 4 6 5 1 3 0 2 3 5 3 2 3 3 1 4 5 3 3 2 3 6 4 2 1 6 0 4 2 0 4 5 2 ## [1999] 4 2 2 3 2 4 0 4 1 1 3 4 3 4 3 3 0 4 0 5 5 4 1 1 3 0 4 2 1 5 0 1 1 4 4 5 5 ## [2036] 4 3 1 5 3 1 6 4 5 4 0 5 3 2 6 2 6 6 5 1 1 0 3 2 2 4 2 0 3 3 3 5 3 1 5 6 3 ## [2073] 1 2 3 5 4 0 0 6 5 3 2 6 6 3 2 4 6 1 1 5 2 6 6 5 4 4 5 0 0 4 5 1 3 2 4 3 3 ## [2110] 4 6 6 3 0 2 4 6 0 1 0 5 1 5 0 3 3 6 5 4 6 5 5 3 5 2 1 6 0 3 0 4 6 4 2 2 2 ## [2147] 2 1 2 5 3 3 6 5 3 4 5 1 5 5 0 0 5 2 6 6 1 3 2 6 5 5 1 3 6 3 4 1 1 2 6 5 3 ## [2184] 2 5 4 5 4 4 1 5 5 6 6 6 5 2 2 3 3 6 2 1 1 5 3 5 0 3 6 4 3 0 5 5 1 5 6 1 6 ## [2221] 2 2 2 2 2 5 3 3 1 2 1 0 1 3 2 3 4 5 4 0 3 5 4 5 6 3 6 5 2 3 6 2 5 6 2 5 2 ## [2258] 4 5 6 5 3 2 0 0 6 2 1 1 5 6 4 4 6 3 1 5 6 2 2 5 4 3 2 6 0 2 4 1 0 4 6 3 5 ## [2295] 4 2 1 3 3 0 3 5 6 6 5 2 4 6 4 5 6 6 5 2 4 4 4 4 4 3 1 5 6 3 4 3 6 1 6 4 5 ## [2332] 1 3 5 3 2 3 2 3 3 3 5 0 4 4 4 6 6 5 3 0 4 2 1 5 4 5 3 5 4 5 6 0 4 2 5 2 5 ## [2369] 3 5 0 3 3 2 3 2 2 2 3 0 3 5 6 5 5 6 2 4 4 6 2 6 6 1 3 1 3 4 3 2 3 2 5 4 4 ## [2406] 6 4 5 2 3 6 1 2 2 5 1 1 6 3 4 2 5 5 4 4 2 3 0 5 2 5 1 1 1 2 6 1 1 4 5 3 3 ## [2443] 1 0 4 6 3 5 1 2 6 2 1 1 4 4 5 3 0 5 5 1 4 2 5 2 0 1 4 2 4 5 2 4 1 5 5 2 1 ## [2480] 6 1 6 1 3 1 2 4 5 0 3 4 5 3 5 1 4 0 0 6 4 5 6 0 4 4 2 5 4 2 4 6 3 2 2 2 0 ## [2517] 4 2 6 3 3 6 6 3 4 2 3 5 3 4 5 1 0 5 0 2 5 3 4 5 0 5 5 1 4 6 6 6 5 4 5 2 1 ## [2554] 5 1 0 4 5 4 4 6 3 3 6 2 4 5 2 6 2 2 6 2 4 3 6 1 3 3 4 6 2 5 5 6 0 2 2 5 3 ## [2591] 3 6 1 2 6 3 2 3 0 2 3 1 4 3 3 2 4 4 2 1 5 5 3 3 3 2 5 2 0 4 1 3 3 3 6 0 1 ## [2628] 1 4 1 2 1 2 6 6 2 2 3 5 2 2 1 4 2 5 4 4 6 4 4 0 4 5 4 5 5 5 6 5 5 5 3 5 3 ## [2665] 6 5 2 3 3 4 5 4 5 5 5 4 5 2 5 3 3 5 5 4 6 4 0 # PSYCHOLOGICAL WELL-BEING/DISCOMFORT (OF GENERAL HEALTH) anovaregpsychwellbeing &lt;- aov(table$PSYCH.WELLBEING~table$REGIONS) summary(anovaregpsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$REGIONS 3 46 15.340 4.569 0.00338 ** ## Residuals 2683 9008 3.358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovaregpsychwellbeing) TukeyHSD(anovaregpsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$REGIONS) ## ## $`table$REGIONS` ## diff lwr upr p adj ## MOST POPULATED-CENTER 0.28148855 0.08151381 0.4814633 0.0017158 ## NORTH-CENTER 0.22902049 -0.05554103 0.5135820 0.1636105 ## SOUTH-CENTER 0.17650527 -0.34355917 0.6965697 0.8191275 ## NORTH-MOST POPULATED -0.05246806 -0.33286583 0.2279297 0.9633105 ## SOUTH-MOST POPULATED -0.10498327 -0.62278119 0.4128146 0.9540137 ## SOUTH-NORTH -0.05251522 -0.60848288 0.5034524 0.9949688 plot(TukeyHSD(anovaregpsychwellbeing)) # significant differences # MOST POPULATED-CENTER p adj 0.0017158 #### MOST POPULATED mean = 3.206107, CENTER mean = 2.924618 tapply(table$PSYCH.WELLBEING,factor(table$REGIONS),mean) ## CENTER MOST POPULATED NORTH SOUTH ## 2.924618 3.206107 3.153639 3.101124 tapply(table$PSYCH.WELLBEING,factor(table$REGIONS),sd) ## CENTER MOST POPULATED NORTH SOUTH ## 1.836446 1.834613 1.838215 1.725774 ########################################################################## ####################### AIM 2 ############################################ ########################################################################## ### Differences in mental health aspects (both general and specific) by four sub-periods of quarantine # PSYCHOLOGICAL WELL-BEING (OF GENERAL HEALTH) anovatemppsychwellbeing &lt;- aov(table$PSYCH.WELLBEING~table$SUB.PERIODS.IN.PRE.AND.POST) summary(anovatemppsychwellbeing) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## table$SUB.PERIODS.IN.PRE.AND.POST 3 83 27.791 8.312 1.68e-05 *** ## Residuals 2683 8971 3.344 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(anovatemppsychwellbeing) TukeyHSD(anovatemppsychwellbeing) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = table$PSYCH.WELLBEING ~ table$SUB.PERIODS.IN.PRE.AND.POST) ## ## $`table$SUB.PERIODS.IN.PRE.AND.POST` ## diff lwr upr ## 2. TWO WEEK PRE-1. ONE WEEK PRE -0.1360136 -0.37421192 0.1021846 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2031072 -0.07139874 0.4776132 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.4771883 0.17578682 0.7785898 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.3391209 0.01851552 0.6597262 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.6132020 0.26928754 0.9571164 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2740811 -0.09590843 0.6440706 ## p adj ## 2. TWO WEEK PRE-1. ONE WEEK PRE 0.4571940 ## 3. ONE WEEK POST-1. ONE WEEK PRE 0.2273681 ## 4. REMAINING WEEKS POST-1. ONE WEEK PRE 0.0002823 ## 3. ONE WEEK POST-2. TWO WEEK PRE 0.0333276 ## 4. REMAINING WEEKS POST-2. TWO WEEK PRE 0.0000283 ## 4. REMAINING WEEKS POST-3. ONE WEEK POST 0.2264068 plot(TukeyHSD(anovatemppsychwellbeing)) # significant differences # 4. REMAINING WEEKS POST-1. ONE WEEK PRE p adj 0.0002823 # 3. ONE WEEK POST-2. TWO WEEK PRE p adj 0.0333276 # 4. REMAINING WEEKS POST-2. TWO WEEK PRE p adj 0.0000283 tapply(table$PSYCH.WELLBEING,factor(table$SUB.PERIODS.IN.PRE.AND.POST),mean) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 3.033156 2.897143 3.236264 ## 4. REMAINING WEEKS POST ## 3.510345 tapply(table$PSYCH.WELLBEING,factor(table$SUB.PERIODS.IN.PRE.AND.POST),sd) ## 1. ONE WEEK PRE 2. TWO WEEK PRE 3. ONE WEEK POST ## 1.823743 1.819344 1.886359 ## 4. REMAINING WEEKS POST ## 1.796256 # Figure S1: plotmeans(table$PSYCH.WELLBEING~table$SUB.PERIODS.IN.PRE.AND.POST, main=&quot;Fig. S1: Psychological well-being/discomfort by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Psychological well-being/discomfort&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) ########################################################################## ############################# THE END #################################### ########################################################################## "],["Workflows_Portfolio_2.html", "Chapter 2 Datamanagement", " Chapter 2 Datamanagement As part of assignment 2 from the DSFB2 Workflows course. For this assignment we focused on data management. This included topics like file naming, Guerilla analytics, project structures, checksums, etc. The Guerilla analytics framework is a tool to establish an efficient, trustworthy and reproducible datamanagement workflow. It is described by Enda Ridge in this booklet. The seven core principles are: Space is cheap, confusion is expensive Use simple, visual project structures and conventions Automate (everything - my addition) with program code Link stored data to data in the analytics environment to data in work products (literate programming with RMarkdown - my addition) Version control changes to data and analytics code (Git/Github.com) Consolidate team knowledge (agree on guidelines and stick to it as a team) Use code that runs from start to finish (literate programming with RMarkdown - my addition) In this assignment the focus was on principle 2, “Use simple, visual project structures and conventions”. This is important to avoid deep nesting of files and prepare for the way a project can evolve over time. If certain guidelines are not followed it will create problems, especially when handing a project over to someone else. The recommended guidelines are as follows: Create a separate folder for each analytics project. Keep the unit of a project small. Do not deeply nest folders (max 2-3 levels). Keep information about the data, close to the data. Store each dataset in its own sub-folder and create sub-folders within it for older versions. Do not change file names or move them! If you have to, record the change in the README. Do not manually edit data source files, always use code. In code, use relative paths (here::here etc) Use one R project per project and don’t reuse projects for other stuff. This principle was put into practice on a previous project, from subject DAUR2. At the time the guidelines had not been discussed so the initial structure was my own personal preference, without thinking about reproducibility. For this assignment the arrangement of the files and folders within that project have been adjusted. The directory tree is shown in figure 1. Since DAUR2 worked with datasets given by the school, those datasets had their own location on the shared server and were not downloaded. They would have been added to the ‘data-raw’ folder with a subfolder per dataset. Figure 1. Directory tree for subject DAUR2. "],["Workflows_Portfolio_3.html", "Chapter 3 Future planning", " Chapter 3 Future planning As part of assignment 3 from the DSFB2 Workflows course. In class we often focused on creating an image of what we want our future to look like. At points we are so busy studying, we lose track of why we are studying and what we are working towards. In this assignment I answered a few questions given by the teachers. With these questions I tried to look ahead and take concrete steps towards my future. Where do I want to be in ~2 years time? In two years I will have finished my studies. Which means I completed the specialisation I’m doing currently, a minor at the professorship and a graduationproject/internship outside of school. I hope to have also started my first job in the field of Data Science or Bioinformatics, with a focus on one of my interests. This is where it gets tricky because I have a lot of interests (maybe too many for my own good). The first ones that come to mind are systembiology, structural biology, biocomplexity, statistics, forensic research and I could go on but will try to keep it short. I would also like my work to reduce the need for animaltesting. How am I doing now with respect to this goal? Aside from specializing in Data Science I am trying to filter my interests. I can’t do all the things at once so I want to start narrowing my view when it comes to my career. This is very difficult for me. Luckily, the teachers are a great help and I’m getting a clearer picture of the best route to take. In my minor I’ll be focusing on applying a probabilistic approach to the risk-evaluation of toxins within an existing pipeline. Right now I think my graduationproject will focus on something along the lines of structural biochemistry. What would be the next skill to learn? Something about how software is used to describe protein structure and folding. Bas van Gestel suggested I look at AlphaFold. They have the open source code available on github and it would be useful to learn more about it. Make a planning on how to start learning this new skill. I have blocked out a couple of days before winterbreak which I will use to work on this assignment. I will dive into the code and see if I can replicate some of it. The result of this can be seen under chapter 9 ‘Free Assignment’. "],["Workflows_Portfolio_5.html", "Chapter 4 Groupproject 4.1 Technology against animaltesting 4.2 ONTOX 4.3 Phymdos 4.4 The Goal", " Chapter 4 Groupproject As part of assignment 5 from the DSFB2 Workflows course. Alongside the subject of Workflows we will be working in groups of three students on a project. With this project we will learn about agile methods of collaboration, apply our data science skills, create a working product and present this product. In our case we are working on a project revolving around ONTOX with Marc Teunis as our cliënt. 4.1 Technology against animaltesting A goal I have with my career is the reduction of animaltesting. Important points of focus within the field of animaltesting are the three R’s. These are replacement, reduction and refinement, as described by Simmonds (2018). Replacement used to focus on insentient material but also included animal species “lower” on the pylogenic scale. In recent years the use of computer software as a replacement is becoming more and more relevant. Reduction focuses on properly designed research to minimize the need for more animals than initially expected. This becomes complicated when using more animals to start with may decrease the chance of needing more animals later on, for example when needing repetitive studies. Using technology to support this would not only reduce the amount of testanimals, it would also create easier options for reproduction of the research. The third R, refinement, is all about reducing the amount of discomfort the animals go through. This can include training the animals to get used to the treatment with positive reinforcement or the use of anesthesia. This takes more time and resources which can be prevented by using computer software. 4.2 ONTOX In their own words “The vision of the ONTOX consortium is to provide a functional and sustainable solution for advancing human risk assessment of chemicals without the use of animals in line with the principles of 21st century toxicity testing and next generation risk assessment”. One of the ways to achieve this is by collecting and combining existing data to be used in the risk assessment. Platforms like SysRev are able to extract useful data from articles. This data consists of metabolic pathways and interactions between compounds. To make the data useful for computerbased analysis it is formatted into what is called an SBtab file. This file, in turn, can be used to create visualisations of the metabolic pathways or it can be turned into other formats like SBML. Different apps and functions have been created to execute these possibilities. One of those apps is Phymdos. 4.3 Phymdos The app Phymdos was created by D. Roodzant for University of Applied Sciences Utrecht. It can take SBtab files, SBML files or extract them directly from SysRev. It allows the user to create SBML style physiological maps in the desired format. This app demonstrates the range of possiblities with this type of data. 4.4 The Goal One downside of the current proces of extracting and analysing SBtab files is how it is only applicable to single files. Looking ahead, ONTOX strives to improve human risk assessment as a general goal. In order to use this existing process for the bigger picture, it should be combining data instead of looking at one particular set. This is why Marc Teunis asked use to make a package with the following functions: 1. Extract the SBtab data from SysRev for one or more articles. 2. Merge multiple SBtab files into one new file. 3. Create a graph, visualizing the data based on the merged SBtab file. 4. Convert the SBtab file to another format like SBML. "],["Workflows_Portfolio_7.html", "Chapter 5 Relational data", " Chapter 5 Relational data As part of assignment 7 from the DSFB2 Workflows course. To be added. "],["Workflows_Portfolio_8.html", "Chapter 6 R packages", " Chapter 6 R packages As part of assignment 8 from the DSFB2 Workflows course. To be added. "],["Workflows_Portfolio_9.html", "Chapter 7 Parameters", " Chapter 7 Parameters As part of assignment 9 from the DSFB2 Workflows course. To be added. "],["Workflows_Portfolio_Free.html", "Chapter 8 Free assignment", " Chapter 8 Free assignment To be added. "],["Workflows_Portfolio_References.html", "Chapter 9 References", " Chapter 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
